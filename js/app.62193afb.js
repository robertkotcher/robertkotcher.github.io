(function(e){function t(t){for(var i,s,r=t[0],l=t[1],h=t[2],p=0,d=[];p<r.length;p++)s=r[p],Object.prototype.hasOwnProperty.call(n,s)&&n[s]&&d.push(n[s][0]),n[s]=0;for(i in l)Object.prototype.hasOwnProperty.call(l,i)&&(e[i]=l[i]);c&&c(t);while(d.length)d.shift()();return o.push.apply(o,h||[]),a()}function a(){for(var e,t=0;t<o.length;t++){for(var a=o[t],i=!0,r=1;r<a.length;r++){var l=a[r];0!==n[l]&&(i=!1)}i&&(o.splice(t--,1),e=s(s.s=a[0]))}return e}var i={},n={app:0},o=[];function s(t){if(i[t])return i[t].exports;var a=i[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,s),a.l=!0,a.exports}s.m=e,s.c=i,s.d=function(e,t,a){s.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},s.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},s.t=function(e,t){if(1&t&&(e=s(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(s.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var i in e)s.d(a,i,function(t){return e[t]}.bind(null,i));return a},s.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return s.d(t,"a",t),t},s.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},s.p="/";var r=window["webpackJsonp"]=window["webpackJsonp"]||[],l=r.push.bind(r);r.push=t,r=r.slice();for(var h=0;h<r.length;h++)t(r[h]);var c=l;o.push([0,"chunk-vendors"]),a()})({0:function(e,t,a){e.exports=a("56d7")},"02c9":function(e,t,a){"use strict";a("db5a")},"055f":function(e,t,a){e.exports=a.p+"img/cryptokitty.4fe516f3.png"},"0da4":function(e,t,a){},"0e71":function(e,t,a){"use strict";a("3f94")},1028:function(e,t,a){e.exports=a.p+"img/rnn.f7da92e3.png"},"12d6":function(e,t,a){e.exports=a.p+"img/final-1.cb8ce791.png"},"148b":function(e,t,a){},"15b0":function(e,t,a){e.exports=a.p+"img/nasa.3fe966d1.png"},1652:function(e,t,a){"use strict";a("27ef")},1771:function(e,t,a){var i={"./adrich.jpg":"6b1c","./argo.png":"28b3","./background.png":"30b3","./carnegie_mellon.png":"f11a","./codecov.jpg":"7265","./cryptokitty.png":"055f","./expii.png":"4384","./gh.png":"cb79","./hf.png":"5e5e","./home-minified.jpg":"3cee","./invisible.png":"c74e","./kahuna-logo.png":"5932","./kitchen_zen.png":"a36a","./lambda.png":"34ea","./li.png":"5db6","./logo.png":"cf053","./logo.svg":"9b19","./moment.png":"9822","./nasa.png":"15b0","./nn.png":"d116","./npm.png":"94c7","./oauth.png":"441f","./pct-of-countries.png":"bcdc","./profile.jpg":"3a19","./profile.png":"c3e7","./quadratic.png":"6819","./rnn.png":"1028","./svd.png":"5b3b","./svm.png":"c5ae","./synthesis.png":"4410","./timing_attack.png":"e750","./travel-map.png":"cbdf","./tunessence.png":"c7ca","./yt.png":"9b6b"};function n(e){var t=o(e);return a(t)}function o(e){if(!a.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=o,e.exports=n,n.id="1771"},"17c6":function(e,t,a){e.exports=a.p+"img/good.2505cc53.png"},"18dd":function(e,t,a){"use strict";a("148b")},"1fe7":function(e,t,a){e.exports=a.p+"img/vanilla.194d154a.png"},"20ea":function(e,t,a){"use strict";a("3e51")},2349:function(e,t,a){e.exports=a.p+"img/gen-3.4d3acaf8.png"},"27ef":function(e,t,a){},2823:function(e,t,a){e.exports=a.p+"img/panama-10.5f0887e7.png"},"28b3":function(e,t,a){e.exports=a.p+"img/argo.903ccda4.png"},"2ab4":function(e,t,a){"use strict";a("cf05")},3013:function(e,t,a){e.exports=a.p+"img/bad.32d4a3c9.png"},"30b3":function(e,t){e.exports="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAAcBAMAAAA6gErHAAAAMFBMVEXf39+1tbXg4OC/v7/Gxsrh4eHHx8rGxsbGxsrExMTDw8PGxsbDw8rExMq/v8q1tctsqyaSAAAAEHRSTlMACAAQMQA5MSkgGSkZIBAI107I3wAAAoNJREFUeF4V0t9rVEccxuF3ZjLtGmOZmXULRgNz9uiFvfqew7oJFmXCHjbZROlkw94IwilZYn4Ja7MkaaKCsAZLERLdprdJG9JCbwIRhHgT6EEKvbJpe99r7/oXuP4BL3weeLG6raWt206UL8JCnZmBD5X6tepQrVFd8evLloQkLYsdgII/55MUr0cmKC9Ku9P1oqSIsb5IqFzt5qJDtpXdkuxf44nJoue+/ys+2KTcC4ez39DmqF0HKFCqQQjXws47nkkHPmHEzhsHVgga2tS5yJKfmZ7QKWSZEIwdqYKfYqRQz3dnApab4Q6270HA3yQnXOUD0/AoH38L/BJbB2IPWZjmViJwUmG4X7PPtB0PZApgh5VOcX5NqSKpkRqCT34KHWQLhNJWJ/LsjyVtp2/MURD2f315F8zBsDsTt2CVfVsRA2N7CpYe7RglUhjL2076orWd4+YpKXA5XLEwDlzwHdJkyIvqMY+8sSgvexY52Cm7cRho0qx7+1UkgDzKC0C91bOfW65IitB9LK8mex51bHSZZilkvm+rewWUjZPB0NgJwlw6/NTKFmJ01Of/4Mv5yAL8wsY1X94bdAop6LN1fnX54mTMDfUCS7fjrsQ8GimQeyntZBKhQEpbrfYfLVmTEWshn/2nB1aOBM8jAKwVBwsauUPhoO5Pi83vWoxxJiVEQAfVbaVqkcPdUfrxeViDKMQhj3qbuS+SE/z9f4ru7+Ua2FsVFKCnvZEjFTuwuPXpDw6bv9WEKJR6AoDXgb6/gKHJS/cdkjkTA2JdQsfaAt9LIX02PuvQflIURR52CDBexP0zJuBk33/86Gx1Nmk329UkaTZXZpN77Wayeq9ZdR8AKlSbB1hHF9QAAAAASUVORK5CYII="},"34ea":function(e,t,a){e.exports=a.p+"img/lambda.95258360.png"},"3a19":function(e,t,a){e.exports=a.p+"img/profile.49acbe3b.jpg"},"3cee":function(e,t,a){e.exports=a.p+"img/home-minified.732b0c07.jpg"},"3e51":function(e,t,a){},"3f94":function(e,t,a){},"41bf":function(e,t,a){e.exports=a.p+"img/final-2.b4185e09.png"},4384:function(e,t,a){e.exports=a.p+"img/expii.f9a5c184.png"},4410:function(e,t,a){e.exports=a.p+"img/synthesis.f17df5cd.png"},"441f":function(e,t,a){e.exports=a.p+"img/oauth.605a7832.png"},4623:function(e,t,a){"use strict";a("0da4")},4726:function(e){e.exports=JSON.parse('[{"name":"Education","items":[{"image":"carnegie_mellon.png","start":2008,"end":2012,"name":"Carnegie Mellon University","tags":["education","computer science","music"],"description":["<p>I focused on both computer science and music - not so much the combination of the two but rather a deeper, independent, focus on each.</p><br/><p>For example, my violin sonata was released under the <a target=\'_blank\' href=\'https://open.spotify.com/album/2QDn3jB196YdTkCeLejQsD?si=o2xHYGXTTPu_xy58UDan0Q\'>Ablaze Records</a> label, and my string quartet was <a target=\'_blank\' href=\'https://youtube.com/watch?v=cOQiKpodSNE\'>recorded</a> by Grammy-nominated JACK string quartet.</p><br/><p>I also published a few <a target=\'_blank\' href=\'https://dl.acm.org/profile/84458745857\'>highly-cited papers</a> as part of my undergraduate coursework, and was a <a target=\'_blank\' href=\'https://www.blackhat.com/us-16/speakers/Robert-Kotcher.html\'>speaker at Black Hat 2016</a>.</p>"]}]},{"name":"Projects","items":[{"image":"gh.png","start":2024,"end":2024,"name":"<a href=\'https://github.com/robertkotcher/app-template\' target=\'_blank\'>Fullstack Application Template</a>","tags":["projects (past year)","python","flask","react","docker"],"description":["<p>I find myself bootstrapping a lot of fullstack applications, so decided to just build a single project that I could use to cut out the initial week or two of development for each of these projects.</p>","<p>This project has the following features:</p>","<p><ul><li>Authentication with JWT / password hashing with sha256</li><li>Email integration with sendgrid, initially used for password reset, etc.</li><li>Stripe payment and subscription management.</li><li>Todo: migrate to <a target=\'_blank\' href=\'https://docs.python.org/3/library/hashlib.html#hashlib.scrypt\'>scrypt</a> key generation, add test suite template</li></ul></p>"]},{"image":"argo.png","start":2023,"end":2024,"name":"<a href=\'https://github.com/argoproj/argo-workflows/pull/9992\' target=\'_blank\'>Argo Workflows Semaphore-Based Scheduling</a>","tags":["projects (past year)","golang","infrastructure","scheduling"],"description":["In 2023 was voted an official maintainer for Argo Workflows.","I noticed that Argo Workflows currently allows for parallelism, but workflows that are submitted later in time must wait to receive resources (as Argo scheduling is queue-based). I thought it\'d be interesting to allow more recent Workflow submissions to optionally have equal access to a share of compute resources at submission time.","In the end, the team decided to not include it as part of the main release cycle, but the fork was used by a few companies who wanted the feature."]},{"image":"hf.png","start":2024,"end":2024,"name":"<a target=\'_blank\' href=\'https://huggingface.co/rkotcher/roberta_legal_experiment\'>Document Section Extractor</a>","tags":["projects (past year)","pytorch","docker"],"description":["<div style=\'position: relative; padding-bottom: 62.5%; height: 0;\'><iframe src=\'https://www.loom.com/embed/44e26061d3444767939435e91e3b73bc?sid=f038eb53-0cc2-4c61-9078-75ea9500387d\' frameborder=\'0\' webkitallowfullscreen mozallowfullscreen allowfullscreen style=\'position: absolute; top: 0; left: 0; width: 100%; height: 100%;\'></iframe></div>","I built a classifier that could extract the background section from legal documents, built a dataset by hand, and was able to achieve a high F1 score with not too much effort."]},{"image":"gh.png","start":2024,"end":2024,"name":"<a target=\'_blank\' href=\'https://github.com/robertkotcher/zoom-meeting-bot\'>Zoom meeting bot</a>","tags":["projects (past year)","docker"],"description":["<div style=\'position: relative; padding-bottom: 62.5%; height: 0;\'><iframe src=\'https://www.loom.com/embed/76151c26376848d48d6220e458e95cbf?sid=eaf00be1-5df3-4c48-841b-5e7ad600a2a2\' frameborder=\'0\' webkitallowfullscreen mozallowfullscreen allowfullscreen style=\'position: absolute; top: 0; left: 0; width: 100%; height: 100%;\'></iframe></div>","This is a project that I built using the Zoom SDK and Docker. It allows me to join, record, and upload Zoom meetings."]},{"image":"yt.png","start":2024,"end":2024,"name":"Cats Off","tags":["projects (past year)","arduino"],"description":["<iframe width=\'100%\' height=\'400\' src=\'https://www.youtube.com/embed/vI_yvnjDnOw?si=NBeBgF22mChMbAUr\' title=\'YouTube video player\' frameborder=\'0\' allow=\'accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\' referrerpolicy=\'strict-origin-when-cross-origin\' allowfullscreen></iframe>","For this project, I built a device using an arduino and found objects to spray my cat off the counter."]}]},{"name":"Industry Experience","items":[{"start":2023,"image":"kahuna-logo.png","name":"<a href=\'https://joinkahuna.com\' target=\'_blank\'>joinkahuna.com</a> - <small>co-founder, CTO</small>","tags":["industry","startups","kubernetes","python","rag pipeline"],"description":["Kahuna is a platform for institutional investors that parses very large, legally-dense documents, into a format that is quickly digestable."]},{"start":2020,"end":2023,"image":"synthesis.png","name":"<a href=\'https://synthesis.ai/\' target=\'_blank\'>Synthesis.ai</a> - <small>second engineering hire</small>","tags":["industry","startups","kubernetes","infrastructure","golang","node js"],"description":["Synthesis AI is building a platform for programmatic generation of synthetic ML data."]},{"start":2020,"end":2022,"image":"codecov.jpg","name":"<a href=\'https://codecov.io/\' target=\'_blank\'>Codecov</a> - <small>research and development</small>","tags":["industry","startups","software_profiling"],"description":["<span><strong style=\'background-color:#ffff00\'>Update: Codecov has been <a target=\'_blank\' href=\'https://about.codecov.io/product/feature/codecov-sentry/\'>acquired by Sentry</a>.</strong> One of my R&D projects was carried over for continued development at Sentry.</span>","I\'m currently working with Codecov to prototype experimental code coverage tools and features for future integration into the core Codecov product."]},{"start":2019,"end":2020,"image":"moment.png","name":"<a href=\'https://inthemoment.io/\' target=\'_blank\'>Moment</a> - <small>first engineering hire</small>","tags":["industry","startups","scalability","big query","node js","startups"],"description":["Moment\'s first product has over 8 million active users, and I maintained the backend infrastructure. I was involved in daily product discussions, where I aim to keep a lean and focused product. Moment\'s vision is to reduce wasted screentime and instead help users focus on their real-life relationships."]},{"start":2018,"end":2019,"image":"adrich.jpg","name":"<a href=\'https://www.adrich.io\' target=\'_blank\'>Adrich</a> - <small>founding software engineer</small>","tags":["industry","startups","node js","react js","ios","android","bluetooth","multi-tenancy","java","objective c"],"description":["Adrich graduated from <a target=\'_blank\' href=\'https://alphalabgear.org/\'>AlphaLab Gear</a> and <a target=\'_blank\' href=\'https://www.plugandplaytechcenter.com/\'>Plug & Play Accelerator</a> in Silicon Valley. I was a founding software engineer, responsible for architecting our services and building the Android and iOS apps."]},{"start":2017,"end":2018,"image":"expii.png","name":"<a href=\'https://www.expii.com/\' target=\'_blank\'>Expii</a> - <small>second hire</small>","tags":["industry","startups","flask","react js"],"description":["I was a full-stack developer for Carnegie Mellon math professor <a target=\'_blank\' href=\'https://www.poshenloh.com/\'>Po-Shen Loh\'s</a> math and science startup."]},{"start":2015,"end":2016,"image":"nasa.png","name":"<a href=\'https://www.nasa.gov/centers/armstrong/home/index.html\' target=\'_blank\'>NASA</a> - <small>software engineer</small>","tags":["industry","java","flight school"],"description":["In total, I spent about a year and a half at NASA\'s Armstrong Flight Research Facility at Edwards Air Force Base. I was working on <a target=\'_blank\' href=\'https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20160007770.pdf\'>ADS-B flight software</a> through <a href=\'https://technology-afrc.ndc.nasa.gov/featurestory/tech-awards\'>UAS in the NAS</a>."]},{"image":"tunessence.png","end":2014,"name":"<a href=\'http://archive.jsonline.com/business/hal-leonard-acquires-guitar-teaching-start-up-tunessence-b99492296z1-302168401.html\' target=\'_blank\'>Tunessence</a> - <small>first hire</small>","tags":["industry","startups"],"description":["I was the first hire at Tunessence while I was a student at Carnegie Mellon. <strong style=\'background-color:#ffff00\'>Tunessence was <a target=\'_blank\' href=\'https://www.innovationworks.org/companies-archive/tunessence-acquired-by-hal-leonard/\'>acquired by Hal Leonard</a>.</strong>"]}]},{"name":"Personal projects","items":[{"start":2022,"end":2022,"name":"<a target=\'_blank\' href=\'/posts/chinese-proverbs-with-airflow\'>Chinese Proverbs with Airflow</a>","tags":["archived projects","airflow","chinese"],"description":["During a week of experimentation with Airflow, I built a Chinese Proverb generator. The aim was to understand what is and isn\'t possible using Airflow."]},{"start":2022,"end":2022,"name":"<a target=\'_blank\' href=\'https://github.com/robertkotcher/goml2/blob/master/main.go\'>goml2: Decision Trees</a>","tags":["archived projects","machine learning","golang"],"description":["Over time I hope to build a comprehensive machine learning library in Go. So far, I have implemented Decision Trees with Cost-Complexity Pruning, both for classification and regression.","At <a target=\'_blank\' href=\'https://synthesis.ai\'>Synthesis AI</a> I shared my work so far at a knowledge exchange meeting, where we talk about things we\'re learning about outside of work. Here are my slides from that: <a target=\'_blank\' href=\'goml2.pdf\'>Decision Trees with Goml2</a>"]},{"start":2021,"end":2021,"image":"lambda.png","name":"<a target=\'_blank\' href=\'https://www.npmjs.com/package/js-to-lambda\'>Javascript to λ-calculus transpiler</a>","tags":["archived projects","computer science"],"description":["This transpiler can take a Javascript function and convert it to λ-calculus. It currently operates on a subset of the Javascript language, but I\'m curious to see how far it can be taken.","This is a project that I\'ve open-sourced and published to npm."]},{"start":2019,"end":2019,"image":"npm.png","name":"<a target=\'_blank\' href=\'https://www.npmjs.com/package/multi-tenant-migrate\'>Multi-Tenant Migrate</a>","tags":["archived projects","multi-tenancy","postgresql","node js"],"description":["I built a multi-tenant migration system that works with node.js and postgresql. It was modeled after <a target=\'_blank\' href=\'https://django-tenant-schemas.readthedocs.io/en/latest/use.html\'>django-tenant-schemas</a> (a multi-tenant migration system for Django)."]},{"start":2018,"end":2018,"image":"quadratic.png","name":"<a target=\'_blank\' href=\'https://v1.expii.com/editor/create/explanation/9019\'>Math and Science Editor</a>","tags":["archived projects","react js"],"description":["I built this application for Expii.com as a standalone project. I wanted to list it here because it\'s something I\'m particularly proud of that showcases my front-end development skillset. Note that you\'ll have to connect with Facebook first."]},{"start":2020,"end":2022,"image":"profile.png","name":"<a target=\'_blank\' href=\'/\'>Personal Website</a>","tags":["archived projects","vue js"],"description":["I did a rewrite of my personal website using <a href=\'_blank\'>Vue.js</a> to get a break from using React.js. You can find a link to the source code <a target=\'_blank\' href=\'https://bitbucket.org/rkotcher/personal-site-vue/src/master/\'>here</a>."]},{"start":2012,"end":2012,"image":"spatianator.png","name":"<a target=\'_blank\' href=\'https://youtu.be/WYzedx83qYM\'>Spatianator</a>","tags":["archived projects","microcontroller","raspberry pi"],"description":["This is an art project that showcases the little bit of work that I\'ve done with hardware. We built four robots (this video shows only one) that communicated over a network and \'activated\' the sounds characteristic of the space they were in."]}]},{"name":"Essays","items":[{"start":2020,"end":2020,"image":"rnn.png","name":"<a target=\'_blank\' href=\'/posts/recurrent-neural-networks\'>Fun with Recurrent Neural Networks</a>","tags":["archived projects","deep learning","recurrent neural networks"],"description":["I walk through the architecture of two different recurrent neural networks and describe their use cases."]},{"start":2020,"end":2020,"image":"cryptokitty.png","name":"<a target=\'_blank\' href=\'/posts/deep-cryptokitties\'>Deep Cryptokitties</a>","tags":["archived projects","deep learning","general adversarial networks"],"description":["In this post I explore the various parameters of simple general adversarial networks (GANs), using cryptokitties as inspiration."]},{"start":2020,"end":2020,"image":"svm.png","name":"<a target=\'_blank\' href=\'/posts/large-margin-classifiers\'>Intuition behind large margin classification</a>","tags":["archived projects","machine learning","svm"],"description":["This essay lays the groundwork for understanding large margin classifiers (Support Vector Machines). It describes both the motivation as well as a geometric interpretation."]},{"start":2020,"end":2020,"image":"nn.png","name":"<a target=\'_blank\' href=\'/posts/vanilla-neural-network\'>I made a vanilla neural network from scratch</a>","tags":["archived projects","machine learning","deep learning"],"description":["This project started with the intention to calculate a few derivatives and translate those over to python code. In the end, I learned so much that it seemed worthwhile to write down every last detail to ensure that my future self would be able to revisit what I did without any confusion."]},{"start":2019,"end":2019,"image":"svd.png","name":"<a target=\'_blank\' href=\'/posts/singular-value-decomposition\'>Singular Value Decomposition: Two Perspectives</a>","tags":["archived projects","linear algebra"],"description":["SVD breaks a matrix into two matrics of orthonormal vectors and an inner matrix of singular values. I wanted to explore what that really means."]}]},{"name":"Publications","items":[{"start":2014,"end":2014,"image":"oauth.png","name":"<a href=\'https://www.researchgate.net/publication/266022550_OAuth_Demystified_for_Mobile_Application_Developers\'>OAuth Demystified for Mobile Applications</a>","tags":["archived projects","security","oauth"],"description":["This paper, which I worked on with researchers from Carnegie Mellon and Microsoft, studies OAuth\'s wide industry adoption and vulnerabilities that have been opened as a result of loose interpretation of the protocols."]},{"start":2013,"end":2013,"image":"timing_attack.png","name":"<a href=\'https://www.semanticscholar.org/paper/Cross-origin-pixel-stealing%3A-timing-attacks-using-Kotcher-Pei/66a6b8b5086454d2f511089ed3c157075239eb7d?navId=paper-header\'>Cross-Origin Pixel Stealing: Timing Attacks Using CSS Filters</a>","tags":["archived projects","security","timing attacks"],"description":["There are a number of older articles that I decided against including on this page, but I couldn\'t resist this timing attack that I discovered. It has been highly cited since publication."]}]},{"name":"Miscellaneous","items":[{"start":1988,"name":"Travel Map (distinct visits)","image":"pct-of-countries.png","tags":["travel"],"description":["\x3c!-- GENERATE % CHART AT https://copyicon.com/generator/svg-gauge-chart --\x3e","<iframe width=\'100%\' height=\'408\' seamless frameborder=\'0\' scrolling=\'no\' src=\'https://docs.google.com/spreadsheets/d/e/2PACX-1vQs7-o9mY_ol8i4CLCxOi7QRC66z5apxFMj7Mdh-6CG2aLz6QVfBncsTn0eYsr7z04EmqBNh4obesZx/pubchart?oid=1486894346&amp;format=interactive\'></iframe></div>","<small>Note: Some of these numbers are approximate. Visits are capped at 10. I consider that I have \\"lived\\" in a location if I have spent at least six cumulative months there.</small>"]},{"start":2021,"name":"<a target=\'_blank\' href=\'https://robertkotcher.github.io/blog\'>Language learning blog</a>","tags":["languages","spanish","chinese"],"description":["我每周写关于一个新想法","Wǒ měi zhōu xiě guānyú yīgè xīn xiǎngfǎ","Cada semana escribo sobre algo."]},{"start":2014,"end":2014,"name":"<a target=\'_blank\' href=\'https://vigilantaerospace.com/wp-content/uploads/2017/06/Application-of-an-ADS-B-Sense-and-Avoid-Algorithm_AFRC-E-DAA-TN30918_20160007770_opt.pdf\'>Federal Laboratory Consortium, Denver</a>","tags":["archived projects","java"],"description":["Selected for the Outstanding Technology Development award for work on ADS-B software at NASA."]},{"start":2014,"end":2014,"name":"<a target=\'_blank\' href=\'https://evernote.com/security/report-issue\'>Evernote Hall of Fame</a>","tags":["archived projects","security","oauth"],"description":["Received bug bounty for a mobile Oauth attack on Evernote."]},{"start":2014,"end":2014,"name":"<a target=\'_blank\' href=\'https://www.facebook.com/whitehat/thanks/\'>Facebook Hall of Fame</a>","tags":["archived projects","security","oauth"],"description":["Received bug bounty for a mobile Oauth attack on Facebook."]},{"start":2012,"end":2012,"image":"millennial_masters.png","name":"Millennial Masters, Vol 3","tags":["archived projects","music composition"],"description":["In addition to software, I write and perform classical music (violin). Here\'s a link to my <a target=\'_blank\' href=\'https://open.spotify.com/track/4c7kpmISQTpYX0adLJaOeb?si=75c9da02d70d4044\'>Violin Sonata No 1</a>, performed by <a target=\'_blank\' href=\'http://www.incontrimusicali.com/current-artists/2017/6/28/emma-steele-violin-usa\'>Emma Steele</a> and <a target=\'_blank\' href=\'https://www.cmu.edu/cfa/music/people/Bios/sargsyan_vahan.html\'>Vahan Sargsyan</a>. You can find other recordings on my <a target=\'_blank\' href=\'https://www.youtube.com/user/rkotcher/videos\'>Youtube channel</a>."]}]}]')},"4b1e":function(e,t,a){"use strict";a("9fd6")},"56d7":function(e,t,a){"use strict";a.r(t);var i=a("2b0e"),n=a("8c4f"),o=a("f309");i["a"].use(o["a"]);var s=new o["a"]({}),r=function(){var e=this,t=e._self._c;return t("div",{class:{centered:e.shouldCenter()},attrs:{id:"root"}},[t("router-view")],1)},l=[];function h(e){return e.location.search.substr(1).split("&").reduce((e,t)=>{if(t){const a=t.split("=");e[a[0]]=a[1]}return e},{})}function c(e,t){const a=e.location.origin,i=e.location.pathname,n=Object.keys(t),o=n.reduce((e,a,i)=>(0==e.length&&(e+="?"),e+=`${a}=${t[a]}`,i<n.length-1&&(e+="&"),e),""),s=`${a}${i}${o}`;e.history.pushState({},"",s)}const p={state:{activeTag:null,possibleTags:null,tagCounts:{}},setPossibleTags:function(e){var t={};e.forEach(e=>{e.items.forEach(e=>{e.tags.forEach(e=>{t[e]?t[e]+=1:t[e]=1})})}),this.state.possibleTags=Object.keys(t).sort((e,a)=>t[a]-t[e]),this.state.tagCounts=t},setActiveTag:function(e){c(window,{tag:e}),this.state.activeTag=e},clearActiveTag:function(){c(window,{}),this.state.activeTag=null}};var d=p,u={name:"App",data:()=>({tagStore:d}),methods:{shouldCenter:function(){return"/"===window.location.pathname&&!d.state.activeTag}}},m=u,g=(a("0e71"),a("2877")),f=Object(g["a"])(m,r,l,!1,null,null,null),w=f.exports,b=function(){var e=this,t=e._self._c;return t("div",[t("Header"),e.tagStore.state.activeTag?t("Body",{attrs:{sections:e.sections}}):e._e()],1)},_=[],v=a("4726"),y=function(){var e=this,t=e._self._c;return t("div",{attrs:{id:"header"}},[e.tagStore.state.activeTag?t("div",{staticClass:"chip-container-small"},[t("a",{staticClass:"header-title-small",attrs:{href:"/"}},[e._v("Robert Kotcher")]),t("Chip",{attrs:{small:"",primary:"",tag:"projects (past year)",text:"projects (past year)",selected:e.isActiveTag("projects")}}),t("Chip",{attrs:{small:"",primary:"",tag:"education",text:"education",selected:e.isActiveTag("education")}}),t("Chip",{attrs:{small:"",primary:"",tag:"startups",text:"startups",selected:e.isActiveTag("startups")}}),t("Chip",{attrs:{small:"",primary:"",tag:"travel",text:"travel",selected:e.isActiveTag("languages")}}),t("Chip",{attrs:{small:"",secondary:"",tag:"archived projects",text:"archived projects",selected:e.isActiveTag("archived projects")}})],1):e._e(),e.tagStore.state.activeTag?e._e():t("div",{staticClass:"header-container"},[e._m(0),t("div",{staticClass:"intro-panel animation-panel-second"},[t("div",{staticClass:"header-title"},[e._v("Robert Kotcher")]),t("p",{staticClass:"header-text"},[e._v(" Computers, lean startup methodology, travel, sushi. ")]),e._m(1),t("div",{staticClass:"chip-container"},[t("Chip",{attrs:{primary:"",tag:"projects (past year)",text:"projects (past year)"}}),t("Chip",{attrs:{primary:"",tag:"education",text:"education"}}),t("Chip",{attrs:{primary:"",tag:"startups",text:"startups"}}),t("Chip",{attrs:{primary:"",tag:"travel",text:"travel"}}),t("Chip",{attrs:{secondary:"",tag:"archived projects",text:"archived projects"}})],1)])])])},A=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"image-panel animation-panel-first"},[t("img",{attrs:{src:a("3cee")}})])},function(){var e=this,t=e._self._c;return t("div",{staticClass:"link-container"},[t("a",{attrs:{target:"_blank",href:"https://huggingface.co/rkotcher"}},[t("img",{staticClass:"header-link",attrs:{src:a("5e5e")}})]),t("a",{attrs:{target:"_blank",href:"https://github.com/robertkotcher"}},[t("img",{staticClass:"header-link",attrs:{src:a("cb79")}})]),t("a",{attrs:{target:"_blank",href:"https://www.linkedin.com/in/robert-kotcher-639105196/"}},[t("img",{staticClass:"header-link",attrs:{src:a("5db6")}})])])}],k=function(){var e=this,t=e._self._c;return t("span",{class:{chip:!0,primary:e.primary,secondary:e.secondary,selected:e.selected,small:e.small}},[t("a",{class:{primary:e.primary,secondary:e.secondary},attrs:{href:e.generateLink(e.tag)}},[e._v(" "+e._s(e.text)+" ")])])},x=[],I={name:"Chip",props:{text:{type:String},tag:{type:String},url:{type:String},primary:{type:Boolean},secondary:{type:Boolean},selected:{type:Boolean},small:{type:Boolean}},methods:{setTag:function(e){d.setActiveTag(e)},generateLink:function(e){return"/?tag="+e}}},T=I,C=(a("ca46"),Object(g["a"])(T,k,x,!1,null,"6c85e1b6",null)),j=C.exports,S={name:"Header",methods:{setTag:function(e){d.setActiveTag(e)},isActiveTag:function(e){return e==d.state.activeTag}},data:()=>({tagStore:d}),components:{Chip:j}},z=S,B=(a("1652"),Object(g["a"])(z,y,A,!1,null,"56bd4cec",null)),E=B.exports,W=function(){var e=this,t=e._self._c;return e.tagStore.state.activeTag?t("div",{attrs:{id:"body"}},[t("FilterMessage",{attrs:{count:e.itemsByTag(e.sections).length,tag:e.tagStore.state.activeTag}}),e._l(e.itemsByTag(e.sections),(function(a,i){return t("div",{key:i,attrs:{name:a.name}},[a.end?t("div",{staticClass:"year"},[e._v(" completed "+e._s(a.end)+": ")]):e._e(),t("Item",{attrs:{item:a}})],1)}))],2):e._e()},N=[],q=function(){var e=this,t=e._self._c;return t("div",{staticClass:"item-container"},[t("img",{staticClass:"item-image",attrs:{src:a("1771")("./"+e.item.image),alt:"Item Image"}}),t("div",{staticClass:"item-text-panel"},[t("h3",{staticClass:"font-weight-bold item-header",domProps:{innerHTML:e._s(e.item.name)}}),e._l(e.item.description,(function(a,i){return t("div",{key:i,staticClass:"item-text"},[t("span",{domProps:{innerHTML:e._s(a)}})])})),t("div",{staticClass:"item-tag-container"},e._l(e.item.tags,(function(a,i){return t("Chip",{key:i,attrs:{small:"",secondary:"",tag:a,text:a,selected:e.isActiveTag(a)},on:{click:function(t){return e.setActiveTag(a)}}})})),1)],2)])},V=[],O={name:"Item",methods:{setActiveTag:e=>d.setActiveTag(e),isActiveTag:function(e){return e==d.state.activeTag}},components:{Chip:j},props:{item:{type:Object,default:()=>{}}}},D=O,P=(a("a2f0"),Object(g["a"])(D,q,V,!1,null,"f3f48df2",null)),R=P.exports,F=function(){var e=this,t=e._self._c;return e.tagStore.state.activeTag?t("span",{staticClass:"filter-message"},[t("span",{staticClass:"filter-text"},[e._v("Showing "+e._s(e.count)+" results for tag "),t("strong",[e._v(e._s(e.tag))]),e._v(".")])]):e._e()},L=[],M={name:"FilterMessage",methods:{removeTag:()=>d.clearActiveTag()},data:()=>({tagStore:d}),props:{tag:{type:String},count:{type:Number},sections:{type:Array,default:()=>[]}}},X=M,U=(a("2ab4"),Object(g["a"])(X,F,L,!1,null,"6880f3da",null)),H=U.exports,Y={name:"Body",components:{FilterMessage:H,Item:R},methods:{itemsByTag:e=>{e=JSON.parse(JSON.stringify(e));const t=e.reduce((e,t)=>(t.items.forEach(t=>{let a=!1;t.tags.forEach(e=>{e==d.state.activeTag&&(a=!0)}),a&&e.push(t)}),e),[]);t.sort((e,t)=>(e.end||(e.end=(new Date).getFullYear()),t.end||(t.end=(new Date).getFullYear()),t.end-e.end));var a=null;return t.forEach(e=>{e.end!=a&&e.end!=(new Date).getFullYear()?a=e.end:delete e.end}),t}},data:()=>({tagStore:d}),props:{sections:{type:Array,default:()=>[]}}},K=Y,G=(a("9e25"),Object(g["a"])(K,W,N,!1,null,"f9c8eb22",null)),$=G.exports;d.setPossibleTags(v);const J=h(window);J.tag&&d.setActiveTag(window.decodeURI(J.tag));var Z={name:"Home",components:{Header:E,Body:$},data:()=>({sections:v,tagStore:d})},Q=Z,ee=Object(g["a"])(Q,b,_,!1,null,null,null),te=ee.exports,ae=a("2bc5"),ie=function(){var e=this,t=e._self._c;return t("div",{attrs:{id:"post-wrapper"}},[t(ae["a"],{staticClass:"px-0",attrs:{items:e.items}}),t("router-view")],1)},ne=[];function oe(e){return e.replace(/_|-/g," ").replace(/(^|\s)\S/g,e=>e.toUpperCase())}var se={name:"PostWrapper",data:function(){const e=this.$route.path.split("/").pop();return{items:[{text:"Home",link:!0,href:"/"},{text:oe(e),disabled:!0}]}}},re=se,le=(a("b9b5"),Object(g["a"])(re,ie,ne,!1,null,null,null)),he=le.exports,ce=function(){var e=this;e._self._c;return e._m(0)},pe=[function(){var e=this,t=e._self._c;return t("div",[t("h2",[e._v("Motivation for large margin classifiers")]),t("p",[e._v(" Remember that with logistic regression we assume that training data is generally linearly separable and our objective is to find a decision boundary between them. Of course, if there are outliers logicstic regression may still work well enough, and there are interesting hacks that can be used to apply even logistic regression to more than two classes. ")]),t("p",[e._v(" As a result, it turns out that logistic regression could probably be used in many of the cases that one may choose a large margin classifier, but the latter is computationally less expensive and may provide a cleaner and more accurate decision boundary. ")]),t("p",[e._v(" Large margin classifiers are probably always chosen over logistic regression in practice, but it is helpful to describe them by formulating logistic regression first. ")]),t("p",[e._v(" (Note that large margin classifiers are commonly called Support Vector Machines, but for consistency I continue to refer to them as large margin classifiers throught the rest of this page.) ")]),t("h2",[e._v("Logistic regression")]),t("p",[e._v(" Logistic regression gives us a function by which we can learn to predict whether or not a data point belongs to class 0 or 1: ")]),t("p",[e._v(" $$h(\\theta) = \\frac{1}{1 + e^{-\\theta^Tx}}$$ ")]),t("p",[e._v(" \\(h(\\theta)\\) is a good predictor if \\(\\theta^Tx\\) is really big when y (the true class label) is equal to 1, and \\(\\theta^Tx\\) is really small when y is equal to 0. ")]),t("p",[e._v(" The loss incurred given input example \\(x^i\\) and a set of parameters \\(\\theta\\) is: ")]),t("p",[e._v(" $$J(\\theta) = -\\Big(y^i \\cdot log(h_{\\theta}(x^i)) + (1 - y^i) \\cdot log(1 - h_{\\theta}(x^i))\\Big)$$ ")]),t("p",[e._v(" For example, if \\(\\theta^Tx\\) is really big, \\(h(\\theta)\\) ends up predicting that y = 1. \\(log(1) = 0\\), so the total loss given these particular parameters for this training example \\(i\\) is 0. ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"200px"},attrs:{src:a("b23b")}}),t("p",{staticStyle:{width:"300px",color:"#888"}},[e._v("Loss function for logistic regression in the case where y = 1.")])]),t("p",[e._v(" Even in the case where \\(h(\\theta)\\) correctly predicts y = 1, training data will continue to affect the output of the loss function. More interestingly, data points that lie further from the decision boundary have a much larger affect on the loss function. ")]),t("h2",[e._v("Large margin classification")]),t("p",[e._v(' Often, hinge loss is used in place of logistic loss to change the cost distribution. As long as \\(\\theta^Tx\\) is "good enough", it will no longer change the loss. Likewise, the cost grows only linearly as \\(\\theta^Tx\\) becomes further from the correct prediction. ')]),t("p",[e._v(" Another property of the hinge loss is its computational efficiency. ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"200px"},attrs:{src:a("87ad")}}),t("p",{staticStyle:{width:"300px",color:"#888"}},[e._v("The green, \\(cost_1\\) line is our new cost function.")])]),t("p",[e._v(" This brings us to our loss function for large margin classifiers: ")]),t("p",[e._v(" $$J(\\theta) = C \\sum_{i=1}^{m}\\Big(y^i \\cdot cost_1(\\theta^Tx^i) + (1 - y^i)cost_0(\\theta^Tx^i)\\Big) + \\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j$$ ")]),t("div",{staticStyle:{display:"flex","justify-content":"center","margin-bottom":"20px"}},[t("ul",[t("li",[e._v("When \\(y = 1\\), we want \\(\\theta^Tx\\) to be >= 1")]),t("li",[e._v("When \\(y = 0\\), we want \\(\\theta^Tx\\) to be <= -1")])])]),t("p",[e._v(" C helps us to control how much each training example effects the total loss: By choosing a larger C our there is more motivation to overfit our model to the training data. ")]),t("p",[e._v(" Also note the use of L2 regularization: \\(\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j\\). In the next section, we'll see why this, paired with our constraints on \\(\\theta^Tx\\), leads to a more intuitive decision boundary. ")]),t("h2",[e._v("Large margin intuition")]),t("p",[e._v(" We'll use the following known properties in this section: ")]),t("ol",[t("li",[e._v(" \\(u^Tv = p \\cdot \\Vert u \\Vert\\), where \\(p\\) is the length of the projection of \\(v\\) onto \\(u\\) ")]),t("li",[e._v(" \\(\\Vert u \\Vert = \\sqrt{u_1^2 + u_2^2}\\) ")])]),t("p",[e._v(" Consider the fact that we used L2 regularization in our objective function. By applying the second of these properties: ")]),t("p",[e._v(" $$\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j = \\frac{1}{2} \\Big(\\sqrt{\\theta_0^2 + \\theta_1^2}\\Big)^2 = \\frac{1}{2} \\Vert \\theta \\Vert^2$$ ")]),t("p",[e._v(" Remember that during optimization, we want \\(\\theta^Tx\\) to be >= 1. If our regularization term is weighted sufficiently (with a small enough C), we will be focused more on minimizing \\(\\theta\\) and less on fitting the decision boundary to each training instance. Therefore, during optimization we will be maximizing the projection of x onto \\(\\theta\\) because for \\(p \\cdot \\Vert \\theta \\Vert >= 1\\) to hold true, \\(p\\) will have to be very large. ")]),t("p",[e._v(" It's pretty hard to draw this relationship in a way that ends up being convincing, but I'll do my best. The first illustration shows a decision boundary that a large margin classifier would probably find: ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"200px"},attrs:{src:a("17c6")}}),t("p",{staticStyle:{width:"300px",color:"#888"}},[e._v("The solid green line represents the vector \\(\\theta\\). The dotted green line represents the resulting decision boundary. The magenta line shows that the training example with the shortest projection onto \\(\\theta\\) has been maximized.")])]),t("p",[e._v(" Each line is described in the caption, but the main point here is that for each of the training examples, the classifer is attempting to maximize its projection onto \\(\\theta\\). As a result, we get this imaginary decision boundary that is far from each of the training examples. ")]),t("p",[e._v(" To see an example of a bad classification, see the image below: ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"200px"},attrs:{src:a("3013")}}),t("p",{staticStyle:{width:"300px",color:"#888"}},[e._v("The magenta line here is the nearest projection given the same training examples, but a different \\(\\theta\\) vector. The dashed green line makes it clear which example is projecting.")])]),t("p",[e._v(" This example, while still perfectly separating all of the training examples, is still not very good one. That's because the projections of many of the training examples onto \\(\\theta\\) are much smaller than they were in the illustration above. ")])])}],de={name:"Main"},ue=de,me=(a("4623"),Object(g["a"])(ue,ce,pe,!1,null,"79d6fb9a",null)),ge=me.exports,fe=function(){var e=this;e._self._c;return e._m(0)},we=[function(){var e=this,t=e._self._c;return t("div",[t("h1",[e._v("Singular Value Decomposition: Two Perspectives")]),t("p",[e._v(" The equality \\(A = U \\Sigma V^{T}\\) is one of most important in linear algebra. In this essay, I want to make SVD a little more intuitive. I'll try to do so with examples, visualizations, and the corresponding math for completeness. ")]),t("p",[e._v(" I'll look at SVD in the following contexts: ")]),t("ol",[t("li",[e._v("Approximating a rank r matrix with a rank k matrix.")]),t("li",[e._v("Understanding the relationships between the four fundamental subspaces.")])]),t("h2",[e._v("1. Approximating a rank r matrix with a rank k matrix")]),t("p",[e._v(" In the first perspective a matrix of rank r is broken down into a set of rank 1 matrices. ")]),t("p",[e._v(" To solve \\(A = U \\Sigma V^{T}\\) we split the problem into two parts, solving for \\(U\\) in the first and \\(V\\) in the second, by diagonalizing \\(AA^{T}\\) and \\(A^{T}A\\). In both cases, we're diagonalizing a positive semidefinite matrix, and we end up with two equivalent orthonormal matrices of eigenvectors and one eigenvalue matrix. For example, finding the columns of \\(V\\) would involve the following: ")]),t("p",[e._v(" $$A^{T}A = V \\Sigma^{T} U^{T} U \\Sigma V^{T} = V \\Sigma^2 V^{T}$$ ")]),t("p",[e._v(" What does this really mean? Because \\(V \\Sigma^2 V^{T}\\) diagonalizes \\(A^{T}A\\), each element \\(v_i\\) is an eigenvector. Similarly, each \\(u_i\\) is an eigenvector of \\(AA^{T}\\). Suppose \\(A\\) is rank r. Then the following are true: ")]),t("ol",[t("li",[e._v("\\(v_1 \\cdots v_r\\) form an orthonormal basis.")]),t("li",[e._v("\\(u_1 \\cdots u_r\\) form an orthonormal basis.")]),t("li",[e._v("\\(r\\) is equal to the number of non-zero diagonal values of \\(\\Sigma\\).")])]),t("p",[e._v(" All vectors of \\(V\\) and \\(U\\) beyond the first r comprise the nullspace, and their corresponding diagonal entries in \\(\\Sigma\\) are zero. ")]),t("p",[e._v(" We can express A as a sum of r rank 1 matrices, each the outer product of a left singular vector, it's singular value, and the corresponding right singular vector: ")]),t("p",[e._v("$$A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$")]),t("p",[e._v("Suppose we start with the following image matrix A:")]),t("div",{staticClass:"image-container"},[t("img",{attrs:{src:a("fa4e")}})]),t("p",[e._v(" If I run SVD on A, and set all of the items on the diagonal of \\(\\Sigma\\) to 0 except for the most significant \\(\\sigma_1\\), and then render the resulting recombination, I end up with the following: ")]),t("div",{staticClass:"image-container"},[t("img",{attrs:{src:a("df8a")}})]),t("p",[e._v(" In this first matrix, each row is really just the combination of the first left singular vector, the first singular value, and the first right singular vector: \\(A' = \\sigma_1 u_1 v_1^T\\). ")]),t("p",[e._v(" Taking a look at the first 10 rank 1 matricies gives us: ")]),t("div",{staticClass:"image-container"},[t("img",{attrs:{src:a("2823")}})]),t("p",[e._v(" We can now start to see many of the features of the original image! Each singular vector of \\(U\\) and \\(V\\) encodes information from the original matrix \\(A\\). ")]),t("h2",[e._v("2. Understanding the relationships between the four fundamental subspaces")]),t("p",[e._v(" Let's instead look at \\(A\\) as a function from \\(R^n \\rightarrow R^m\\). Because \\(U\\) is orthogonal with sides of length m and \\(V\\) is orthogonal with sides of length n, we can represent any \\(x\\) as \\(Vc\\), where \\(c\\) describes how to make \\(x\\) out of basis \\(V\\). ")]),t("p",[e._v(" Looking at \\(A\\) in this way, we can write \\(Ax = U \\Sigma V^T Vc = U \\Sigma c\\): ")]),t("div",{staticClass:"image-container"},[t("img",{attrs:{src:a("9ad3")}})]),t("p",[e._v(" So \\(x = Vc\\) and \\(Ax = U \\Sigma c\\). The input, a vector in \\(R^n\\), is transformed to \\(R^m\\) via the function that is \\(A\\). ")]),t("p",[e._v(" The number of non-zero values in \\(c\\) is equal to the rank of both \\(A\\) and \\(A^T\\). When calculating the output \\(Ax\\), the remaining \\(m-r\\) zero entries in \\(c\\) will cancel out the last \\(m-r\\) columns of \\(U\\). ")]),t("p",[e._v(" Most importantly, this brings to light the four fundamental subspaces that are players in the \\(Ax\\) operation: ")]),t("ol",[t("li",[e._v("The "),t("strong",[e._v("nullspace of \\(A^T\\)")]),e._v(' is a subspace of \\(R^m\\) that is "unreachable" during the operation \\(Ax\\). It\'s the last \\(m-r\\) columns of \\(U\\) in our SVD and can never be reached because \\(\\Sigma c \\in R^m\\), and the \\(r-m\\) final vector positions are zeros.')]),t("li",[e._v("The "),t("strong",[e._v("nullspace of \\(A\\)")]),e._v(" is a subspace of \\(R^n\\). Similarly here, changes in this subspace don't matter because they'll ultimately be killed by the zero entries of \\(\\Sigma\\). We'll call this the \"doesn't matter\" space.")]),t("li",[e._v("The "),t("strong",[e._v("columnspace of A")]),e._v(' is a subspace of \\(R^m\\). In the SVD view, the column space is the part of \\(U\\) that can actually be manipulated, i.e., the "reachable space".')]),t("li",[e._v("The "),t("strong",[e._v("rowspace of A")]),e._v(' is a subspace of \\(R^n\\), and is the part of \\(x\\) that will affect the output \\(Ax\\). It\'s the "matters" space.')])]),t("p",[e._v("To sum up this perspective, SVD can be used as a tool to illustrate the fundamental subspaces of \\(A\\). When \\(A\\) is viewed as a function, it can help us to understand what changes in the input will make an inpact on the output \\(Ax\\).")]),t("small",[e._v("Robert Kotcher, 2019")])])}],be={name:"Main"},_e=be,ve=(a("4b1e"),Object(g["a"])(_e,fe,we,!1,null,"a8ae26b0",null)),ye=ve.exports,Ae=function(){var e=this;e._self._c;return e._m(0)},ke=[function(){var e=this,t=e._self._c;return t("div",[t("h1",[e._v("I made a Vanilla Neural Network from Scratch")]),t("h2",[e._v("Motivation")]),t("p",[e._v(" As I continue to grow my understanding of deep learning, it seems an inevitable rite of passage to build a functional neural network without the help of libraries. I have tinkered with the likes of Keras and Tensorflow, always amazed by the results and still yet somehow unfulfilled knowing that the brilliance behind what was actually happening was inside the black box. ")]),t("p",[e._v(" This project started with the intention to calculate a few derivatives and translate those over to python code. In the end, I learned so much that it seemed worthwhile to write down every last detail to ensure that my future self would be able to revisit what I did without any confusion. ")]),t("h2",[e._v("Neural networks are functions")]),t("p",[e._v(' I\'ll use the words "neural network" and "function" interchangeably in this writeup. The operations that together make up a neural network form a computational graph when expressed visually. We\'ll be walking through the example graph that is depicted below. It\'s actually pretty hard to illustrate every last computational detail visually, and I try to elaborate in words where the imagery is lacking. ')]),t("p",[e._v(' In general, inputs to a neural network consist of some data that you are trying to understand, the function reshapes that data such that the output can be provide insight into the input. For just a few examples, the output can classify the input ("this is a picture of a cat") or it can predict the next value in a sequence ("My favorite movie is The Jedi Strikes ___"). ')]),t("p",[e._v(' For now, it actually doesn\'t matter a whole lot what we expect our example network to do. We can assume that the data we use is arbitrary. This example is used more to show the mechanics neural networks, how to measure the success of their output, and how to optimize (or "train") their parameters in order to improve output accuracy. ')]),t("p",[e._v(" I mentioned that a neural network is just a function. Let's first look at what exactly this function looks like, and then we'll talk about how to \"teach\" this function to learn our task at hand. ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"350px"},attrs:{src:a("f92b")}}),t("p",{staticStyle:{width:"500px",color:"#888"}},[e._v("A neural network with input \\(X\\), two bias parameters \\(B_1\\) and \\(B_2\\), a hidden layer with three neurons, and output \\(O\\). Edges connecting \\(X\\) to \\(H\\), and \\(H\\) to \\(O\\) each contain a weight. The image above only shows \\(w_1\\).")])]),t("p",[e._v(' The image above is a representation of a pretty straightforward function. It takes two input scalars, passes them to a "hidden layer", which in turn spits out two output scalars. This same thing can be concisely represented with the function: ')]),t("p",[e._v("$$f(\\theta, X) = \\phi_2(\\phi_1(XW_1 + B_1)W_2 + B_2)$$")]),t("p",[e._v(' The \\(\\theta\\) stands for "all of the parameters involved in calculating the output of this function", and will be omitted from the function signature for brevity. ')]),t("h5",[e._v("Expressing the transformation from \\(X\\) to \\(H\\) in matrix notation")]),t("p",[e._v(" Let's move across the network from left to right and see how this relates to the underlying math. ")]),t("p",[e._v("The \\(X\\) matrix contains the inputs to our function. In this example it has one row and two columns. ")]),t("p",[e._v(" $$X = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}$$ ")]),t("p",[e._v(' You can see that \\(X\\) is multiplied by \\(W_1\\). I allude to \\(W_1\\) in the image above with \\(w_1\\), but \\(W_1\\) (capital "W") is a matrix containing the \\(w_1\\) edge weight together with 5 others. ')]),t("p",[e._v(" $$W_1 = \\begin{bmatrix}w_1 & w_3 & w_5\\\\w_2 & w_4 & w_6\\end{bmatrix}$$ ")]),t("p",[e._v('You\'ll often see a "bias" value, in this case \\(B_1\\), which is used to translate the hidden state away from the origin. In other words, bias values are added to the matrix that results from multiplying input values by the weight parameters. ')]),t("p",[e._v(" $$B_1 = \\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\end{bmatrix}$$ ")]),t("p",[e._v(" Let's put together the first half of this network, which takes us from the input \\(X\\) to the hidden layer: ")]),t("p",[e._v(" $$XW_1 + B_1 = \\begin{bmatrix}x_1 & x_2\\end{bmatrix} \\begin{bmatrix}w_1 & w_3 & w_5\\\\w_2 & w_4 & w_6\\end{bmatrix} + \\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\end{bmatrix} = \\begin{bmatrix}h_1 & h_2 & h_3\\end{bmatrix} = H$$ ")]),t("h5",[e._v("Roles of each player")]),t("p",[e._v(" Each of the players mentioned so far have different roles, and I want to make sure I'm particularly clear on what those are before moving forward. ")]),t("p",[e._v(' \\(X\\), \\(O\\), and \\(H\\) are, for lack of a better word, "placeholders" in the computational graph. For each execution of the function they will be filled in based on the current input. ')]),t("p",[e._v(' The weight matrices \\(W_1\\) and \\(W_2\\), as well as biases \\(B_1\\) and \\(B_2\\) are referred to as "variables", or maybe more commonly "parameters" or "weights". These players stay the same between executions. It\'s our goal to figure out the best parameter values during the training process. Typically these are "initialized" with random numbers. ')]),t("h5",[e._v("Activation functions")]),t("p",[e._v(' Returning to our function, we now arrive at the "hidden layer": \\(H = \\begin{bmatrix}h_1 & h_2 & h_3\\end{bmatrix}\\). There\'s one really important thing to note here: Each neuron \\(h_i\\) in the hidden layer is actually comprised of two numbers. The first is the immediate result of the matrix expression just mentioned. We\'ll refer to that as the "net" output. The second is the "activated" output. We\'ll call that "out" for brevity. I won\'t use the names \\(H\\) or \\(h_i\\) anymore because we actually need to be a little more specific. This section will explain how we\'ll refer to the hidden layer for the remainder of the writeup. ')]),t("p",[e._v(' You might have noticed that our equation above contained \\(\\phi_1\\) and \\(\\phi_2\\). This is where neural networks really get their power from. Both of these \\(\\phi\\)s are functions too, but they are a little different from what we\'ve seen so far because they are non-linear. We call them "activation" functions because they activate the output of the linear operations inside of them. (An important thing to note about activation functions is that they must be differentiable.) ')]),t("p",[e._v(" There are lots of types of activation functions. For this example, we'll use the logistic activation function \\(\\phi(x) = \\frac{1}{1 + e^{-x}}\\). ")]),t("p",[e._v(" We said that \\(H = XW_1 + B_1\\), which is true. But let's be more specific and say that \\(XW_1 + B_1 = H_{net} = \\begin{bmatrix}h_{net,1} & h_{net,2} & h_{net,3}\\end{bmatrix}\\) of net values. To this we apply the activation function to each item individually for the final out matrix of the hidden layer \\(H_{out} = \\begin{bmatrix}h_{out,1} & h_{out,2} & h_{out,3}\\end{bmatrix} = \\begin{bmatrix}\\phi(h_{net,1}) & \\phi(h_{net,2}) & \\phi(h_{net,3})\\end{bmatrix}\\). ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"220px"},attrs:{src:a("de2b")}}),t("p",{staticStyle:{width:"500px",color:"#888"}},[e._v('\\(h_{out,1}\\), or the "activated" first element \\(h_1\\) in the hidden layer is calculated by applying the logistic activation function to \\(h_{net,1}\\).')])]),t("p",[e._v(" The operation described here is to be done three times when going from the input layer \\(X\\) to the hidden layer \\(H\\), once for each \\(h_i\\). Referring back to the entire function \\(O = \\phi_2(\\phi_1(XW_1 + B_1)W_2 + B_2)\\), \\(h_{out,1}\\) ends up being the first element of \\(\\phi_1(XW_1 + B_1)\\). Expanding the matrix operations we get \\(h_{out,1} = \\phi((x_1w_1 + x_2w_2) + b_{1,1})\\). ")]),t("p",[e._v(" Likewise, \\(h_{out,2} = \\phi((x_1w_3 + x_2w_4) + b_{1,2})\\) and \\(h_{out,3} = \\phi((x_1w_5 + x_2w_6) + b_{1,3})\\). ")]),t("h5",[e._v("Hidden layer to output layer")]),t("p",[e._v(" We want to turn the \\(H_{out}\\) that we just calculated into the output layer \\(O\\). The output layer can also be though of in two parts, \\(O_{net}\\) and \\(O_{out}\\). ")]),t("p",[e._v(" To get to \\(O_{net}\\), we similarly multiply by a weight matrix \\(W_2\\), which has 3 rows and 2 columns of weight parameters, and add a bias \\(B_2\\) to the result. Finally we apply a logistic activation function element-wise to get to our final network output, \\(O_{out}\\). ")]),t("p",[e._v(' \\(O_{out}\\) contains the "final output" of our function. ')]),t("h2",[e._v("Calculating loss")]),t("p",[e._v(' Because the values of our parameters \\(W_1\\), \\(W_2\\), \\(B_1\\), and \\(B_2\\) were initialized randomly, the output of our function will, at first, be random as well. This is where the concept of "loss" comes in. Loss is a measure of how well a function is able to match the provided training label \\(T\\) for a particular data point. A label is really just another matrix, generally of the same shape as \\(O_{out}\\). By running both the label and output for a given training example through a loss function \\(E\\), we can get a numeric value that tells us how well our function performed on that data point. ')]),t("p",[e._v(" Let's use the squared error for this example. For an arbitrary training example with label \\(T = \\begin{bmatrix}t_{1} & t_{2}\\end{bmatrix}\\) we can calculate the squared loss as: ")]),t("p",[e._v(" $$E_{total}(\\theta) = \\sum_{i=1}^2\\frac{1}{2}(t_i - o_{out,i})^2$$ ")]),t("p",[e._v(" Notice that the error relies on \\(\\theta\\), which is really just a placeholder for all of the parameters in the model. \\(\\theta\\) again will be omitted in the future. Our goal is to figure out how to change all of the individual matrix values to minimize \\(E_{total}\\). ")]),t("h2",[e._v("Minimizing error using gradient descent")]),t("p",[e._v(" Up until this point we've seen that a neural network is just a function whose output accuracy is measured by a loss function. Next, we want to train the model parameters to minimize what the loss function returns. ")]),t("p",[e._v(" Gradient descent is a technique that is used to update paramater weights. By taking the derivative of the total error with respect to each parameter individually, we can determine how that parameter affects the total error. Consequently, this gives us insight on how to change the weight to decrease the total error. ")]),t("p",[e._v(" For example, to update the parameter \\(w_7\\), which is the edge connecting \\(h_1\\) and \\(o_1\\) above, you would perform the following update: ")]),t("p",[e._v(" $$w_7 := w_7 - \\alpha\\Big(\\frac{\\delta E_{total}}{\\delta w_7}\\Big)$$ ")]),t("p",[e._v(' \\(\\alpha\\) is called the "learning rate". It\'s just a parameter that helps determine how quickly the parameters are updated. A value too large may explode, and a value too small might take forever to converge. ')]),t("p",[e._v(" In the upcoming subsections, we'll work through the math required to perform updates on \\(w_7\\) and \\(w_1\\), although the same principle should be used to update all of the function's parameters. ")]),t("h5",[e._v("Updating weight \\(w_7\\)")]),t("p",[e._v(" Let's first locate where weight \\(w_7\\) even is in our computational graph. In the very first image up at the top I actually don't label it, but it's the edge connecting \\(h_1\\) with \\(o_1\\). To perform the update as described, we need to determine \\(\\frac{\\delta E_{total}}{\\delta w_7}\\). ")]),t("p",[e._v(" Breaking this down using the chain rule gives us: ")]),t("p",[e._v(" \\(\\frac{\\delta E_{total}}{\\delta w_7} = \\frac{\\delta E_{total}}{\\delta o_{out,1}} \\frac{\\delta o_{out,1}}{\\delta o_{net,1}} \\frac{\\delta o_{net,1}}{\\delta w_7} \\) ")]),t("p",[e._v(" You can see that each of these derivatives are stepping backwards from the error to the weight being updated, which is why this step is called backpropagation. One of the nice features of backpropagation is that it gives us modular blocks that we can recycle when we're finding gradients higher up in the graph. We'll see this when we update \\(w_1\\) next. ")]),t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"220px"},attrs:{src:a("9643")}}),t("p",{staticStyle:{width:"500px",color:"#888"}},[e._v("The error that our network currently outputs for \\(o_1\\) is \\(\\frac{1}{2}(t_1 - o_{out,1})^2\\)")])]),t("p",[e._v(" In order for us to find \\(\\frac{\\delta E_{total}}{\\delta w_7}\\) let's first individually solve for each item in the chain rule decomposition. ")]),t("p",[e._v(" \\(\\frac{\\delta E_{total}}{\\delta o_{out,1}}\\): ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(E_{total} = \\frac{1}{2}(t_1 - o_{out,1})^2 + \\frac{1}{2}(t_2 - o_{out,2})^2\\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta E_{total}}{\\delta o_{out,1}} = -(t_1 - o_{out,1})\\). ")]),t("p",[e._v(" \\(\\frac{\\delta o_{out,1}}{\\delta o_{net,1}}\\): ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(o_{out,1} = \\frac{1}{1 + e^{-o_{net,1}}}\\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta o_{out,1}}{\\delta o_{net,1}} = o_{out,1}(1 - o_{out,1})\\) ")]),t("p",[e._v(" \\(\\frac{\\delta o_{net,1}}{\\delta w_7}\\): ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\( o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1} \\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta o_{net,1}}{\\delta w_7} = h_{out,1}\\) ")]),t("p",[e._v(" And the complete update to \\(w_7\\) would be: ")]),t("p",[e._v(" $$w_7 := w_7 - \\alpha\\Big( -(o_{out,1})(h_{out,1})(t_1 - o_{out,1})(1 - o_{out,1}) \\Big)$$ ")]),t("p",[e._v(" How is this actually used in practice? After a given training example is passed through the function, the resulting values for each of these variables are then used to update \\(w_7\\). In an actual coded implementation, for example, the values ending up in each of these variables would be cached during forward propagation so that they could be used to calculate the gradient during backpropagation. ")]),t("h5",[e._v("Updating weight \\(w_1\\)")]),t("p",[e._v(" Updating \\(w_1\\) is really similar to updating \\(w_7\\), the main difference being that \\(w_1\\) affects two components of \\(E_{total}\\): \\(E_1\\) and \\(E_2\\). ")]),t("p",[e._v(" $$ \\frac{\\delta E_{total}}{\\delta w_1} = \\frac{\\delta E_{total}}{\\delta h_{out,1}} \\frac{\\delta h_{out,1}}{\\delta h_{net,1}} \\frac{\\delta h_{net,1}}{\\delta w_1} $$ ")]),t("p",[e._v(" $$ \\frac{\\delta E_{total}}{\\delta w_1} = \\Big( \\frac{\\delta E_1}{\\delta h_{out,1}} + \\frac{\\delta E_2}{\\delta h_{out,1}} \\Big) \\frac{\\delta h_{out,1}}{\\delta h_{net,1}} \\frac{\\delta h_{net,1}}{\\delta w_1} $$ ")]),t("p",[e._v(" We'll find each of these partial derivatives, but note that we've already done these before for the most part when we did our updates for the weights in the hidden to output layer. ")]),t("p",[e._v(" \\(\\frac{\\delta E_1}{\\delta h_{out,1}}\\): ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta E_1}{\\delta h_{out,1}} = \\frac{\\delta E_1}{\\delta o_{out,1}} \\frac{\\delta o_{out,1}}{\\delta o_{net,1}} \\frac{\\delta o_{net,1}}{\\delta h_{out,1}}\\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta E_1}{\\delta o_{out,1}} \\frac{\\delta o_{out,1}}{\\delta o_{net,1}} = -o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\). ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1}\\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta o_{net,1}}{\\delta h_{out,1}} = w_7\\) ")]),t("p",{staticStyle:{"margin-left":"30px"}},[e._v(" \\(\\frac{\\delta E_1}{\\delta h_{out,1}} = -w_7o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\) ")]),t("p",[e._v(" \\(\\frac{\\delta E_2}{\\delta h_{out,1}} = -w_{10}o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\) ")]),t("p",[e._v(" \\(\\frac{\\delta h_{out,1}}{\\delta h_{net,1}} = h_{out,1}(1 - h_{out,1})\\) ")]),t("p",[e._v(" \\(\\frac{\\delta h_{net,1}}{\\delta w_1} = x_1\\) ")]),t("p",[e._v(" Inserting all of these partial derivatives into the larger expression above will give us how we should update \\(w_1\\) for this training example. ")]),t("p",[e._v(" And there we have it! To see this neural network in action, you can check out the "),t("a",{attrs:{href:"./simple_nn_from_scratch.py",download:""}},[e._v("corresponding code")]),e._v(". ")]),t("small",[e._v("Robert Kotcher, 2019")])])}],xe={name:"Main"},Ie=xe,Te=(a("18dd"),Object(g["a"])(Ie,Ae,ke,!1,null,"0d57277a",null)),Ce=Te.exports,je=function(){var e=this;e._self._c;return e._m(0)},Se=[function(){var e=this,t=e._self._c;return t("div",[t("h1",[e._v("Deep CryptoKitties")]),t("p",[e._v(" Back when Ethereum was popular I wanted to own a "),t("a",{attrs:{target:"_blank",href:"https://www.cryptokitties.co/about"}},[e._v("CryptoKitty")]),e._v(" like all of the cool kids did. I remember spending about $30 on a completely unique, second generation CryptoKitty, and enjoying that for about 5 minutes before completely forgetting about it. ")]),t("div",{staticClass:"image-container"},[t("img",{staticClass:"cryptokitty",attrs:{src:a("e528")}}),t("small",[e._v("My 2nd generation CryptoKitty")])]),t("p",[e._v(" For those who are unfamiliar with the project, a CryptoKitty is essentially a blockchain-based game where users can breed CryptoKitties that they own to unlock new traits. It comes with the whole proof-of-ownership guarantee that blockchains provide, but that's a whole different discussion. ")]),t("p",[e._v(" In this post, I resurrect my interest in CryptoKitties. The objective here is to build a general adversarial network (GAN) to learn what a CryptoKitty is and generate new and unique creatures by studying existing ones. ")]),t("h2",[e._v(" Data creation ")]),t("p",[e._v(" As this was the first time I had ever built a GAN, I decided to simplify the input data to a level I thought would be reasonably challenging without risking not having enough memory/compute power/knowledge to get results worth posting about. ")]),t("p",[e._v(" A quick check of the "),t("a",{attrs:{href:"https://cryptokitties.co"}},[e._v("cryptokitties.co")]),e._v("source code revealed that I'd be able to fetch images by querying "),t("a",{attrs:{href:"https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg"}},[e._v("https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg")]),e._v(", and replacing the image name (1831008.svg) with randomly-generated numbers. ")]),t("p",[e._v(" Because the images were SVG, I had to use a command-line tool to both convert to PNG and select a fixed size. Not knowing the scale by which adding a few more pixels would increase memory/compute requirements, I decided to be conservative and set each image at 200x200. ")]),t("p",[e._v(" Finally I reduced the number of channels from 3 to 1. ")]),t("h2",[e._v(" GAN overview ")]),t("p",[e._v(" At a high level, a GAN consists of two deep neural networks that are being optimized at the same time: ")]),t("ul",[t("li",[e._v(" The "),t("strong",[e._v("generator")]),e._v(" tries to create output that matches the distribution of the training data, given some arbitrary input. In this example, out input data is random and normally distributed. ")]),t("li",[e._v(" The "),t("strong",[e._v("discriminator")]),e._v(" tries to determine whether or not an input image is real or fake. ")])]),t("br"),t("p",[e._v(" The following code snippet is the final version of my "),t("strong",[e._v("generator")]),e._v(": ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(25*25*400, use_bias=False, input_shape=(1500,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((25, 25, 400)))\n    assert model.output_shape == (None, 25, 25, 400) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(100, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 50, 50, 100)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(50, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 100, 100, 50)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 200, 200, 1)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    return model\n")]),t("p",[e._v(" What's happening here is that this network takes as input a batch of training data that has shape [None, 1500]. Remember that this data is normally distributed, and the generator's goal is to learn how to redistribute the data so that it matches the distribution of the training data. ")]),t("p",[e._v(' Something that I wanted to point out is that the Conv2DTranspose "spreads" the data from a lower to a higher dimensionality, which is how we\'re able to go from a shape of (25, 25, 400) to a shape of (50, 50, 100). This operation is sometimes misleadingly referred to as a "deconvolution". Notice how the strides actually multiply the length and width of each layer, rather than dividing. ')]),t("p",[e._v(" The following code snippet is the final version of my "),t("strong",[e._v("discriminator")]),e._v(": ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("def make_discriminator_model():\n    model = tf.keras.Sequential()\n\n    # 64 because 64 filters were used, 100 bc stride was 2\n    # From docs: if Conv2D is the first layer of the network, the sample axis\n    # should not be included in \"input_shape\"\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[200, 200, 1]))\n    assert model.output_shape == (None, 100, 100, 64)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    assert model.output_shape == (None, 50, 50, 128)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    assert model.output_shape == (None, 25, 25, 256)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n")]),t("p",[e._v(" The discriminator is composed of more standard convolution, activation, and normalization layers. The objective on the discriminator is, given an image, to determine whether it is real or fake. ")]),t("p",[e._v(" As I monitored the loss of this network during training I found that it was almost always monotonically decreasing, therefore I found myself spending much less time tweaking the hyperparameters of this network. ")]),t("p",[e._v(" Speaking of loss functions - here are the loss functions: ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n")]),t("p",[e._v(" We know the discriminator did well if "),t("strong",[e._v("real_output")]),e._v(" is all ones and "),t("strong",[e._v("fake_output")]),e._v(" is all zeros, i.e., it is able to correctly determine which images are real and which are fake. ")]),t("p",[e._v(" We know the generator did well if it was able to trick the discriminator, which equates to the discriminator assigning a higher value for a fake input. ")]),t("h2",[e._v("Results")]),t("p",[e._v(" At first, both networks perform randomly. For example, here is the generator's output from a number of different runs, prior to the first epoch of training. ")]),t("div",{staticClass:"image-container"},[t("div",{staticClass:"image-container-row"},[t("img",{staticClass:"example-image",attrs:{src:a("8c12")}}),t("img",{staticClass:"example-image",attrs:{src:a("923d")}})]),t("small",[e._v("Normally distributed random data across two runs")])]),t("p",[e._v(" I was trying to think of the most insightful way to group and describe images that were generated during various training attempts. There are so many variants of each network (depth, dimensionality, activation, bias, dropout, etc) that I ran that it quickly became an overwhelming task to organize them in an informative and fun way - so I now give to you some generator outputs in no particular order: ")]),t("div",{staticClass:"image-container"},[t("div",{staticClass:"image-container-row"},[t("img",{staticClass:"example-image",attrs:{src:a("9175")}}),t("img",{staticClass:"example-image",attrs:{src:a("8f3b")}}),t("img",{staticClass:"example-image",attrs:{src:a("2349")}}),t("img",{staticClass:"example-image",attrs:{src:a("9e99")}})]),t("small",[e._v("Examples of generator output during training")])]),t("p",[e._v(" A key breakthrough that I made was that, although increasing the depth and dimensionality of the generator network itself did change the its accuracy, the single most impactful change I made was to increase the dimensionality of the input data. In my final and most successful run, where each training sample had 1500 normally-distributed numbers, I was able to achieve the following results: ")]),t("div",{staticClass:"image-container"},[t("div",{staticClass:"image-container-row"},[t("img",{staticClass:"example-image",attrs:{src:a("12d6")}}),t("img",{staticClass:"example-image",attrs:{src:a("41bf")}}),t("img",{staticClass:"example-image",attrs:{src:a("d0d7")}}),t("img",{staticClass:"example-image",attrs:{src:a("57da")}})]),t("small",[e._v(" Although the input data is random, I was able to increase output quality significantly just by increasing its dimensionality. ")])])])}],ze={name:"Main"},Be=ze,Ee=(a("02c9"),Object(g["a"])(Be,je,Se,!1,null,"1996c03d",null)),We=Ee.exports,Ne=a("0798"),qe=function(){var e=this,t=e._self._c;return t("div",[t("h1",[e._v("Fun with Recurrent Neural Networks")]),t("p",[e._v(" Recurrent Neural Networks (RNNs) are useful for making sense of sequential data in a way that would be more difficult for a standard feed-forward neural network. ")]),t("p",[e._v(" The name RNN is a broad term used to describe many network implementations that exhibit temporal awareness. Some RNNs can be unrolled to look like a feedforward network due to the fact that the length of the input sequence being evaluated is finite, however the main difference here is that, in an RNN, a new piece of input data is introduced at each layer and combined with an internal state that has been passed along from previous states. ")]),t("p",[e._v(" Vanilla RNNs (Elman network, Jordan network, for example) have drawbacks that make them not very useful in practice, however I am going to step through two variants and use cases of the Elman network to build a foundation upon which more modern architectures can be built. ")]),t("h3",[e._v("Simple network overview")]),t("p",[e._v(" It is assumed that you have some ordered sequence of data \\(X\\) that you want to train your network with. Examples of data types that work nicely with RNNs include, but are not limited to characters, numbers, and words in an ordered sequence. We will see both types throughout this writeup. ")]),e._m(0),t("p",[e._v(" We'll be iterating through the sequence, one item at a time. ")]),t("p",[e._v(' At step \\(t\\) of sequence \\(X\\), we want to "introduce" new information to the network that will persist through the remainder of the timesteps. We introduce the information by combining the current information (represented by \\(h_t\\), or the "state" at step \\(t\\)) with the new input \\(x_t\\). ')]),t("p",[e._v(" At time step 0 the state is usually initialized with zeros. ")]),t("p",[e._v(" Before choosing a particular implementation of our simple RNN, there are a few basic questions that need to be considered: What is the desired output? Are we using the network for generation or for classification? What is the dimensionality of the data we're working with? ")]),t("p",[e._v(" In this writeup, I'm going to put together two simple networks with the above topology, one for text generation and one for text classification, and describe how the implementation details change between the two. ")]),t("h2",[e._v("Network 1: Echo RNN")]),t("p",[e._v(' In the first network we consider an input pattern constructed from zeros and ones. When passed through the feed-forward pass of the network we wish to add a delay (or "echo") to the pattern. In effect, the network will learn to shift the input pattern by some number of timesteps. You could generate dummy data in the following way: ')]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("def generateData():\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n    y = np.roll(x, DELAY_STEPS)\n    y[0:DELAY_STEPS] = 0\n\n    x = x.reshape((batch_size, -1))\n    y = y.reshape((batch_size, -1))\n\n    return (x, y)\n")]),t("p",[e._v(' Now that we have some training data, we\'ll want to group it into minibatches in order to perform "minibatch gradient descent". This is worthy of another entire writeup, but putting training samples into small batches is a common training practice that, by assuming that the batch is distributed in the same way that your entire pool of training data is distributed, can speed up learning by performing the backward pass more frequently, all while decreasing the resources required. ')]),t("h3",[e._v("Input data and placeholders")]),t("p",[e._v(" The graph is first initialized with three placeholders, as shown in the code sample below. batchX_placeholder accepts a tensor containing a batch of the original (no echo) sequences, batchY_placeholder accepts a tensor containing a batch of the expected output (input delayed by DELAY_STEPS timesteps), and init_state, which is the state that is passed between each timestep in the sequence. ")]),t("p",[e._v(' Note that "truncated_backprop_length" refers to the number of steps in the sequence that we\'re training the network on. ')]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size])\n")]),t("p",[e._v(" Another way to understand these placeholders is to consider the shape and nature of the training examples that will be fed to them as they enter the network: ")]),t("p",[e._v(" Before building the matrices, let's create a dummy sequence of zeros and ones that we'll use to train our network: ")]),t("p",[e._v(" $$1 0 0 0 0 ... 0 1 0 0 0 ...$$ ")]),t("p",[e._v(" What we'll do, although in the end the exact details of this aren't super important as they're specific to this particular case study, is break up our sequence into batches. For our example, let's create two batches of 5 consecutive items, as shown above. ")]),t("p",[e._v(" We could create the following batched input \\(X\\) to the network: ")]),t("div",[e._v(" $$X = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\end{bmatrix}$$ ")]),t("p",[e._v(" \\(X\\) contains a batch of two training examples (batch size = 2), with truncated_backprop_length of 5. ")]),t("p",[e._v(' The corresponding "echoed" matrix \\(Y\\) would then look like this if, for example, DELAY_STEPS was set to 3: ')]),t("div",[e._v(" $$Y = \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{bmatrix}$$ ")]),t("p",[e._v(" Notice that we've just taken the items from \\(X\\) and shifted them 3 positions to the right. ")]),t("p",[e._v(" We finally pass in a state matrix that is initialized to all zeros, whose size is [batch_size state_size]: ")]),t("div",[e._v(" $$h_0 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$$ ")]),t("p",[e._v(" The size of the column axis of \\(h_0\\) (axis = 1) might seem arbitrary, and in some ways it is. In practice, if the state is too small the network might not have the capacity to learn the nature of the training data, i.e. as the complexities of the data being learned grows, so must the available state in order to learn it. On the other hand, too much state leads to inefficiencies with storage and computation. ")]),t("h3",[e._v("Variables")]),t("p",[e._v(" The variables introduced in this section will come into play throughout the remainder of this section of the writeup - I'm just presenting them now for reference. The values that they receive upon initialization is less important because these are the values that will be learned during training. I'll talk more about their dimensionality as we go. ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\nb = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n")]),t("h3",[e._v("Network architecture")]),t("p",[e._v(" This is a nice transition into the remainder of the computational graph, which combines these three placeholders to compute both the predicted output as well as loss. As before, let's start with the relevant code. ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("inputs_series = tf.unstack(batchX_placeholder, axis=1)\nlabels_series = tf.unstack(batchY_placeholder, axis=1)\n\ncurrent_state = init_state\nstates_series = []\nfor current_input in inputs_series:\n    current_input = tf.reshape(current_input, [batch_size, 1])\n    input_and_state_concatenated = tf.concat([current_input, current_state], 1)\n\n    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n    states_series.append(next_state)\n    current_state = next_state\n")]),t("p",[e._v(" For both the input (batchX_placeholder) and corresponding labels (batchY_placeholder), we split on the column and end with a list of tensors, each of size [batch_size 1]. In our dummy example, inputs_series would look like: ")]),t("p"),t("div",[e._v(" $$[\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, ...]$$ ")]),t("p",[e._v(" At each iteration of the loop, we combine the input with the most up-to-date state value, of size [batch_size, state_size]. For example, on the first iteration, we end up with: ")]),t("div",[e._v(" $$\\mathrm{input\\_and\\_state\\_concatenated = concat([x\\_0, h\\_0], axis=1)} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0\\end{bmatrix}$$ ")]),t("p",[e._v(" We then transition to the next state by matrix multiplying with \\(W\\). We must choose \\(W\\) in such a way that we again end up with a state matrix of shape [batch_size, state_size]. In other words, we want to find a shape [A, B] for \\(W\\) such that: ")]),t("div",[e._v(" $$[batch\\_size, state\\_size + 1] @ [A, B] = [batch\\_size, state\\_size]$$ ")]),t(Ne["a"],{attrs:{icon:"mdi-help",prominent:"",text:"",type:"info"}},[t("p",[e._v(' Lately I\'ve been using "@" to mean "matrix multiply" because '),t("strong",[e._v("Since python >= 3.5 the @ operator is supported (see PEP 465). In TensorFlow, it simply calls the tf.matmul() function...")])])]),t("p",[e._v(' It\'s clear that \\(W\\) should be of shape [state_size + 1, state_size], and that exactly what is done (see "Variables" above). ')]),t("h3",[e._v("Output layer")]),t("p",[e._v(" After iterating through all of the input values in input_series, we end up with truncated_backprop_length state matrices. Each of these matrices is then multiplied by \\(W_2\\). The purpose of \\(W_2\\) is to take the shape down to [batch_size, 2]. The same \\(W_2\\) is used for each multiplication within a batch. ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("logits_series = [tf.matmul(state, W2) + b2 for state in states_series]\n\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n\ntotal_loss = tf.reduce_mean(losses)\n")]),t("p",[e._v(" We end up with a truncated_backprop_length logits (weights that are intended to be converted to probabilities) of the following size: ")]),t("div",[e._v(" $$[ [batch\\_size, num\\_classes], ...]$$ ")]),t("p",[e._v(" After running each of the logits through a softmax, we end up with a series of probabilities, where the matrix at \\(i\\) gives the probability of each output per class, per batch index. So, using the \\(X\\) we defined with dummy data above, we might end up with the following predication_series: ")]),t("div",[e._v(" $$ [ \\left[ \\begin{array}{cc} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{array} \\right] , \\left[ \\begin{array}{cc} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{array} \\right] , \\left[ \\begin{array}{cc} 0.5 & 0.5 \\\\ 0.5 & 0.5 \\end{array} \\right] , \\left[ \\begin{array}{cc} 0.1 & 0.9 \\\\ 0.9 & 0.1 \\end{array} \\right] , \\left[ \\begin{array}{cc} 0.9 & 0.1 \\\\ 0.1 & 0.9 \\end{array} \\right] ] $$ ")]),t("p",[e._v(" To interpret this dummy output, the network expects that for the first batch the output at the 4th index is most likely to be a 1, followed by a 0. Conversely, for the second item in the batch, the 4th and 5th item in the sequence is expected to be a 0 followed by a 1. ")]),t("p",[e._v(" We could use these predictions to compute the loss ourselves, but since this is such a common series of operations, Tensorflow provides a function sparse_softmax_cross_entropy_with_logits, which basically takes the logits, computes the softmax over it, and then finds the cross-entropy loss. See the note on cross-entropy below. ")]),t(Ne["a"],{attrs:{icon:"mdi-help",prominent:"",text:"",type:"info"}},[t("p",[t("strong",[e._v("Cross-Entropy")]),e._v(" measures the performance of a classification model whose output is a probability value between 0 and 1. Loss increase the further away the predicted label is from the true label. ")]),t("p",[e._v(" $$H(p,q) = -\\sum_{i \\in C} p(i) log q(i)$$ ")]),t("p",[e._v(" where \\(p(i)\\) is the true class label for the \\(i^{th}\\) training example, \\(q(i)\\) is the predicted probability, and C is the number of classes. ")]),t("p",[e._v(" What this means is, for each training example, we only take into consideratihow close the probability of the true class is to 1. That means we can simplify to \\(H(p,q) = -log(q(i))\\). ")])]),t("p",[e._v(" The built-in function sparse_softmax_cross_entropy_with_logits will give us a per-batch index, per-sequence step loss. For example, ")]),t("div",[e._v(" $$[\\begin{bmatrix} 0.84 \\\\ 0.75 \\end{bmatrix}, \\begin{bmatrix} 0.75 \\\\ 0.8 \\end{bmatrix}, ...]$$ ")]),t("p",[e._v(" To find a single loss value, we find the mean across all axis, but may not be the only way to do so. ")]),t("h2",[e._v("Network 2: Classification network")]),t("p",[e._v(" We have just seen a network that produces N outputs for N inputs. This network is great when translating one sequence to another, but dosen't really make sense for the use case where a sequence is being classified because the output is non-sequential. ")]),t("p",[e._v(" I decided to take the Echo RNN and modify it to accept sequential data and output a one-hot vector containing a 1 in the position representing the class label. The particular use case that I had in mind was a language detector, however in the end I could have equivalently been measuring anything - it really depends on how you decide to label your training data. ")]),t("p",[e._v(" The architectural differences between this network and the Echo RNN actually ended up being subtle, but I did run into some unrelated challenges when it came to dealing with batch data and 1-hot encoded data, which I touch on briefly below. ")]),t("h3",[e._v("Classification network architecture")]),t("p",[e._v(' Recall how we maintain a states_series variable while looping over input_series in the Echo RNN from section 1. In the classification network, this is no longer necessary because our output is non-sequential (i.e. has a different shape) and information is passed across all timesteps. We can just pass the final state \\(h\\) to a "dense layer" to reduce its dimensionality. ')]),t("p",[e._v(" It's probably easiest to show the code for this in its entirety, and then after discuss the points of interest. ")]),t("pre",{staticClass:"prettyprint",attrs:{"data-lang":"python"}},[e._v("batchX_placeholder = tf.placeholder(tf.float32,\n    [batch_size, truncated_backprop_length, encoding_size])\nbatchY_placeholder = tf.placeholder(tf.int32,\n    [batch_size, num_classes])\n\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size, 1])\n\nWxh = tf.Variable(np.random.rand(batch_size, state_size, encoding_size), dtype=tf.float32)\nbh = tf.Variable(np.zeros((batch_size, state_size, 1)), dtype=tf.float32)\n\nWhh = tf.Variable(np.random.rand(batch_size, state_size, state_size), dtype=tf.float32)\n\nWhy = tf.Variable(np.random.rand(batch_size, num_classes, state_size), dtype=tf.float32)\nby = tf.Variable(np.zeros((batch_size, num_classes, 1)), dtype=tf.float32)\n\ninputs_series = tf.unstack(batchX_placeholder, axis=1)\n\nh = init_state\nfor x in inputs_series:\n    x = tf.reshape(x, [batch_size, encoding_size, 1])\n    h = tf.tanh(tf.matmul(Wxh, x) + tf.matmul(Whh, h) + bh)\n\ny_logits = tf.tanh(tf.matmul(Why, h) + by)\ny_logits = tf.reshape(y_logits, [batch_size, num_classes])\n")]),t("p",[e._v(" One challenge I ran into when dealing with text data was dealing with the extra dimension of encoding. In our Echo RNN, we were fortunate that our input data consisted of just 0s and 1s. ")]),e._m(1),t("p",[e._v(" Dealing with batches and one-hot data at the same time ended up being more straightforward than I thought it would be. It turns out that, although you can't matrix multiply a tensor with anything other than 2 dimensions, matrix multiplication in both Tensorflow and numpy only require that the inner two dimensions are compatible. ")]),t("p",[e._v(" For example, multiplying matrices with shapes [3, 4, 5] and [3, 5, 4] will result in a matrix with a shape of [3, 4, 4]. Matrix multiplying in this case would result in 3 matrix multiplies, one for each of the 3 inputs in the batch. ")]),t("h2",[e._v("The vanishing gradient")]),t("p",[e._v(' In the end, vanilla RNNs aren\'t really used in practice. The reason is that backpropagation across so many layers makes it difficult for the network to learn because the gradients become "vanishingly" small. ')]),t("p",[e._v(" Common solutions to the vanishing gradient problem include using LSTM or GRU networks, and I'll have to wait for another day to do a writeup on these ones. ")])],1)},Ve=[function(){var e=this,t=e._self._c;return t("div",{staticClass:"image-container",staticStyle:{display:"flex","flex-direction":"column","align-items":"center"}},[t("img",{staticStyle:{height:"150px"},attrs:{src:a("1fe7")}}),t("p",{staticStyle:{width:"500px",color:"#888"}},[e._v(" Figure 1: A high-level, details-omitted view of a simple RNN. For each item in sequence \\(X\\), \\(x_t\\), there is a corresponding state \\(h_t\\). Together, \\(x_t\\) and \\(h_t\\) can be used to determine the next state \\(h_{t+1}\\) and the output for that layer, \\(y_t\\). ")])])},function(){var e=this,t=e._self._c;return t("p",[e._v(" When dealing with text data however, you end up needing to encode each input with 1-hot vectors (or "),t("a",{attrs:{target:"_blank",href:"https://en.wikipedia.org/wiki/Word_embedding"}},[e._v("word embeddings")]),e._v("). ")])}],Oe={name:"Main"},De=Oe,Pe=(a("20ea"),Object(g["a"])(De,qe,Ve,!1,null,"25959adf",null)),Re=Pe.exports,Fe=function(){var e=this;e._self._c;return e._m(0)},Le=[function(){var e=this,t=e._self._c;return t("div",[t("h1",[e._v("Chinese Proverbs with Airflow")]),t("p",[e._v(" My current job involves generating AI data at scale in a pipeline. As we develop the system, the Apache Airflow project has come up in conversation quite a bit. Since I’d never used it before, I thought I’d give it a try. ")]),t("p",[e._v(" This project is a Chinese Proverb generator. Every day, it scans my Chinese blog to see what words I’m using and their frequencies. Then it reads a dataset of Chinese proverbs from "),t("a",{attrs:{href:"https://www.kaggle.com/datasets/bryanb/phrases-and-sayings",target:"_blank"}},[e._v("Kaggle")]),e._v(" and suggests one that introduces new words (but not too many new words). ")]),t("p",[e._v(" The objective was to understand what is possible with Airflow and where it falls short. Any efforts that did not meet that description were done quickly and quite possibly suboptimally. ")]),t("h2",[e._v("Airflow")]),t("p",[e._v(" Airflow is a platform for defining, running, and monitoring workflows. The workflows are defined as a "),t("a",{attrs:{target:"_blank",href:"https://en.wikipedia.org/wiki/Directed_acyclic_graph"}},[e._v("DAG")]),e._v(" containing many nodes - in airflow these are called tasks. A scheduler is responsible for understanding the relationships between tasks in a DAG, and triggering tasks when their parents have completed. A scheduler is also responsible for creating new task instances, i.e., for “kicking off” the DAG and orchestrating the instantiation of tasks. There are lots of ways to do this, but in this project I kick off the workflow once per day. In other words, my scheduler suggests one new Chinese proverb per day. ")]),t("p",[e._v(" Another important concept in Airflow is an operator, which is essentially a class that, when instantiated, becomes a task. Operators define different ways that data can be operated on, for example through a script, pod execution, or network call. In this project, I generally found that just running all tasks as a Python script was pretty easy, but it’s good to know that the flexibility of having other operator types exists. For example, there is an HttpOperator but I found that just using Python’s requests module was more intuitive for me. Whatever. ")]),t("h2",[e._v("Chinese Proverb Pipeline")]),t("p",[e._v(" The Chinese Proverb Pipeline is fairly straightforward, as depicted in the diagram below. "),t("strong",[e._v("fetch_known_words")]),e._v(" scans my "),t("a",{attrs:{target:"_blank",href:"https://robertkotcher.github.io/blog"}},[e._v("language blog")]),e._v(" for Chinese words that I've used in my writeups. "),t("strong",[e._v("fetch_proverbs")]),e._v(" fetches a CSV containing about 150 Chinese proverbs. "),t("strong",[e._v("choose_proverb")]),e._v(" combines the results of both of these operations and chooses a proverb that introduces new words, but not too many new words. Finally, "),t("strong",[e._v("save_proverb")]),e._v(" stores the proverb in an in-cluster data store. ")]),t("div",{staticClass:"image-container"},[t("img",{staticClass:"pipeline-diagram",attrs:{src:a("aab4")}})]),t("p",[e._v(" I used the "),t("a",{attrs:{target:"_blank",href:"https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html"}},[e._v("TaskFlow API")]),e._v(", which is part of Airflow 2. TaskFlow allows the developer to define tasks as Python functions with decorators. In the case that the developer wants to write a task that isn’t in the form of a Python function (which operator is used?), they can fall back to instantiating that Operator directly. It does other things like abstract out the inner-workings of "),t("a",{attrs:{target:"_blank",href:"https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"}},[e._v("Xcoms")]),e._v(". ")]),t("p",[e._v(" I found TaskFlow to be much easier and cleaner to work with. ")]),t("h3",[e._v("Design Choices")]),t("ul",[t("li",[e._v(" I wanted to be sure that, as the number of known Chinese words grew, that the mechanism that passes data between tasks would be able to handle the growing size of the frequency map. Airflow allows task instances to communicate between one another with xcom. xcom uses the “metadata DB” to store return values from one task instance, and reveal those return values to children. I set up my metadata DB using Postgres (required when running tasks on kubernetes), and found that there weren’t any size constraints on the DB column that would be storing the word table. ")]),t("li",[e._v(" I debated different ways that the daily chosen Proverb would be communicated to me. As this project was focused on learning Airtable, I thought that configuring the final task to send me an email would be a good exercise. This can be done by overriding the pod template for the final task with credentials (set in a secret). Since email providers are paid, and I didn't feel like entering credit card information anywhere, I built a simple key/value store, spun it up in my cluster, and used that to store the most recently chosen proverb. ")])]),t("h3",[e._v("Things I really like about working with Airflow")]),t("ul",[t("li",[e._v(" Being able to run task instances locally, either via a full workflow instance, or as a single task instance. ")]),t("li",[e._v(" The ease of setting up a workflow, especially with TaskFlow API. ")]),t("li",[e._v(" Configurability (the flexibility of being able to run the same workflow on different environments, with different underlying transport mechanisms) ")]),t("li",[e._v(" A really intuitive GUI for monitoring, that just works quite well out-of-the-box. ")])]),t("h3",[e._v("Frustrations")]),t("p",[e._v(" While I actually had no frustrations with Airflow (at least with this simple project), I found that DigitalOcean and Airtable don't always play together nicely. This isn’t really a limitation of Airflow. For example, volumes can only run in ReadWriteOnce accessMode. This has implications for larger clusters. For example, if I have many nodes, I have to be sure that pods belonging to the same airflow task are always schedule on the same node. ")]),t("h2",[e._v("The final product")]),t("p",[e._v("Finally, here’s the pipeline in action. The following proverb was generated at 8 am UTC, based on the words I used in my blog:")]),t("div",{staticClass:"proverb-container"},[t("span",{staticClass:"proverb"},[e._v(" 今天的中国谚语是: 不作不死。 ")])])])}],Me={name:"Main"},Xe=Me,Ue=(a("e49c"),Object(g["a"])(Xe,Fe,Le,!1,null,"d3540cfa",null)),He=Ue.exports,Ye=[{path:"large-margin-classifiers",component:ge,meta:{title:"Large margin classifiers"}},{path:"singular-value-decomposition",component:ye,meta:{title:"Singular Value Decomposition"}},{path:"vanilla-neural-network",component:Ce,meta:{title:"I made a vanilla neural network from scratch"}},{path:"deep-cryptokitties",component:We,meta:{title:"Deep Cryptokitties"}},{path:"recurrent-neural-networks",component:Re,meta:{title:"Some Recurrent Neural Networks, just for fun"}},{path:"chinese-proverbs-with-airflow",component:He,meta:{title:"Chinese Proverb generator with Airflow"}}],Ke=Ye;i["a"].use(n["a"]),i["a"].config.productionTip=!1;const Ge=new n["a"]({mode:"history",routes:[{path:"/",component:te,meta:{title:"Robert Kotcher"}},{path:"/posts",component:he,children:Ke}]});Ge.beforeEach((e,t,a)=>{document.title=e.meta.title,a()}),new i["a"]({router:Ge,vuetify:s,render:e=>e(w)}).$mount("#root")},"57da":function(e,t,a){e.exports=a.p+"img/final-4.4fe516f3.png"},5932:function(e,t,a){e.exports=a.p+"img/kahuna-logo.3856b229.png"},"5b3b":function(e,t,a){e.exports=a.p+"img/svd.f3200088.png"},"5db6":function(e,t,a){e.exports=a.p+"img/li.ba045568.png"},"5e5e":function(e,t,a){e.exports=a.p+"img/hf.a1f7d9ec.png"},6819:function(e,t,a){e.exports=a.p+"img/quadratic.9d2016c3.png"},"6b1c":function(e,t,a){e.exports=a.p+"img/adrich.da0bf58e.jpg"},7265:function(e,t,a){e.exports=a.p+"img/codecov.b6dfe4ee.jpg"},"87ad":function(e,t,a){e.exports=a.p+"img/large-margin.b196f5e9.png"},"8c12":function(e,t,a){e.exports=a.p+"img/random-1.a7a4888d.png"},"8f3b":function(e,t,a){e.exports=a.p+"img/gen-2.2ce520b8.png"},9175:function(e,t,a){e.exports=a.p+"img/gen-1.3f00c017.png"},"923d":function(e,t,a){e.exports=a.p+"img/random-2.c4646f3f.png"},"94c7":function(e,t,a){e.exports=a.p+"img/npm.e7ec3ec0.png"},9643:function(e,t,a){e.exports=a.p+"img/w7.58f44988.png"},"965c":function(e,t,a){},9822:function(e,t,a){e.exports=a.p+"img/moment.616a10a3.png"},"9ad3":function(e,t,a){e.exports=a.p+"img/functionA.25b4df33.png"},"9b19":function(e,t,a){e.exports=a.p+"img/logo.63a7d78d.svg"},"9b6b":function(e,t,a){e.exports=a.p+"img/yt.0bf78019.png"},"9e25":function(e,t,a){"use strict";a("9ee6")},"9e99":function(e,t,a){e.exports=a.p+"img/gen-4.5f3e67e7.png"},"9ee6":function(e,t,a){},"9fd6":function(e,t,a){},a2f0:function(e,t,a){"use strict";a("965c")},a36a:function(e,t,a){e.exports=a.p+"img/kitchen_zen.6b036aba.png"},aab4:function(e,t,a){e.exports=a.p+"img/chinese_proverb.6be6cff3.png"},b23b:function(e,t,a){e.exports=a.p+"img/logistic-regression.00af903d.png"},b9b5:function(e,t,a){"use strict";a("ca72")},bcdc:function(e,t,a){e.exports=a.p+"img/pct-of-countries.a1ab8f7c.png"},bdeb:function(e,t,a){},c3e7:function(e,t,a){e.exports=a.p+"img/profile.918c4550.png"},c5ae:function(e,t,a){e.exports=a.p+"img/svm.4acc9e8f.png"},c74e:function(e,t){e.exports="data:image/png;base64,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"},c7ca:function(e,t,a){e.exports=a.p+"img/tunessence.dc5c1060.png"},ca46:function(e,t,a){"use strict";a("bdeb")},ca72:function(e,t,a){},cb79:function(e,t,a){e.exports=a.p+"img/gh.84c8be2f.png"},cbdf:function(e,t,a){e.exports=a.p+"img/travel-map.1564c9c8.png"},cf05:function(e,t,a){},cf053:function(e,t,a){e.exports=a.p+"img/logo.82b9c7a5.png"},d0d7:function(e,t,a){e.exports=a.p+"img/final-3.2b9af48f.png"},d116:function(e,t,a){e.exports=a.p+"img/nn.daa9842c.png"},d154:function(e,t,a){},db5a:function(e,t,a){},de2b:function(e,t,a){e.exports=a.p+"img/zoom.74e22543.png"},df8a:function(e,t,a){e.exports=a.p+"img/panama-1.82e5725f.png"},e49c:function(e,t,a){"use strict";a("d154")},e528:function(e,t,a){e.exports=a.p+"img/cryptokitty.0cbe12d5.png"},e750:function(e,t,a){e.exports=a.p+"img/timing_attack.4ab4d27c.png"},f11a:function(e,t,a){e.exports=a.p+"img/carnegie_mellon.4e4972d8.png"},f92b:function(e,t,a){e.exports=a.p+"img/basic-net.e9111ddb.png"},fa4e:function(e,t,a){e.exports=a.p+"img/panama.73980cde.jpg"}});
//# sourceMappingURL=app.62193afb.js.map