{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./src/posts/deep_cryptokitties/Main.vue?5dec","webpack:///./src/assets/cryptokitty.png","webpack:///./src/assets/rnn.png","webpack:///./src/components/Chip.vue?3393","webpack:///./src/posts/deep_cryptokitties/final-1.png","webpack:///./src/assets/nasa.png","webpack:///./src/assets sync ^\\.\\/.*$","webpack:///./src/posts/large_margin_classifiers/good.png","webpack:///./src/posts/vanilla_neural_network/Main.vue?9bbe","webpack:///./src/App.vue?c973","webpack:///./src/posts/rnns/vanilla.png","webpack:///./src/posts/rnns/Main.vue?494b","webpack:///./src/posts/deep_cryptokitties/gen-3.png","webpack:///./src/posts/singular_value_decomposition/panama-10.png","webpack:///./src/components/Home/FilterMessage.vue?0e9d","webpack:///./src/posts/large_margin_classifiers/bad.png","webpack:///./src/assets/background.png","webpack:///./src/assets/lambda.png","webpack:///./src/assets/profile.jpg","webpack:///./src/assets/home-minified.jpg","webpack:///./src/posts/deep_cryptokitties/final-2.png","webpack:///./src/assets/expii.png","webpack:///./src/assets/synthesis.png","webpack:///./src/assets/oauth.png","webpack:///./src/posts/large_margin_classifiers/Main.vue?de4b","webpack:///./src/posts/singular_value_decomposition/Main.vue?a812","webpack:///./src/plugins/vuetify.js","webpack:///./src/App.vue","webpack:///./src/util/url.js","webpack:///./src/TagStore.js","webpack:///src/App.vue","webpack:///./src/App.vue?2ef9","webpack:///./src/App.vue?315a","webpack:///./src/components/Home/Home.vue","webpack:///./src/components/Home/Header.vue","webpack:///./src/components/Chip.vue","webpack:///src/components/Chip.vue","webpack:///./src/components/Chip.vue?1f5a","webpack:///./src/components/Chip.vue?c311","webpack:///src/components/Home/Header.vue","webpack:///./src/components/Home/Header.vue?42b4","webpack:///./src/components/Home/Header.vue?1c36","webpack:///./src/components/Home/Body.vue","webpack:///./src/components/Home/Item.vue","webpack:///src/components/Home/Item.vue","webpack:///./src/components/Home/Item.vue?88b1","webpack:///./src/components/Home/Item.vue?eaad","webpack:///./src/components/Home/FilterMessage.vue","webpack:///src/components/Home/FilterMessage.vue","webpack:///./src/components/Home/FilterMessage.vue?19ff","webpack:///./src/components/Home/FilterMessage.vue?3955","webpack:///src/components/Home/Body.vue","webpack:///./src/components/Home/Body.vue?62d8","webpack:///./src/components/Home/Body.vue?89c4","webpack:///src/components/Home/Home.vue","webpack:///./src/components/Home/Home.vue?147a","webpack:///./src/components/Home/Home.vue?ccaf","webpack:///./src/components/Posts/PostWrapper.vue","webpack:///src/components/Posts/PostWrapper.vue","webpack:///./src/components/Posts/PostWrapper.vue?312e","webpack:///./src/components/Posts/PostWrapper.vue?6408","webpack:///./src/posts/large_margin_classifiers/Main.vue","webpack:///src/posts/large_margin_classifiers/Main.vue","webpack:///./src/posts/large_margin_classifiers/Main.vue?0d92","webpack:///./src/posts/large_margin_classifiers/Main.vue?e698","webpack:///./src/posts/singular_value_decomposition/Main.vue","webpack:///src/posts/singular_value_decomposition/Main.vue","webpack:///./src/posts/singular_value_decomposition/Main.vue?8476","webpack:///./src/posts/singular_value_decomposition/Main.vue?8c77","webpack:///./src/posts/vanilla_neural_network/Main.vue","webpack:///src/posts/vanilla_neural_network/Main.vue","webpack:///./src/posts/vanilla_neural_network/Main.vue?ebab","webpack:///./src/posts/vanilla_neural_network/Main.vue?05b6","webpack:///./src/posts/deep_cryptokitties/Main.vue","webpack:///src/posts/deep_cryptokitties/Main.vue","webpack:///./src/posts/deep_cryptokitties/Main.vue?42eb","webpack:///./src/posts/deep_cryptokitties/Main.vue?5632","webpack:///./src/posts/rnns/Main.vue","webpack:///src/posts/rnns/Main.vue","webpack:///./src/posts/rnns/Main.vue?e540","webpack:///./src/posts/rnns/Main.vue?8bd5","webpack:///./src/posts/chinese_proverbs_with_airflow/Main.vue","webpack:///src/posts/chinese_proverbs_with_airflow/Main.vue","webpack:///./src/posts/chinese_proverbs_with_airflow/Main.vue?3d4d","webpack:///./src/posts/chinese_proverbs_with_airflow/Main.vue?a69d","webpack:///./src/posts/index.js","webpack:///./src/components/Posts/PostSubrouter.js","webpack:///./src/main.js","webpack:///./src/posts/deep_cryptokitties/final-4.png","webpack:///./src/assets/svd.png","webpack:///./src/assets/li.png","webpack:///./src/assets/hf.png","webpack:///./src/components/Home/Header.vue?440b","webpack:///./src/assets/quadratic.png","webpack:///./src/assets/adrich.jpg","webpack:///./src/assets/codecov.jpg","webpack:///./src/posts/large_margin_classifiers/large-margin.png","webpack:///./src/posts/deep_cryptokitties/random-1.png","webpack:///./src/posts/deep_cryptokitties/gen-2.png","webpack:///./src/posts/deep_cryptokitties/gen-1.png","webpack:///./src/posts/deep_cryptokitties/random-2.png","webpack:///./src/assets/npm.png","webpack:///./src/posts/vanilla_neural_network/w7.png","webpack:///./src/assets/moment.png","webpack:///./src/posts/singular_value_decomposition/functionA.png","webpack:///./src/assets/logo.svg","webpack:///./src/components/Home/Body.vue?7b8a","webpack:///./src/posts/deep_cryptokitties/gen-4.png","webpack:///./src/components/Home/Item.vue?f323","webpack:///./src/assets/kitchen_zen.png","webpack:///./src/posts/chinese_proverbs_with_airflow/chinese_proverb.png","webpack:///./src/posts/large_margin_classifiers/logistic-regression.png","webpack:///./src/components/Posts/PostWrapper.vue?8971","webpack:///./src/assets/profile.png","webpack:///./src/assets/svm.png","webpack:///./src/assets/tunessence.png","webpack:///./src/assets/gh.png","webpack:///./src/assets/travel-map.png","webpack:///./src/assets/logo.png","webpack:///./src/posts/deep_cryptokitties/final-3.png","webpack:///./src/assets/nn.png","webpack:///./src/posts/vanilla_neural_network/zoom.png","webpack:///./src/posts/singular_value_decomposition/panama-1.png","webpack:///./src/posts/chinese_proverbs_with_airflow/Main.vue?cc3f","webpack:///./src/posts/deep_cryptokitties/cryptokitty.png","webpack:///./src/assets/timing_attack.png","webpack:///./src/assets/carnegie_mellon.png","webpack:///./src/posts/vanilla_neural_network/basic-net.png","webpack:///./src/posts/singular_value_decomposition/panama.jpg"],"names":["webpackJsonpCallback","data","moduleId","chunkId","chunkIds","moreModules","executeModules","i","resolves","length","Object","prototype","hasOwnProperty","call","installedChunks","push","modules","parentJsonpFunction","shift","deferredModules","apply","checkDeferredModules","result","deferredModule","fulfilled","j","depId","splice","__webpack_require__","s","installedModules","exports","module","l","m","c","d","name","getter","o","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","p","jsonpArray","window","oldJsonpFunction","slice","map","webpackContext","req","id","webpackContextResolve","e","Error","code","keys","resolve","Vue","use","Vuetify","render","_vm","this","_c","_self","class","centered","shouldCenter","attrs","staticRenderFns","getUrlParams","windowRef","location","search","substr","split","reduce","memo","val","tuple","setSingleUrlParam","params","origin","path","pathname","newUrl","history","pushState","tagStore","state","activeTag","possibleTags","tagCounts","setPossibleTags","sections","tags","forEach","section","items","item","tag","sort","a","b","setActiveTag","clearActiveTag","methods","component","_e","staticClass","_v","isActiveTag","_m","require","chip","primary","secondary","selected","small","on","$event","setTag","_s","text","props","type","String","url","Boolean","components","Chip","itemsByTag","_l","end","image","domProps","description","default","count","removeTag","Number","Array","FilterMessage","Item","JSON","parse","stringify","append","Date","getFullYear","lastYearSeen","TagStore","urlParams","decodeURI","Header","Body","convertSlugToTitle","slug","replace","toUpperCase","$route","pop","link","href","disabled","staticStyle","LargeMarginClassifiers","meta","title","SingularValueDecomposition","VanillaNeuralNetwork","DeepCryptokitties","Rnns","ChineseProverbsWithAirflow","routes","VueRouter","config","productionTip","router","Home","PostWrapper","children","PostSubrouter","beforeEach","to","from","next","document","vuetify","h","App","$mount"],"mappings":"aACE,SAASA,EAAqBC,GAQ7B,IAPA,IAMIC,EAAUC,EANVC,EAAWH,EAAK,GAChBI,EAAcJ,EAAK,GACnBK,EAAiBL,EAAK,GAIHM,EAAI,EAAGC,EAAW,GACpCD,EAAIH,EAASK,OAAQF,IACzBJ,EAAUC,EAASG,GAChBG,OAAOC,UAAUC,eAAeC,KAAKC,EAAiBX,IAAYW,EAAgBX,IACpFK,EAASO,KAAKD,EAAgBX,GAAS,IAExCW,EAAgBX,GAAW,EAE5B,IAAID,KAAYG,EACZK,OAAOC,UAAUC,eAAeC,KAAKR,EAAaH,KACpDc,EAAQd,GAAYG,EAAYH,IAG/Be,GAAqBA,EAAoBhB,GAE5C,MAAMO,EAASC,OACdD,EAASU,OAATV,GAOD,OAHAW,EAAgBJ,KAAKK,MAAMD,EAAiBb,GAAkB,IAGvDe,IAER,SAASA,IAER,IADA,IAAIC,EACIf,EAAI,EAAGA,EAAIY,EAAgBV,OAAQF,IAAK,CAG/C,IAFA,IAAIgB,EAAiBJ,EAAgBZ,GACjCiB,GAAY,EACRC,EAAI,EAAGA,EAAIF,EAAed,OAAQgB,IAAK,CAC9C,IAAIC,EAAQH,EAAeE,GACG,IAA3BX,EAAgBY,KAAcF,GAAY,GAE3CA,IACFL,EAAgBQ,OAAOpB,IAAK,GAC5Be,EAASM,EAAoBA,EAAoBC,EAAIN,EAAe,KAItE,OAAOD,EAIR,IAAIQ,EAAmB,GAKnBhB,EAAkB,CACrB,IAAO,GAGJK,EAAkB,GAGtB,SAASS,EAAoB1B,GAG5B,GAAG4B,EAAiB5B,GACnB,OAAO4B,EAAiB5B,GAAU6B,QAGnC,IAAIC,EAASF,EAAiB5B,GAAY,CACzCK,EAAGL,EACH+B,GAAG,EACHF,QAAS,IAUV,OANAf,EAAQd,GAAUW,KAAKmB,EAAOD,QAASC,EAAQA,EAAOD,QAASH,GAG/DI,EAAOC,GAAI,EAGJD,EAAOD,QAKfH,EAAoBM,EAAIlB,EAGxBY,EAAoBO,EAAIL,EAGxBF,EAAoBQ,EAAI,SAASL,EAASM,EAAMC,GAC3CV,EAAoBW,EAAER,EAASM,IAClC3B,OAAO8B,eAAeT,EAASM,EAAM,CAAEI,YAAY,EAAMC,IAAKJ,KAKhEV,EAAoBe,EAAI,SAASZ,GACX,qBAAXa,QAA0BA,OAAOC,aAC1CnC,OAAO8B,eAAeT,EAASa,OAAOC,YAAa,CAAEC,MAAO,WAE7DpC,OAAO8B,eAAeT,EAAS,aAAc,CAAEe,OAAO,KAQvDlB,EAAoBmB,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQlB,EAAoBkB,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,kBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAKxC,OAAOyC,OAAO,MAGvB,GAFAvB,EAAoBe,EAAEO,GACtBxC,OAAO8B,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOlB,EAAoBQ,EAAEc,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIRtB,EAAoB0B,EAAI,SAAStB,GAChC,IAAIM,EAASN,GAAUA,EAAOiB,WAC7B,WAAwB,OAAOjB,EAAO,YACtC,WAA8B,OAAOA,GAEtC,OADAJ,EAAoBQ,EAAEE,EAAQ,IAAKA,GAC5BA,GAIRV,EAAoBW,EAAI,SAASgB,EAAQC,GAAY,OAAO9C,OAAOC,UAAUC,eAAeC,KAAK0C,EAAQC,IAGzG5B,EAAoB6B,EAAI,IAExB,IAAIC,EAAaC,OAAO,gBAAkBA,OAAO,iBAAmB,GAChEC,EAAmBF,EAAW3C,KAAKsC,KAAKK,GAC5CA,EAAW3C,KAAOf,EAClB0D,EAAaA,EAAWG,QACxB,IAAI,IAAItD,EAAI,EAAGA,EAAImD,EAAWjD,OAAQF,IAAKP,EAAqB0D,EAAWnD,IAC3E,IAAIU,EAAsB2C,EAI1BzC,EAAgBJ,KAAK,CAAC,EAAE,kBAEjBM,K,6ECvJT,W,uBCAAW,EAAOD,QAAU,IAA0B,gC,8CCA3CC,EAAOD,QAAU,IAA0B,wB,oCCA3C,W,uBCAAC,EAAOD,QAAU,IAA0B,4B,gDCA3CC,EAAOD,QAAU,IAA0B,yB,qBCA3C,IAAI+B,EAAM,CACT,eAAgB,OAChB,mBAAoB,OACpB,wBAAyB,OACzB,gBAAiB,OACjB,oBAAqB,OACrB,cAAe,OACf,WAAY,OACZ,WAAY,OACZ,sBAAuB,OACvB,oBAAqB,OACrB,eAAgB,OAChB,WAAY,OACZ,aAAc,QACd,aAAc,OACd,eAAgB,OAChB,aAAc,OACd,WAAY,OACZ,YAAa,OACb,cAAe,OACf,gBAAiB,OACjB,gBAAiB,OACjB,kBAAmB,OACnB,YAAa,OACb,YAAa,OACb,YAAa,OACb,kBAAmB,OACnB,sBAAuB,OACvB,mBAAoB,OACpB,mBAAoB,QAIrB,SAASC,EAAeC,GACvB,IAAIC,EAAKC,EAAsBF,GAC/B,OAAOpC,EAAoBqC,GAE5B,SAASC,EAAsBF,GAC9B,IAAIpC,EAAoBW,EAAEuB,EAAKE,GAAM,CACpC,IAAIG,EAAI,IAAIC,MAAM,uBAAyBJ,EAAM,KAEjD,MADAG,EAAEE,KAAO,mBACHF,EAEP,OAAOL,EAAIE,GAEZD,EAAeO,KAAO,WACrB,OAAO5D,OAAO4D,KAAKR,IAEpBC,EAAeQ,QAAUL,EACzBlC,EAAOD,QAAUgC,EACjBA,EAAeE,GAAK,Q,uBClDpBjC,EAAOD,QAAU,IAA0B,yB,oCCA3C,W,oCCAA,W,uBCAAC,EAAOD,QAAU,IAA0B,4B,oCCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,0B,qBCA3CC,EAAOD,QAAU,IAA0B,8B,oCCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,wB,qBCA3CC,EAAOD,QAAU,sjC,uBCAjBC,EAAOD,QAAU,IAA0B,2B,uBCA3CC,EAAOD,QAAU,IAA0B,4B,uBCA3CC,EAAOD,QAAU,IAA0B,kC,gDCA3CC,EAAOD,QAAU,IAA0B,4B,qBCA3CC,EAAOD,QAAU,IAA0B,0B,qBCA3CC,EAAOD,QAAU,IAA0B,8B,uBCA3CC,EAAOD,QAAU,IAA0B,0B,kCCA3C,W,87aCAA,W,mFCGAyC,OAAIC,IAAIC,QAEO,UAAIA,OAAQ,ICLvBC,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,MAAM,CAAEC,SAAUL,EAAIM,gBAAiBC,MAAM,CAAC,GAAK,SAAS,CAACL,EAAG,gBAAgB,IAEzJM,EAAkB,GCCf,SAASC,EAAaC,GAC3B,OAAOA,EAAUC,SAASC,OAAOC,OAAO,GACrCC,MAAM,KACNC,OAAO,CAACC,EAAMC,KAEb,GAAIA,EAAK,CACP,MAAMC,EAAQD,EAAIH,MAAM,KACxBE,EAAKE,EAAM,IAAMA,EAAM,GAGzB,OAAOF,GACN,IAGA,SAASG,EAAkBT,EAAWU,GAC3C,MAAMC,EAASX,EAAUC,SAASU,OAC5BC,EAAOZ,EAAUC,SAASY,SAE1B7B,EAAO5D,OAAO4D,KAAK0B,GACnBR,EAASlB,EAAKqB,OAAO,CAACC,EAAMxC,EAAK7C,KAClB,GAAfqF,EAAKnF,SACPmF,GAAQ,KAGVA,GAAQ,GAAGxC,KAAO4C,EAAO5C,KAErB7C,EAAI+D,EAAK7D,OAAS,IACpBmF,GAAQ,KAGHA,GACN,IAEGQ,EAAS,GAAGH,IAASC,IAAOV,IAElCF,EAAUe,QAAQC,UAAU,GAAI,GAAIF,GClCtC,MAAMG,EAAW,CACfC,MAAO,CACLC,UAAW,KACXC,aAAc,KACdC,UAAW,IAEbC,gBAAiB,SAASC,GACxB,IAAIC,EAAO,GAEXD,EAASE,QAAQC,IACfA,EAAQC,MAAMF,QAAQG,IACpBA,EAAKJ,KAAKC,QAAQI,IACZL,EAAKK,GACPL,EAAKK,IAAQ,EAEbL,EAAKK,GAAO,QAMpBtC,KAAK2B,MAAME,aAAehG,OAAO4D,KAAKwC,GAAMM,KAAK,CAACC,EAAGC,IAC5CR,EAAKQ,GAAKR,EAAKO,IAExBxC,KAAK2B,MAAMG,UAAYG,GAEzBS,aAAc,SAASJ,GACrBpB,EAAkBpC,OAAQ,CAAEwD,QAC5BtC,KAAK2B,MAAMC,UAAYU,GAEzBK,eAAgB,WACdzB,EAAkBpC,OAAQ,IAC1BkB,KAAK2B,MAAMC,UAAY,OAIZF,QC/Bf,GACAlE,KAAA,MACApC,UACA,CACAsG,aAGAkB,QAAA,CACAvC,aAAA,WACA,YAAAvB,OAAA4B,SAAAY,WAAAI,EAAAC,MAAAC,aClB6T,I,wBCQzTiB,EAAY,eACd,EACA,EACAtC,GACA,EACA,KACA,KACA,MAIa,EAAAsC,E,QCnBX/C,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,UAAWF,EAAI2B,SAASC,MAAMC,UAAW3B,EAAG,OAAO,CAACK,MAAM,CAAC,SAAWP,EAAIiC,YAAYjC,EAAI+C,MAAM,IAE5KvC,EAAkB,G,YCFlBT,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACK,MAAM,CAAC,GAAK,WAAW,CAAEP,EAAI2B,SAASC,MAAMC,UAAW3B,EAAG,MAAM,CAAC8C,YAAY,wBAAwB,CAAC9C,EAAG,IAAI,CAAC8C,YAAY,qBAAqBzC,MAAM,CAAC,KAAO,MAAM,CAACP,EAAIiD,GAAG,oBAAoB/C,EAAG,OAAO,CAACK,MAAM,CAAC,MAAQ,GAAG,QAAU,GAAG,IAAM,WAAW,KAAO,WAAW,SAAWP,EAAIkD,YAAY,eAAehD,EAAG,OAAO,CAACK,MAAM,CAAC,MAAQ,GAAG,QAAU,GAAG,IAAM,WAAW,KAAO,WAAW,SAAWP,EAAIkD,YAAY,eAAehD,EAAG,OAAO,CAACK,MAAM,CAAC,MAAQ,GAAG,QAAU,GAAG,IAAM,SAAS,KAAO,SAAS,SAAWP,EAAIkD,YAAY,gBAAgBhD,EAAG,OAAO,CAACK,MAAM,CAAC,MAAQ,GAAG,UAAY,GAAG,IAAM,MAAM,KAAO,MAAM,SAAWP,EAAIkD,YAAY,WAAW,GAAGlD,EAAI+C,KAAO/C,EAAI2B,SAASC,MAAMC,UAA8kB7B,EAAI+C,KAAvkB7C,EAAG,MAAM,CAAC8C,YAAY,oBAAoB,CAAChD,EAAImD,GAAG,GAAGjD,EAAG,MAAM,CAAC8C,YAAY,sCAAsC,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,gBAAgB,CAAChD,EAAIiD,GAAG,oBAAoB/C,EAAG,IAAI,CAAC8C,YAAY,eAAe,CAAChD,EAAIiD,GAAG,iCAAiCjD,EAAImD,GAAG,GAAGjD,EAAG,MAAM,CAAC8C,YAAY,kBAAkB,CAAC9C,EAAG,OAAO,CAACK,MAAM,CAAC,QAAU,GAAG,IAAM,WAAW,KAAO,cAAcL,EAAG,OAAO,CAACK,MAAM,CAAC,QAAU,GAAG,IAAM,WAAW,KAAO,cAAcL,EAAG,OAAO,CAACK,MAAM,CAAC,QAAU,GAAG,IAAM,SAAS,KAAO,YAAYL,EAAG,OAAO,CAACK,MAAM,CAAC,UAAY,GAAG,IAAM,MAAM,KAAO,UAAU,UAE7zCC,EAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAAC8C,YAAY,qCAAqC,CAAC9C,EAAG,MAAM,CAACK,MAAM,CAAC,IAAM6C,EAAQ,cAChK,WAAY,IAAIpD,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAAC8C,YAAY,kBAAkB,CAAC9C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,oCAAoC,CAACL,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,IAAM6C,EAAQ,aAA4BlD,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,qCAAqC,CAACL,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,IAAM6C,EAAQ,aAA4BlD,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,0DAA0D,CAACL,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,IAAM6C,EAAQ,iBCHxjBrD,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,OAAO,CAACE,MAAM,CAAEiD,MAAM,EAAMC,QAAStD,EAAIsD,QAASC,UAAWvD,EAAIuD,UAAWC,SAAUxD,EAAIwD,SAAUC,MAAOzD,EAAIyD,OAAQC,GAAG,CAAC,MAAQ,SAASC,GAAQ,OAAO3D,EAAI4D,OAAO5D,EAAIuC,QAAQ,CAACvC,EAAIiD,GAAG,IAAIjD,EAAI6D,GAAG7D,EAAI8D,MAAM,QAEjRtD,EAAkB,GCUtB,GACA/C,KAAA,OACAsG,MAAA,CACAD,KAAA,CAAAE,KAAAC,QACA1B,IAAA,CAAAyB,KAAAC,QACAC,IAAA,CAAAF,KAAAC,QACAX,QAAA,CAAAU,KAAAG,SACAZ,UAAA,CAAAS,KAAAG,SACAX,SAAA,CAAAQ,KAAAG,SACAV,MAAA,CAAAO,KAAAG,UAEAtB,QAAA,CACAe,OAAA,SAAArB,GACAZ,EAAAgB,aAAAJ,MCzB6U,ICQzU,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCmBf,GACA9E,KAAA,SACAoF,QAAA,CACAe,OAAA,SAAArB,GACAZ,EAAAgB,aAAAJ,IAEAW,YAAA,SAAAX,GACA,OAAAA,GAAAZ,EAAAC,MAAAC,YAGAxG,UACA,CACAsG,aAGAyC,WAAA,CACAC,SCtD8V,ICQ1V,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCnBXtE,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAQF,EAAI2B,SAASC,MAAMC,UAAW3B,EAAG,MAAM,CAACK,MAAM,CAAC,GAAK,SAAS,CAACL,EAAG,gBAAgB,CAACK,MAAM,CAAC,MAAQP,EAAIsE,WAAWtE,EAAIiC,UAAUpG,OAAO,IAAMmE,EAAI2B,SAASC,MAAMC,aAAa7B,EAAIuE,GAAIvE,EAAIsE,WAAWtE,EAAIiC,WAAW,SAASK,EAAK3G,GAAG,OAAOuE,EAAG,MAAM,CAAC1B,IAAI7C,EAAE4E,MAAM,CAAC,KAAO+B,EAAK7E,OAAO,CAAE6E,EAAKkC,IAAKtE,EAAG,MAAM,CAAC8C,YAAY,QAAQ,CAAChD,EAAIiD,GAAG,cAAcjD,EAAI6D,GAAGvB,EAAKkC,KAAK,QAAQxE,EAAI+C,KAAK7C,EAAG,OAAO,CAACK,MAAM,CAAC,KAAO+B,MAAS,OAAM,GAAGtC,EAAI+C,MAEpevC,EAAkB,GCFlBT,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAAC8C,YAAY,kBAAkB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,aAAazC,MAAM,CAAC,IAAM6C,UAAQ,KAAYpD,EAAIsC,KAAKmC,OAAS,IAAM,gBAAgBvE,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,KAAK,CAAC8C,YAAY,+BAA+B0B,SAAS,CAAC,UAAY1E,EAAI6D,GAAG7D,EAAIsC,KAAK7E,SAASuC,EAAIuE,GAAIvE,EAAIsC,KAAKqC,aAAa,SAASb,EAAKnI,GAAG,OAAOuE,EAAG,MAAM,CAAC1B,IAAI7C,EAAEqH,YAAY,aAAa,CAAC9C,EAAG,OAAO,CAACwE,SAAS,CAAC,UAAY1E,EAAI6D,GAAGC,WAAa5D,EAAG,MAAM,CAAC8C,YAAY,sBAAsBhD,EAAIuE,GAAIvE,EAAIsC,KAAKJ,MAAM,SAASK,EAAI5G,GAAG,OAAOuE,EAAG,OAAO,CAAC1B,IAAI7C,EAAE4E,MAAM,CAAC,MAAQ,GAAG,UAAY,GAAG,IAAMgC,EAAI,KAAOA,EAAI,SAAWvC,EAAIkD,YAAYX,IAAMmB,GAAG,CAAC,MAAQ,SAASC,GAAQ,OAAO3D,EAAI2C,aAAaJ,UAAW,IAAI,MAE5vB/B,EAAkB,GCiCtB,GACA/C,KAAA,OACAoF,QAAA,CACAF,aAAAJ,GAAAZ,EAAAgB,aAAAJ,GACAW,YAAA,SAAAX,GACA,OAAAA,GAAAZ,EAAAC,MAAAC,YAGAuC,WAAA,CACAC,QAEAN,MAAA,CACAzB,KAAA,CACA0B,KAAAlI,OACA8I,kBCjD4V,ICQxV,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCnBX7E,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAQF,EAAI2B,SAASC,MAAMC,UAAW3B,EAAG,OAAO,CAAC8C,YAAY,kBAAkB,CAAC9C,EAAG,OAAO,CAAC8C,YAAY,eAAe,CAAChD,EAAIiD,GAAG,WAAWjD,EAAI6D,GAAG7D,EAAI6E,OAAO,qBAAqB3E,EAAG,SAAS,CAACF,EAAIiD,GAAGjD,EAAI6D,GAAG7D,EAAIuC,QAAQvC,EAAIiD,GAAG,SAASjD,EAAI+C,MAEnSvC,EAAkB,GCUtB,GACA/C,KAAA,gBACAoF,QAAA,CACAiC,cAAAnD,EAAAiB,kBAEAvH,UACA,CACAsG,aAGAoC,MAAA,CACAxB,IAAA,CAAAyB,KAAAC,QACAY,MAAA,CAAAb,KAAAe,QACA9C,SAAA,CACA+B,KAAAgB,MACAJ,YAAA,MC3BqW,ICQjW,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCOf,GACAnH,KAAA,OACA2G,WAAA,CACAa,gBACAC,QAEArC,QAAA,CACAyB,WAAArC,IACAA,EAAAkD,KAAAC,MAAAD,KAAAE,UAAApD,IACA,MAAAI,EAAAJ,EAAAlB,OAAA,CAAAC,EAAAoB,KACAA,EAAAC,MAAAF,QAAAG,IACA,IAAAgD,GAAA,EACAhD,EAAAJ,KAAAC,QAAAI,IACAA,GAAAZ,EAAAC,MAAAC,YACAyD,GAAA,KAGAA,GAAAtE,EAAA7E,KAAAmG,KAEAtB,GACA,IAGAqB,EAAAG,KAAA,CAAAC,EAAAC,KACAD,EAAA+B,MAAA/B,EAAA+B,KAAA,IAAAe,MAAAC,eACA9C,EAAA8B,MAAA9B,EAAA8B,KAAA,IAAAe,MAAAC,eAEA9C,EAAA8B,IAAA/B,EAAA+B,MAGA,IAAAiB,EAAA,KASA,OARApD,EAAAF,QAAAG,IACAA,EAAAkC,KAAAiB,GAAAnD,EAAAkC,MAAA,IAAAe,MAAAC,cACAC,EAAAnD,EAAAkC,WAEAlC,EAAAkC,MAIAnC,IAGAhH,UACA,CACAsG,aAGAoC,MAAA,CACA9B,SAAA,CACA+B,KAAAgB,MACAJ,YAAA,MC5E4V,ICQxV,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCAfc,EAAA1D,gBAAAC,GAEA,MAAA0D,EAAAlF,EAAA1B,QACA4G,EAAApD,KACAmD,EAAA/C,aAAA5D,OAAA6G,UAAAD,EAAApD,MAGA,OACA9E,KAAA,OACA2G,WAAA,CACAyB,SACAC,QAEAzK,UACA,CACA4G,WACAN,SAAA+D,KCnC4V,ICOxV,GAAY,eACd,EACA,EACA,GACA,EACA,KACA,KACA,MAIa,M,qBClB6BxF,GAAOC,WAAS,MAAU,KAAOI,EAAK,EAAC,S,OAAC,EAAK,OAAc,OAAIL,GAAG,iBAAoCK,GAAK,QAAC,C,YAASP,OAAS,OAAIE,MAAG,WAE7KM,kBAAoB,I,MCOxB,SAAAuF,GAAAC,GACA,OAAAA,EACAC,QAAA,YACAA,QAAA,YAAA9H,KAAA+H,eAGA,QACAzI,KAAA,cACApC,KAAA,WACA,MAAA2K,EAAA,KAAAG,OAAA7E,KAAAR,MAAA,KAAAsF,MAEA,OACA/D,MAAA,CACA,CAAAyB,KAAA,OAAAuC,MAAA,EAAAC,KAAA,KACA,CAAAxC,KAAAiC,GAAAC,GAAAO,UAAA,OCvBmW,MCQ/V,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,KACA,OAIa,M,QCnBXxG,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAImD,GAAG,IAEtE3C,GAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,6CAA6C/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yWAAyW/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qQAAqQ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8KAA8K/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qMAAqM/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,yBAAyB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qIAAqI/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uDAAuD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,sLAAsL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iGAAiG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,mHAAmH/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qNAAqN/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAgClD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,sEAAsE/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,4QAA4Q/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,iCAAiC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qRAAuR/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2EAA2E/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAyBlD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,8DAA8D/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yEAAyE/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8JAA8J/C,EAAG,MAAM,CAACsG,YAAY,CAAC,QAAU,OAAO,kBAAkB,SAAS,gBAAgB,SAAS,CAACtG,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,gEAAgE/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,mLAAmL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oOAAoO/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,4BAA4B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iEAAiE/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,oHAAoH/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,uDAAuD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+HAA+H/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,wJAAwJ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,ydAAyd/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oNAAoN/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAiBlD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,wPAAwP/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iSAAiS/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uEAAuE/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAgBlD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,iMAAiM/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,kQCqHpiO,IACAxF,KAAA,QCxH4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBXsC,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAImD,GAAG,IAEtE3C,GAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,oDAAoD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,mPAAmP/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,mDAAmD/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,yDAAyD/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,+EAA+E/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iGAAiG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yaAAya/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,4EAA4E/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6PAA6P/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,sDAAsD/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,sDAAsD/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,oFAAoF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yJAAyJ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2KAA2K/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iDAAiD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yDAAyD/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAACK,MAAM,CAAC,IAAM6C,EAAQ,aAAqBlD,EAAG,IAAI,CAACF,EAAIiD,GAAG,oNAAoN/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAACK,MAAM,CAAC,IAAM6C,EAAQ,aAAuBlD,EAAG,IAAI,CAACF,EAAIiD,GAAG,uMAAuM/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,gEAAgE/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAACK,MAAM,CAAC,IAAM6C,EAAQ,aAAwBlD,EAAG,IAAI,CAACF,EAAIiD,GAAG,6KAA6K/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,+EAA+E/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uSAAuS/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+FAA+F/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAACK,MAAM,CAAC,IAAM6C,EAAQ,aAAwBlD,EAAG,IAAI,CAACF,EAAIiD,GAAG,mJAAmJ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iPAAiP/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yHAAyH/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,QAAQ/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,4BAA4BjD,EAAIiD,GAAG,4PAA6P/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,QAAQ/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,0BAA0BjD,EAAIiD,GAAG,iNAAiN/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,QAAQ/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,sBAAsBjD,EAAIiD,GAAG,0JAA4J/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,QAAQ/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,mBAAmBjD,EAAIiD,GAAG,gIAAiI/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,wPAAwP/C,EAAG,QAAQ,CAACF,EAAIiD,GAAG,8BC6FjvL,IACAxF,KAAA,QChG4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBXsC,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAImD,GAAG,IAEtE3C,GAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,kDAAkD/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,gBAAgB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2XAA2X/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,wSAAwS/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,mCAAmC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2ZAA4Z/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0XAA8X/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uVAAwV/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,4MAA4M/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAsBlD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,yQAAyQ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0PAA4P/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+DAA+D/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8KAAgL/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,8EAA8E/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yGAAyG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8GAA8G/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uDAAuD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0NAA4N/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,kFAAkF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2PAA4P/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2EAA2E/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qHAAqH/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iPAAiP/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,0BAA0B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0JAA0J/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,sMAAwM/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2WAAkX/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,0BAA0B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6oBAAgpB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+eAAgf/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6JAA6J/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qeAAqe/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAiBlD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,sKAAwK/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8ZAA8Z/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,gIAAgI/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,kCAAkC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,sLAAsL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iSAAiS/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,kEAAoE/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,sBAAsB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,knBAAonB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8LAA8L/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6EAA6E/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6RAA6R/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,6CAA6C/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6NAA6N/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oUAAoU/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2JAA2J/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,kFAAkF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6NAA8N/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+MAA+M/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,+BAA+B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oTAAoT/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yDAAyD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,wLAAwL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6WAA6W/C,EAAG,MAAM,CAAC8C,YAAY,kBAAkBwD,YAAY,CAAC,QAAU,OAAO,iBAAiB,SAAS,cAAc,WAAW,CAACtG,EAAG,MAAM,CAACsG,YAAY,CAAC,OAAS,SAASjG,MAAM,CAAC,IAAM6C,EAAQ,WAAelD,EAAG,IAAI,CAACsG,YAAY,CAAC,MAAQ,QAAQ,MAAQ,SAAS,CAACxG,EAAIiD,GAAG,2GAA2G/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0JAA0J/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,2FAA2F/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,gFAAgF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,uDAAuD/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,qFAAqF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qDAAqD/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,gFAAgF/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,gEAAgE/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,sDAAsD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qGAAqG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yZAAyZ/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,+BAA+B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,gLAAgL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uLAAuL/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,wOAAwO/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,4LAA4L/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qDAAqD/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,uLAAuL/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,+IAA+I/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,8EAA8E/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,gEAAgE/C,EAAG,IAAI,CAACsG,YAAY,CAAC,cAAc,SAAS,CAACxG,EAAIiD,GAAG,oGAAoG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uGAAuG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qFAAqF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0DAA0D/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0JAA0J/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,uFAAuF/C,EAAG,IAAI,CAACK,MAAM,CAAC,KAAO,8BAA8B,SAAW,KAAK,CAACP,EAAIiD,GAAG,wBAAwBjD,EAAIiD,GAAG,QAAQ/C,EAAG,QAAQ,CAACF,EAAIiD,GAAG,8BCqPhmiB,IACAxF,KAAA,QCxP4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBXsC,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAImD,GAAG,IAEtE3C,GAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,wBAAwB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,sDAAsD/C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,uCAAuC,CAACP,EAAIiD,GAAG,iBAAiBjD,EAAIiD,GAAG,wMAAwM/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,IAAM6C,EAAQ,WAAwBlD,EAAG,QAAQ,CAACF,EAAIiD,GAAG,qCAAqC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2SAA2S/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,kOAAkO/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,qBAAqB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yPAAyP/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0BAA0B/C,EAAG,IAAI,CAACK,MAAM,CAAC,KAAO,6BAA6B,CAACP,EAAIiD,GAAG,sBAAsBjD,EAAIiD,GAAG,sEAAsE/C,EAAG,IAAI,CAACK,MAAM,CAAC,KAAO,wFAAwF,CAACP,EAAIiD,GAAG,yFAAyFjD,EAAIiD,GAAG,oFAAoF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qRAAqR/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,6DAA6D/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,oBAAoB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,8GAA8G/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,SAAS/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,eAAejD,EAAIiD,GAAG,kLAAkL/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,SAAS/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,mBAAmBjD,EAAIiD,GAAG,2EAA2E/C,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,eAAejD,EAAIiD,GAAG,QAAQ/C,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,YAAY,WAAW,CAACP,EAAIiD,GAAG,ujCAAujC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oSAAoS/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,4YAA+Y/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2DAA2D/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,mBAAmBjD,EAAIiD,GAAG,QAAQ/C,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,YAAY,WAAW,CAACP,EAAIiD,GAAG,4/BAA4/B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,yMAAyM/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0NAA0N/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iEAAiE/C,EAAG,MAAM,CAAC8C,YAAY,cAAczC,MAAM,CAAC,YAAY,WAAW,CAACP,EAAIiD,GAAG,6WAA6W/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2CAA2C/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,iBAAiBjD,EAAIiD,GAAG,qBAAqB/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,iBAAiBjD,EAAIiD,GAAG,uGAAuG/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+JAA+J/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,aAAa/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oKAAoK/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,uBAAuB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAqBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,aAAuBlD,EAAG,QAAQ,CAACF,EAAIiD,GAAG,wDAAwD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0ZAA0Z/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,uBAAuB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAkBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAkBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAkBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,aAAoBlD,EAAG,QAAQ,CAACF,EAAIiD,GAAG,oDAAoD/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,gZAAgZ/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,uBAAuB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAoBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAoBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,WAAoBlD,EAAG,MAAM,CAAC8C,YAAY,gBAAgBzC,MAAM,CAAC,IAAM6C,EAAQ,aAAsBlD,EAAG,QAAQ,CAACF,EAAIiD,GAAG,2ICuL9oT,IACAxF,KAAA,QC1L4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,qBCnB6ByC,GAAOC,WAAS,MAAU,KAAyvF6C,UAAY,G,OAAmB,EAAC,ysF,YAAY,cAAS,OAAIhD,YAAO,WAA83CO,GAAK,GAAC,q1C,YAAY,cAAS,OAAIP,YAAO,WAAinFO,GAAK,GAAC,uiF,YAAY,cAAS,OAAIP,YAAO,WAAulBO,GAAK,GAAC,sjB,YAAY,cAAS,OAAIP,YAAO,W,CAAumD,KAAM,gfAAW,0lC,MAAC,CAAe,KAAO,WAAG,UAAO,GAAM,QAAIE,KAAO,SAAqxBK,GAAK,IAAC,wuB,YAAY,cAAS,OAAIP,YAAO,W,CAAq6D,KAAM,mVAAW,sjD,MAAC,CAAe,KAAO,WAAG,UAAO,GAAM,QAAIE,KAAO,SAAuoFK,GAAK,IAAC,8jF,YAAY,cAAS,OAAIP,YAAO,WAE7qdQ,ggCAA8B,ivCAAcN,GAAe,YAAC,MAAU,KAAO8C,UAAY,G,OAAkBwD,QAAY,C,YAAW,kB,YAAO,CAA0B,eAAsB,0BAAItG,cAAS,W,CAAc,QAAS,CAAO,YAAC,CAACK,OAAM,SAA+B,OAAIL,IAAM,EAAC,WAAc,MAAO,C,YAAiB,CAAM,cAAIF,MAAO,SACnU,CAAIA,EAAG,GAAC,kS,WAAqB,MAAU,KAA6HO,EAAK,EAAC,S,OAAC,MAAS,MAAQ,gH,MAAC,CAAqD,gBAAIP,KAAO,iDAG3Q,GAASD,wBAAuB,gBC4ThC,IACAtC,KAAA,QCnU4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBXsC,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAImD,GAAG,IAEtE3C,GAAkB,CAAC,WAAY,IAAIR,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,mCAAmC/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qOAAqO/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,0LAA0L/C,EAAG,IAAI,CAACK,MAAM,CAAC,KAAO,6DAA6D,OAAS,WAAW,CAACP,EAAIiD,GAAG,YAAYjD,EAAIiD,GAAG,gFAAgF/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,iMAAiM/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,aAAa/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,2GAA2G/C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,yDAAyD,CAACP,EAAIiD,GAAG,SAASjD,EAAIiD,GAAG,ogBAAogB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,mjBAAmjB/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,8BAA8B/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,+FAA+F/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,uBAAuBjD,EAAIiD,GAAG,cAAc/C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,yCAAyC,CAACP,EAAIiD,GAAG,mBAAmBjD,EAAIiD,GAAG,sDAAsD/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,oBAAoBjD,EAAIiD,GAAG,0DAA0D/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,oBAAoBjD,EAAIiD,GAAG,4IAA4I/C,EAAG,SAAS,CAACF,EAAIiD,GAAG,kBAAkBjD,EAAIiD,GAAG,uDAAuD/C,EAAG,MAAM,CAAC8C,YAAY,mBAAmB,CAAC9C,EAAG,MAAM,CAAC8C,YAAY,mBAAmBzC,MAAM,CAAC,IAAM6C,EAAQ,aAA8BlD,EAAG,IAAI,CAACF,EAAIiD,GAAG,gBAAgB/C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,qFAAqF,CAACP,EAAIiD,GAAG,kBAAkBjD,EAAIiD,GAAG,0WAA0W/C,EAAG,IAAI,CAACK,MAAM,CAAC,OAAS,SAAS,KAAO,8EAA8E,CAACP,EAAIiD,GAAG,WAAWjD,EAAIiD,GAAG,QAAQ/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oEAAoE/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,oBAAoB/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,6jBAA6jB/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,qiBAAqiB/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,qDAAqD/C,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIiD,GAAG,oHAAoH/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,wEAAwE/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,0JAA0J/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,2FAA2F/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,kBAAkB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,qcAAqc/C,EAAG,KAAK,CAACF,EAAIiD,GAAG,uBAAuB/C,EAAG,IAAI,CAACF,EAAIiD,GAAG,oIAAoI/C,EAAG,MAAM,CAAC8C,YAAY,qBAAqB,CAAC9C,EAAG,OAAO,CAAC8C,YAAY,WAAW,CAAChD,EAAIiD,GAAG,6BCqEl9M,IACAxF,KAAA,QCxE4V,MCQxV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCTA,IACb,CACE6D,KAAM,2BACNwB,UAAW2D,GACXC,KAAM,CAAEC,MAAO,6BAEjB,CACErF,KAAM,+BACNwB,UAAW8D,GACXF,KAAM,CAAEC,MAAO,iCAEjB,CACErF,KAAM,yBACNwB,UAAW+D,GACXH,KAAM,CAAEC,MAAO,iDAEjB,CACErF,KAAM,qBACNwB,UAAWgE,GACXJ,KAAM,CAAEC,MAAO,uBAEjB,CACErF,KAAM,4BACNwB,UAAWiE,GACXL,KAAM,CAAEC,MAAO,iDAEjB,CACErF,KAAM,gCACNwB,UAAWkE,GACXN,KAAM,CAAEC,MAAO,4CCrCJM,MCQfrH,OAAIC,IAAIqH,QACRtH,OAAIuH,OAAOC,eAAgB,EAE3B,MAAMC,GAAS,IAAIH,OAAU,CAC3B9I,KAAM,UACN6I,OAAQ,CACN,CACE3F,KAAM,IACNwB,UAAWwE,GACXZ,KAAM,CAAEC,MAAO,mBAEjB,CACErF,KAAM,SACNwB,UAAWyE,GACXC,SAAUC,OAKhBJ,GAAOK,WAAW,CAACC,EAAIC,EAAMC,KAC3BC,SAASnB,MAAQgB,EAAGjB,KAAKC,MACzBkB,MAGF,IAAIjI,OAAI,CACNyH,UACAU,UACAhI,OAAQiI,GAAKA,EAAEC,KACdC,OAAO,U,uBCtCV9K,EAAOD,QAAU,IAA0B,4B,uBCA3CC,EAAOD,QAAU,IAA0B,wB,gDCA3CC,EAAOD,QAAU,IAA0B,uB,uBCA3CC,EAAOD,QAAU,IAA0B,uB,oCCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,8B,uBCA3CC,EAAOD,QAAU,IAA0B,2B,qBCA3CC,EAAOD,QAAU,IAA0B,4B,8CCA3CC,EAAOD,QAAU,IAA0B,iC,uBCA3CC,EAAOD,QAAU,IAA0B,6B,uBCA3CC,EAAOD,QAAU,IAA0B,0B,qBCA3CC,EAAOD,QAAU,IAA0B,0B,uBCA3CC,EAAOD,QAAU,IAA0B,6B,uBCA3CC,EAAOD,QAAU,IAA0B,wB,qBCA3CC,EAAOD,QAAU,IAA0B,uB,8CCA3CC,EAAOD,QAAU,IAA0B,2B,uBCA3CC,EAAOD,QAAU,IAA0B,8B,uBCA3CC,EAAOD,QAAU,IAA0B,yB,oCCA3C,W,uBCAAC,EAAOD,QAAU,IAA0B,0B,oFCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,gC,qBCA3CC,EAAOD,QAAU,IAA0B,oC,qBCA3CC,EAAOD,QAAU,IAA0B,wC,kCCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,4B,qBCA3CC,EAAOD,QAAU,IAA0B,wB,qBCA3CC,EAAOD,QAAU,IAA0B,+B,4CCA3CC,EAAOD,QAAU,IAA0B,uB,qBCA3CC,EAAOD,QAAU,IAA0B,+B,6CCA3CC,EAAOD,QAAU,IAA0B,yB,qBCA3CC,EAAOD,QAAU,IAA0B,4B,qBCA3CC,EAAOD,QAAU,IAA0B,uB,mECA3CC,EAAOD,QAAU,IAA0B,yB,qBCA3CC,EAAOD,QAAU,IAA0B,6B,kCCA3C,W,qBCAAC,EAAOD,QAAU,IAA0B,gC,qBCA3CC,EAAOD,QAAU,IAA0B,kC,qBCA3CC,EAAOD,QAAU,IAA0B,oC,4CCA3CC,EAAOD,QAAU,IAA0B,8B,qBCA3CC,EAAOD,QAAU,IAA0B","file":"js/app.1b858147.js","sourcesContent":[" \t// install a JSONP callback for chunk loading\n \tfunction webpackJsonpCallback(data) {\n \t\tvar chunkIds = data[0];\n \t\tvar moreModules = data[1];\n \t\tvar executeModules = data[2];\n\n \t\t// add \"moreModules\" to the modules object,\n \t\t// then flag all \"chunkIds\" as loaded and fire callback\n \t\tvar moduleId, chunkId, i = 0, resolves = [];\n \t\tfor(;i < chunkIds.length; i++) {\n \t\t\tchunkId = chunkIds[i];\n \t\t\tif(Object.prototype.hasOwnProperty.call(installedChunks, chunkId) && installedChunks[chunkId]) {\n \t\t\t\tresolves.push(installedChunks[chunkId][0]);\n \t\t\t}\n \t\t\tinstalledChunks[chunkId] = 0;\n \t\t}\n \t\tfor(moduleId in moreModules) {\n \t\t\tif(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {\n \t\t\t\tmodules[moduleId] = moreModules[moduleId];\n \t\t\t}\n \t\t}\n \t\tif(parentJsonpFunction) parentJsonpFunction(data);\n\n \t\twhile(resolves.length) {\n \t\t\tresolves.shift()();\n \t\t}\n\n \t\t// add entry modules from loaded chunk to deferred list\n \t\tdeferredModules.push.apply(deferredModules, executeModules || []);\n\n \t\t// run deferred modules when all chunks ready\n \t\treturn checkDeferredModules();\n \t};\n \tfunction checkDeferredModules() {\n \t\tvar result;\n \t\tfor(var i = 0; i < deferredModules.length; i++) {\n \t\t\tvar deferredModule = deferredModules[i];\n \t\t\tvar fulfilled = true;\n \t\t\tfor(var j = 1; j < deferredModule.length; j++) {\n \t\t\t\tvar depId = deferredModule[j];\n \t\t\t\tif(installedChunks[depId] !== 0) fulfilled = false;\n \t\t\t}\n \t\t\tif(fulfilled) {\n \t\t\t\tdeferredModules.splice(i--, 1);\n \t\t\t\tresult = __webpack_require__(__webpack_require__.s = deferredModule[0]);\n \t\t\t}\n \t\t}\n\n \t\treturn result;\n \t}\n\n \t// The module cache\n \tvar installedModules = {};\n\n \t// object to store loaded and loading chunks\n \t// undefined = chunk not loaded, null = chunk preloaded/prefetched\n \t// Promise = chunk loading, 0 = chunk loaded\n \tvar installedChunks = {\n \t\t\"app\": 0\n \t};\n\n \tvar deferredModules = [];\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"/\";\n\n \tvar jsonpArray = window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || [];\n \tvar oldJsonpFunction = jsonpArray.push.bind(jsonpArray);\n \tjsonpArray.push = webpackJsonpCallback;\n \tjsonpArray = jsonpArray.slice();\n \tfor(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);\n \tvar parentJsonpFunction = oldJsonpFunction;\n\n\n \t// add entry module to deferred list\n \tdeferredModules.push([0,\"chunk-vendors\"]);\n \t// run deferred modules when ready\n \treturn checkDeferredModules();\n","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=1996c03d&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/cryptokitty.4fe516f3.png\";","module.exports = __webpack_public_path__ + \"img/rnn.f7da92e3.png\";","export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Chip.vue?vue&type=style&index=0&id=cdb4c35a&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/final-1.cb8ce791.png\";","module.exports = __webpack_public_path__ + \"img/nasa.3fe966d1.png\";","var map = {\n\t\"./adrich.jpg\": \"6b1c\",\n\t\"./background.png\": \"30b3\",\n\t\"./carnegie_mellon.png\": \"f11a\",\n\t\"./codecov.jpg\": \"7265\",\n\t\"./cryptokitty.png\": \"055f\",\n\t\"./expii.png\": \"4384\",\n\t\"./gh.png\": \"cb79\",\n\t\"./hf.png\": \"5e5e\",\n\t\"./home-minified.jpg\": \"3cee\",\n\t\"./kitchen_zen.png\": \"a36a\",\n\t\"./lambda.png\": \"34ea\",\n\t\"./li.png\": \"5db6\",\n\t\"./logo.png\": \"cf053\",\n\t\"./logo.svg\": \"9b19\",\n\t\"./moment.png\": \"9822\",\n\t\"./nasa.png\": \"15b0\",\n\t\"./nn.png\": \"d116\",\n\t\"./npm.png\": \"94c7\",\n\t\"./oauth.png\": \"441f\",\n\t\"./profile.jpg\": \"3a19\",\n\t\"./profile.png\": \"c3e7\",\n\t\"./quadratic.png\": \"6819\",\n\t\"./rnn.png\": \"1028\",\n\t\"./svd.png\": \"5b3b\",\n\t\"./svm.png\": \"c5ae\",\n\t\"./synthesis.png\": \"4410\",\n\t\"./timing_attack.png\": \"e750\",\n\t\"./travel-map.png\": \"cbdf\",\n\t\"./tunessence.png\": \"c7ca\"\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tif(!__webpack_require__.o(map, req)) {\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn map[req];\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = \"1771\";","module.exports = __webpack_public_path__ + \"img/good.2505cc53.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=0d57277a&prod&scoped=true&lang=css\"","export * from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--1-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&id=ab3a2e6a&prod&lang=css\"","module.exports = __webpack_public_path__ + \"img/vanilla.194d154a.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=25959adf&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/gen-3.4d3acaf8.png\";","module.exports = __webpack_public_path__ + \"img/panama-10.5f0887e7.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./FilterMessage.vue?vue&type=style&index=0&id=6880f3da&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/bad.32d4a3c9.png\";","module.exports = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADEAAAAcBAMAAAA6gErHAAAAMFBMVEXf39+1tbXg4OC/v7/Gxsrh4eHHx8rGxsbGxsrExMTDw8PGxsbDw8rExMq/v8q1tctsqyaSAAAAEHRSTlMACAAQMQA5MSkgGSkZIBAI107I3wAAAoNJREFUeF4V0t9rVEccxuF3ZjLtGmOZmXULRgNz9uiFvfqew7oJFmXCHjbZROlkw94IwilZYn4Ja7MkaaKCsAZLERLdprdJG9JCbwIRhHgT6EEKvbJpe99r7/oXuP4BL3weeLG6raWt206UL8JCnZmBD5X6tepQrVFd8evLloQkLYsdgII/55MUr0cmKC9Ku9P1oqSIsb5IqFzt5qJDtpXdkuxf44nJoue+/ys+2KTcC4ez39DmqF0HKFCqQQjXws47nkkHPmHEzhsHVgga2tS5yJKfmZ7QKWSZEIwdqYKfYqRQz3dnApab4Q6270HA3yQnXOUD0/AoH38L/BJbB2IPWZjmViJwUmG4X7PPtB0PZApgh5VOcX5NqSKpkRqCT34KHWQLhNJWJ/LsjyVtp2/MURD2f315F8zBsDsTt2CVfVsRA2N7CpYe7RglUhjL2076orWd4+YpKXA5XLEwDlzwHdJkyIvqMY+8sSgvexY52Cm7cRho0qx7+1UkgDzKC0C91bOfW65IitB9LK8mex51bHSZZilkvm+rewWUjZPB0NgJwlw6/NTKFmJ01Of/4Mv5yAL8wsY1X94bdAop6LN1fnX54mTMDfUCS7fjrsQ8GimQeyntZBKhQEpbrfYfLVmTEWshn/2nB1aOBM8jAKwVBwsauUPhoO5Pi83vWoxxJiVEQAfVbaVqkcPdUfrxeViDKMQhj3qbuS+SE/z9f4ru7+Ua2FsVFKCnvZEjFTuwuPXpDw6bv9WEKJR6AoDXgb6/gKHJS/cdkjkTA2JdQsfaAt9LIX02PuvQflIURR52CDBexP0zJuBk33/86Gx1Nmk329UkaTZXZpN77Wayeq9ZdR8AKlSbB1hHF9QAAAAASUVORK5CYII=\"","module.exports = __webpack_public_path__ + \"img/lambda.95258360.png\";","module.exports = __webpack_public_path__ + \"img/profile.49acbe3b.jpg\";","module.exports = __webpack_public_path__ + \"img/home-minified.732b0c07.jpg\";","module.exports = __webpack_public_path__ + \"img/final-2.b4185e09.png\";","module.exports = __webpack_public_path__ + \"img/expii.f9a5c184.png\";","module.exports = __webpack_public_path__ + \"img/synthesis.f17df5cd.png\";","module.exports = __webpack_public_path__ + \"img/oauth.605a7832.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=79d6fb9a&prod&scoped=true&lang=css\"","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=a8ae26b0&prod&scoped=true&lang=css\"","import Vue from 'vue';\nimport Vuetify from 'vuetify/lib';\n\nVue.use(Vuetify);\n\nexport default new Vuetify({\n});\n","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{class:{ centered: _vm.shouldCenter() },attrs:{\"id\":\"root\"}},[_c('router-view')],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// URLSearchParams is not available in IE, so I'm going to toss together\n// some quick functions for managing URL parameters\n\nexport function getUrlParams(windowRef) {\n  return windowRef.location.search.substr(1)\n    .split('&')\n    .reduce((memo, val) => {\n      // NOTE: \"\".split('&') -> ['']\n      if (val) {\n        const tuple = val.split('=');\n        memo[tuple[0]] = tuple[1];\n      }\n\n      return memo;\n    }, {});\n}\n\nexport function setSingleUrlParam(windowRef, params) {\n  const origin = windowRef.location.origin;\n  const path = windowRef.location.pathname;\n\n  const keys = Object.keys(params);\n  const search = keys.reduce((memo, key, i) => {\n    if (memo.length == 0) {\n      memo += '?';\n    }\n\n    memo += `${key}=${params[key]}`;\n\n    if (i < keys.length - 1) {\n      memo += '&';\n    }\n\n    return memo;\n  }, '');\n\n  const newUrl = `${origin}${path}${search}`;\n\n  windowRef.history.pushState({}, \"\", newUrl);\n}\n","import { setSingleUrlParam } from './util/url';\n\n// As we do not need this advanced functionality, we will follow the simpler\n// covention as described: https://vuejs.org/v2/guide/state-management.html#\nconst tagStore = {\n  state: {\n    activeTag: null,\n    possibleTags: null,\n    tagCounts: {},\n  },\n  setPossibleTags: function(sections) {\n    var tags = {};\n\n    sections.forEach(section => {\n      section.items.forEach(item => {\n        item.tags.forEach(tag => {\n          if (tags[tag]) {\n            tags[tag] += 1;\n          } else {\n            tags[tag] = 1;\n          }\n        })\n      })\n    });\n\n    this.state.possibleTags = Object.keys(tags).sort((a, b) => {\n      return tags[b] - tags[a];\n    });\n    this.state.tagCounts = tags;\n  },\n  setActiveTag: function(tag) {\n    setSingleUrlParam(window, { tag });\n    this.state.activeTag = tag;\n  },\n  clearActiveTag: function() {\n    setSingleUrlParam(window, {});\n    this.state.activeTag = null;\n  },\n};\n\nexport default tagStore;\n","<template>\n  <div id=\"root\" :class=\"{ centered: shouldCenter() }\">\n    <router-view/>\n  </div>\n</template>\n\n<script>\n  import tagStore from './TagStore';\n\n  export default {\n    name: 'App',\n    data: () => {\n      return {\n        tagStore: tagStore\n      };\n    },\n    methods: {\n      shouldCenter: function() {\n        return window.location.pathname === '/' && !tagStore.state.activeTag;\n      },\n    },\n  };\n</script>\n\n<style>\n  #root {\n    overflow: scroll;\n    position: fixed;\n    width: 100%;\n    height: 100%;\n    display: flex;\n    padding: 18px;\n    flex-direction: column;\n    font-family: Avenir, Helvetica, Arial, sans-serif;\n    -webkit-font-smoothing: antialiased;\n    -moz-osx-font-smoothing: grayscale;\n    color: #2c3e50;\n  }\n\n  .centered {\n    justify-content: center;\n    align-items: center;\n  }\n\n  .root-level-content-wrapper {\n    max-width: 960px;\n  }\n  .space-before-next {\n    margin-bottom: 28px;\n  }\n\n  /* This is purposefully site-wide */\n  a {\n    text-decoration: none !important;\n    color: #00e;\n  }\n\n  @media only screen and (max-width: 650px) {\n    #root {\n      padding: 0px;\n    }\n\n    .root-level-content-wrapper {\n      padding: 16px;\n      width: 100%;\n    }\n  }\n</style>\n","import mod from \"-!../node_modules/cache-loader/dist/cjs.js??ref--13-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--1-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../node_modules/cache-loader/dist/cjs.js??ref--13-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--1-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=ab3a2e6a\"\nimport script from \"./App.vue?vue&type=script&lang=js\"\nexport * from \"./App.vue?vue&type=script&lang=js\"\nimport style0 from \"./App.vue?vue&type=style&index=0&id=ab3a2e6a&prod&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',[_c('Header'),(_vm.tagStore.state.activeTag)?_c('Body',{attrs:{\"sections\":_vm.sections}}):_vm._e()],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{attrs:{\"id\":\"header\"}},[(_vm.tagStore.state.activeTag)?_c('div',{staticClass:\"chip-container-small\"},[_c('a',{staticClass:\"header-title-small\",attrs:{\"href\":\"/\"}},[_vm._v(\"Robert Kotcher\")]),_c('Chip',{attrs:{\"small\":\"\",\"primary\":\"\",\"tag\":\"learning\",\"text\":\"learning\",\"selected\":_vm.isActiveTag('learning')}}),_c('Chip',{attrs:{\"small\":\"\",\"primary\":\"\",\"tag\":\"industry\",\"text\":\"industry\",\"selected\":_vm.isActiveTag('industry')}}),_c('Chip',{attrs:{\"small\":\"\",\"primary\":\"\",\"tag\":\"travel\",\"text\":\"travel\",\"selected\":_vm.isActiveTag('languages')}}),_c('Chip',{attrs:{\"small\":\"\",\"secondary\":\"\",\"tag\":\"old\",\"text\":\"old\",\"selected\":_vm.isActiveTag('old')}})],1):_vm._e(),(!_vm.tagStore.state.activeTag)?_c('div',{staticClass:\"header-container\"},[_vm._m(0),_c('div',{staticClass:\"intro-panel animation-panel-second\"},[_c('div',{staticClass:\"header-title\"},[_vm._v(\"Robert Kotcher\")]),_c('p',{staticClass:\"header-text\"},[_vm._v(\" Computers, travel, sushi. \")]),_vm._m(1),_c('div',{staticClass:\"chip-container\"},[_c('Chip',{attrs:{\"primary\":\"\",\"tag\":\"learning\",\"text\":\"learning\"}}),_c('Chip',{attrs:{\"primary\":\"\",\"tag\":\"industry\",\"text\":\"industry\"}}),_c('Chip',{attrs:{\"primary\":\"\",\"tag\":\"travel\",\"text\":\"travel\"}}),_c('Chip',{attrs:{\"secondary\":\"\",\"tag\":\"old\",\"text\":\"old\"}})],1)])]):_vm._e()])\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"image-panel animation-panel-first\"},[_c('img',{attrs:{\"src\":require(\"../../assets/home-minified.jpg\")}})])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"link-container\"},[_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://huggingface.co/rkotcher\"}},[_c('img',{staticClass:\"header-link\",attrs:{\"src\":require(\"../../assets/hf.png\")}})]),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://github.com/robertkotcher\"}},[_c('img',{staticClass:\"header-link\",attrs:{\"src\":require(\"../../assets/gh.png\")}})]),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://www.linkedin.com/in/robert-kotcher-639105196/\"}},[_c('img',{staticClass:\"header-link\",attrs:{\"src\":require(\"../../assets/li.png\")}})])])\n}]\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('span',{class:{ chip: true, primary: _vm.primary, secondary: _vm.secondary, selected: _vm.selected, small: _vm.small },on:{\"click\":function($event){return _vm.setTag(_vm.tag)}}},[_vm._v(\" \"+_vm._s(_vm.text)+\" \")])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <span\n      v-on:click=\"setTag(tag)\"\n      :class=\"{ chip: true, primary, secondary, selected, small }\"\n    >\n      {{text}}\n    </span>\n</template>\n\n<script>\n  import tagStore from '../TagStore';\n\n  export default {\n    name: 'Chip',\n    props: {\n      text: { type: String },\n      tag: { type: String },\n      url: { type: String },\n      primary: { type: Boolean },\n      secondary: { type: Boolean },\n      selected: { type: Boolean },\n      small: { type: Boolean },\n    },\n    methods: {\n      setTag: function(tag) {\n        tagStore.setActiveTag(tag);\n      },\n    },\n  };\n</script>\n\n<style scoped>\n    .primary {\n        background-color: #4caf50 !important;\n        color: white;\n        filter: brightness(1);\n    }\n    .secondary {\n        background-color: #dedede !important;\n        color: #444;\n        filter: brightness(1);\n    }\n    .selected {\n      filter: brightness(0.75);\n    }\n    .chip {\n        margin: 9px 9px 9px 0;\n        font-size: 18px;\n        padding: 12px 16px;\n        border-radius: 28px;\n        cursor: pointer;\n    }\n    .small {\n      font-size: 12px;\n      margin: 0px;\n      margin-right: 4px;\n      padding: 5px 6px;\n      border-radius: 5px;\n    }\n</style>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Chip.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Chip.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Chip.vue?vue&type=template&id=cdb4c35a&scoped=true\"\nimport script from \"./Chip.vue?vue&type=script&lang=js\"\nexport * from \"./Chip.vue?vue&type=script&lang=js\"\nimport style0 from \"./Chip.vue?vue&type=style&index=0&id=cdb4c35a&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"cdb4c35a\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div id=\"header\">\n    <div class=\"chip-container-small\" v-if=\"tagStore.state.activeTag\">\n      <a href=\"/\" class=\"header-title-small\">Robert Kotcher</a>\n      <Chip small primary tag=\"learning\" text=\"learning\" :selected=\"isActiveTag('learning')\"/>\n      <Chip small primary tag=\"industry\" text=\"industry\" :selected=\"isActiveTag('industry')\"/>\n      <Chip small primary tag=\"travel\" text=\"travel\" :selected=\"isActiveTag('languages')\"/>\n      <Chip small secondary tag=\"old\" text=\"old\" :selected=\"isActiveTag('old')\"/>\n    </div>\n    <div class=\"header-container\" v-if=\"!tagStore.state.activeTag\">\n      <div class=\"image-panel animation-panel-first\">\n        <img src=\"../../assets/home-minified.jpg\"/>\n      </div>\n      <div class=\"intro-panel animation-panel-second\">\n        <div class=\"header-title\">Robert Kotcher</div>\n        <p class=\"header-text\">\n          Computers, travel, sushi.\n        </p>\n        <div class=\"link-container\">\n          <a target=\"_blank\" href=\"https://huggingface.co/rkotcher\"><img src=\"../../assets/hf.png\" class=\"header-link\"/></a>\n          <a target=\"_blank\" href=\"https://github.com/robertkotcher\"><img src=\"../../assets/gh.png\" class=\"header-link\"/></a>\n          <a target=\"_blank\" href=\"https://www.linkedin.com/in/robert-kotcher-639105196/\"><img src=\"../../assets/li.png\" class=\"header-link\"/></a>\n        </div>\n        <div class=\"chip-container\">\n          <Chip primary tag=\"learning\" text=\"learning\"/>\n          <Chip primary tag=\"industry\" text=\"industry\"/>\n          <Chip primary tag=\"travel\" text=\"travel\"/>\n          <Chip secondary tag=\"old\" text=\"old\"/>\n        </div>\n      </div>\n    </div>\n  </div>\n</template>\n\n<script>\n  import tagStore from '../../TagStore';\n  import Chip from '../Chip';\n\n  export default {\n    name: 'Header',\n    methods: {\n      setTag: function(tag) {\n        tagStore.setActiveTag(tag);\n      },\n      isActiveTag: function(tag) {\n        return tag == tagStore.state.activeTag;\n      },\n    },\n    data: () => {\n      return {\n        tagStore: tagStore\n      };\n    },\n    components: {\n      Chip,\n    },\n  }\n</script>\n\n<style scoped>\n  #header {\n    margin: 18px;\n    max-width: 960px;\n  }\n\n  .header-container {\n    display: flex;\n    flex-direction: row;\n    justify-content: center;\n    flex-wrap: wrap;\n  }\n\n  .animation-panel-first {\n    animation-name: fadeIn;\n    animation-duration: 750ms;\n    animation-fill-mode: forwards;\n  }\n\n  .animation-panel-second {\n    animation-name: fadeIn;\n    animation-delay: 100ms;\n    animation-duration: 1250ms;\n    animation-fill-mode: forwards;\n  }\n\n  .animation-panel-third {\n    animation-name: fadeIn;\n    animation-delay: 250ms;\n    animation-duration: 1500ms;\n    animation-fill-mode: forwards;\n  }\n\n  .left-panel {\n    width: 200px;\n    margin-right: 18px;\n    opacity: 0;\n  }\n\n  .chip-container {\n    display: flex;\n    flex-wrap: wrap;\n    flex-direction: row;\n    justify-content: center;\n    align-items: center;\n    margin: 12px 0;\n    width: 100%;\n  }\n\n  .chip-container-small {\n    flex: 1;\n    display: flex;\n    flex-wrap: wrap;\n    flex-direction: row;\n    align-items: center;\n    margin: 8px 0;\n    width: 100%;\n    height: 38px;\n  }\n\n  .link-container {\n    display: flex;\n    flex-wrap: wrap;\n    flex-direction: row;\n    justify-content: center;\n    align-items: center;\n    width: 100%;\n    margin-bottom: 40px;\n  }\n\n  .link-container > a {\n    margin: 0 8px;\n  }\n\n  @media only screen and (max-width: 550px) {\n    .left-panel {\n      display: none;\n    }\n  }\n\n  .image-panel {\n    position: fixed;\n    top: 0;\n    left: 0;\n    width: 50%;\n    height: 100%;\n    overflow: hidden;\n  }\n\n  .image-panel img {\n    position: absolute;\n    top: 50%;\n    left: 50%;\n    transform: translate(-50%, -50%) scale(0.5);\n    min-width: 100%;\n    min-height: 100%;\n    object-fit: cover;\n  }\n\n  .intro-panel {\n    position: fixed;\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    justify-content: center;\n    top: 0;\n    right: 0;\n    width: 50%;\n    height: 100%;\n    overflow: hidden;\n  }\n\n  .header-title {\n    text-align: center;\n    font-size: 50px;\n    font-weight: 500;\n    line-height: 1;\n  }\n\n  .header-title-small {\n    margin-right: 8px;\n    color: black;\n    text-decoration: none;\n    font-size: 24px;\n    font-weight: 500;\n    line-height: 1;\n  }  \n\n  .header-text {\n    margin: 16px 0;\n    display: flex;\n    flex-wrap: wrap;\n    flex-direction: row;\n    justify-content: center;\n    align-items: center;\n    width: 100%;\n    font-size: 20px;\n    text-align: center;\n    padding: 0 8px;\n  }\n\n  .header-link {\n    width: 32px;\n    height: 32px;\n  }\n\n  .right-panel {\n    flex: 1;\n    display: flex;\n    flex-direction: column;\n    opacity: 0;\n  }\n\n  .intro-table {\n    font-size: 26px;\n  }\n\n  .upper-col {\n    padding: 0px 10px 10px 0;    \n  }\n\n  .inner-col {\n    padding: 10px 10px 10px 0;\n    vertical-align: top;\n    line-height: 1.2;\n  }\n\n  .emoji-col {\n    width: 38px;\n  }\n\n  .outer-col {\n    line-height: 1;\n  }\n\n  @keyframes fadeIn {\n    0% {opacity:0;}\n    100% {opacity:1;}\n  }\n\n  @-moz-keyframes fadeIn {\n    0% {opacity:0;}\n    100% {opacity:1;}\n  }\n\n  @-webkit-keyframes fadeIn {\n    0% {opacity:0;}\n    100% {opacity:1;}\n  }\n\n  @-o-keyframes fadeIn {\n    0% {opacity:0;}\n    100% {opacity:1;}\n  }\n\n  @-ms-keyframes fadeIn {\n    0% {opacity:0;}\n    100% {opacity:1;}\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Header.vue?vue&type=template&id=666ed3f6&scoped=true\"\nimport script from \"./Header.vue?vue&type=script&lang=js\"\nexport * from \"./Header.vue?vue&type=script&lang=js\"\nimport style0 from \"./Header.vue?vue&type=style&index=0&id=666ed3f6&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"666ed3f6\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return (_vm.tagStore.state.activeTag)?_c('div',{attrs:{\"id\":\"body\"}},[_c('FilterMessage',{attrs:{\"count\":_vm.itemsByTag(_vm.sections).length,\"tag\":_vm.tagStore.state.activeTag}}),_vm._l((_vm.itemsByTag(_vm.sections)),function(item,i){return _c('div',{key:i,attrs:{\"name\":item.name}},[(item.end)?_c('div',{staticClass:\"year\"},[_vm._v(\" completed \"+_vm._s(item.end)+\": \")]):_vm._e(),_c('Item',{attrs:{\"item\":item}})],1)})],2):_vm._e()\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"item-container\"},[_c('img',{staticClass:\"item-image\",attrs:{\"src\":require(`@/assets/${_vm.item.image}`),\"alt\":\"Item Image\"}}),_c('div',{staticClass:\"item-text-panel\"},[_c('h3',{staticClass:\"font-weight-bold item-header\",domProps:{\"innerHTML\":_vm._s(_vm.item.name)}}),_vm._l((_vm.item.description),function(text,i){return _c('div',{key:i,staticClass:\"item-text\"},[_c('span',{domProps:{\"innerHTML\":_vm._s(text)}})])}),_c('div',{staticClass:\"item-tag-container\"},_vm._l((_vm.item.tags),function(tag,i){return _c('Chip',{key:i,attrs:{\"small\":\"\",\"secondary\":\"\",\"tag\":tag,\"text\":tag,\"selected\":_vm.isActiveTag(tag)},on:{\"click\":function($event){return _vm.setActiveTag(tag)}}})}),1)],2)])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"item-container\">\n    <img :src=\"require(`@/assets/${item.image}`)\" alt=\"Item Image\" class=\"item-image\" />\n    <div class=\"item-text-panel\">\n      <h3\n        v-html=\"item.name\"\n        class=\"font-weight-bold item-header\"\n      />\n      <div\n        v-for=\"(text, i) in item.description\"\n        :key=\"i\"\n        class=\"item-text\"\n      >\n        <span v-html=\"text\" />\n      </div>\n      <div class=\"item-tag-container\">\n        <Chip\n          small\n          secondary\n          :tag=tag\n          :text=tag\n          v-for=\"(tag, i) in item.tags\"\n          :selected=\"isActiveTag(tag)\"\n          v-on:click=\"setActiveTag(tag)\"\n          v-bind:key=\"i\"\n        />\n      </div>\n    </div>\n  </div>\n</template>\n\n<script>\n  import tagStore from '../../TagStore';\n  import Chip from '../Chip';\n\n  export default {\n    name: 'Item',\n    methods: {\n      setActiveTag: tag => tagStore.setActiveTag(tag),\n      isActiveTag: function(tag) {\n        return tag == tagStore.state.activeTag;\n      },\n    },\n    components: {\n      Chip,\n    },\n    props: {\n      item: {\n        type: Object,\n        default: () => {}\n      }\n    }\n  };\n</script>\n\n<style scoped>\n  .item-container {\n    display: flex;\n    flex-direction: row;\n    justify-content: space-around;\n    align-items: flex-start;\n    flex-wrap: wrap;\n    margin: 28px 0;\n  }\n\n  .item-text-panel {\n    flex: 1;\n    display: flex;\n    flex-direction: column;\n    justify-content: space-around;\n  }\n\n  .item-header {\n    line-height: 1;\n  }\n\n  .item-tag-container {\n    margin-top: 6px;\n    display: flex;\n    flex-direction: row;\n    align-items: center;\n    flex-wrap: wrap;\n  }\n\n  .item-tag {\n    margin: 6px 6px 6px 0;\n  }\n\n  .item-text {\n    margin: 7px 0 2px 0;\n  }\n\n  .item-image {\n    width: 50px;\n    margin-right: 25px;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Item.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Item.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Item.vue?vue&type=template&id=f3f48df2&scoped=true\"\nimport script from \"./Item.vue?vue&type=script&lang=js\"\nexport * from \"./Item.vue?vue&type=script&lang=js\"\nimport style0 from \"./Item.vue?vue&type=style&index=0&id=f3f48df2&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"f3f48df2\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return (_vm.tagStore.state.activeTag)?_c('span',{staticClass:\"filter-message\"},[_c('span',{staticClass:\"filter-text\"},[_vm._v(\"Showing \"+_vm._s(_vm.count)+\" results for tag \"),_c('strong',[_vm._v(_vm._s(_vm.tag))]),_vm._v(\".\")])]):_vm._e()\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <span \n    class=\"filter-message\"\n    v-if=\"tagStore.state.activeTag\"\n  >\n    <span class=\"filter-text\">Showing {{count}} results for tag <strong>{{tag}}</strong>.</span>\n  </span>\n</template>\n\n<script>\n  import tagStore from '../../TagStore';\n\n  export default {\n    name: 'FilterMessage',\n    methods: {\n      removeTag: () => tagStore.clearActiveTag(),\n    },\n    data: () => {\n      return {\n        tagStore: tagStore\n      };\n    },\n    props: {\n      tag: { type: String },\n      count: { type: Number },\n      sections: {\n        type: Array,\n        default: () => []\n      }\n    }\n  }\n</script>\n\n<style scoped>\n  .filter-message {\n    display: flex;\n    flex-direction: row;\n    justify-content: flex-start;\n    align-items: center;\n    margin-bottom: 18px;\n  }\n\n  .filter-text {\n    margin-right: 8px;\n    font-size: 12px;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./FilterMessage.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./FilterMessage.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./FilterMessage.vue?vue&type=template&id=6880f3da&scoped=true\"\nimport script from \"./FilterMessage.vue?vue&type=script&lang=js\"\nexport * from \"./FilterMessage.vue?vue&type=script&lang=js\"\nimport style0 from \"./FilterMessage.vue?vue&type=style&index=0&id=6880f3da&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"6880f3da\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div\n    id=\"body\"\n    v-if=\"tagStore.state.activeTag\"\n  >\n    <FilterMessage :count=\"itemsByTag(sections).length\" :tag=\"tagStore.state.activeTag\"/>\n    <div\n      v-for=\"(item, i) in itemsByTag(sections)\"\n      v-bind:key=\"i\"\n      v-bind:name=\"item.name\"\n    >\n      <div class=\"year\" v-if=\"item.end\">\n        completed {{item.end}}:\n      </div>\n      <Item\n        :item=item\n      />\n    </div>\n  </div>\n</template>\n\n<script>\n  import Item from './Item.vue';\n  import tagStore from '../../TagStore';\n  import FilterMessage from './FilterMessage';\n\n  export default {\n    name: 'Body',\n    components: {\n      FilterMessage,\n      Item\n    },\n    methods: {\n      itemsByTag: (sections) => {\n        sections = JSON.parse(JSON.stringify(sections));\n        const items = sections.reduce((memo, section) => {\n          section.items.forEach(item => {\n            let append = false;\n            item.tags.forEach(tag => {\n              if (tag == tagStore.state.activeTag) {\n                append = true;\n              }\n            })\n            if (append) memo.push(item);\n          });\n          return memo;\n        }, []);\n\n        // sort by year of completion\n        items.sort((a, b) => {\n          if (!a.end) a.end = new Date().getFullYear();\n          if (!b.end) b.end = new Date().getFullYear();\n\n          return b.end - a.end;\n        });\n\n        var lastYearSeen = null;\n        items.forEach(item => {\n          if (item.end != lastYearSeen && item.end != new Date().getFullYear()) {\n            lastYearSeen = item.end;\n          } else {\n            delete item.end;\n          }\n        });\n\n        return items;\n      }\n    },\n    data: () => {\n      return {\n        tagStore: tagStore\n      };\n    },\n    props: {\n      sections: {\n        type: Array,\n        default: () => []\n      }\n    }\n  };\n</script>\n\n<style scoped>\n  #body {\n    padding: 18px;\n    width: 100%;\n    max-width: 800px;\n  }\n\n  ul.sections-list {\n    padding-left: 0;\n    list-style-type: none;\n  }\n\n  .cluster-status-container {\n    display: flex;\n    flex-direction: row;\n    justify-content: flex-end;\n  }\n\n  .year {\n    width: 100%;\n    display: flex;\n    justify-content: flex-end;\n    margin-top: 40px;\n    font-style: italic;\n    font-size: 14px;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Body.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Body.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Body.vue?vue&type=template&id=f9c8eb22&scoped=true\"\nimport script from \"./Body.vue?vue&type=script&lang=js\"\nexport * from \"./Body.vue?vue&type=script&lang=js\"\nimport style0 from \"./Body.vue?vue&type=style&index=0&id=f9c8eb22&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"f9c8eb22\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div>\n    <Header/>\n    <Body\n      v-if=\"tagStore.state.activeTag\"\n      :sections=\"sections\"\n    />\n  </div>\n</template>\n\n<script>\n  import sections from '../../sections/sections.json';\n\n  import TagStore from '../../TagStore';  \n  import { getUrlParams } from '../../util/url';\n\n  import Header from './Header.vue';\n  import Body from './Body.vue';\n\n  TagStore.setPossibleTags(sections);\n\n  const urlParams = getUrlParams(window);\n  if (urlParams.tag) {\n    TagStore.setActiveTag(window.decodeURI(urlParams.tag));\n  }\n\n  export default {\n    name: 'Home',\n    components: {\n      Header,\n      Body,\n    },\n    data: () => {\n      return {\n        sections: sections,\n        tagStore: TagStore,\n      };\n    },\n  };\n</script>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Home.vue?vue&type=template&id=1bc065bc\"\nimport script from \"./Home.vue?vue&type=script&lang=js\"\nexport * from \"./Home.vue?vue&type=script&lang=js\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{attrs:{\"id\":\"post-wrapper\"}},[_c('v-breadcrumbs',{staticClass:\"px-0\",attrs:{\"items\":_vm.items}}),_c('router-view')],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div id=\"post-wrapper\">\n    <!-- px-0 is a Material Design formatted classname -->\n    <v-breadcrumbs class=\"px-0\" :items=\"items\"/>\n    <router-view/>\n  </div>\n</template>\n\n<script>\n  function convertSlugToTitle(slug) {\n    return slug\n      .replace(/_|-/g, ' ')\n      .replace(/(^|\\s)\\S/g, t => t.toUpperCase());\n  }\n\n  export default {\n    name: 'PostWrapper',\n    data: function() {\n      const slug = this.$route.path.split('/').pop();\n\n      return {\n        items: [\n          { text: 'Home', link: true, href: '/' },\n          { text: convertSlugToTitle(slug), disabled: true }\n        ]\n      }\n    }\n  };\n</script>\n\n<style>\n\t#post-wrapper p {\n\t\tmargin: 12px 0;\n\t}\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./PostWrapper.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./PostWrapper.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./PostWrapper.vue?vue&type=template&id=11b6bad8\"\nimport script from \"./PostWrapper.vue?vue&type=script&lang=js\"\nexport * from \"./PostWrapper.vue?vue&type=script&lang=js\"\nimport style0 from \"./PostWrapper.vue?vue&type=style&index=0&id=11b6bad8&prod&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h2',[_vm._v(\"Motivation for large margin classifiers\")]),_c('p',[_vm._v(\" Remember that with logistic regression we assume that training data is generally linearly separable and our objective is to find a decision boundary between them. Of course, if there are outliers logicstic regression may still work well enough, and there are interesting hacks that can be used to apply even logistic regression to more than two classes. \")]),_c('p',[_vm._v(\" As a result, it turns out that logistic regression could probably be used in many of the cases that one may choose a large margin classifier, but the latter is computationally less expensive and may provide a cleaner and more accurate decision boundary. \")]),_c('p',[_vm._v(\" Large margin classifiers are probably always chosen over logistic regression in practice, but it is helpful to describe them by formulating logistic regression first. \")]),_c('p',[_vm._v(\" (Note that large margin classifiers are commonly called Support Vector Machines, but for consistency I continue to refer to them as large margin classifiers throught the rest of this page.) \")]),_c('h2',[_vm._v(\"Logistic regression\")]),_c('p',[_vm._v(\" Logistic regression gives us a function by which we can learn to predict whether or not a data point belongs to class 0 or 1: \")]),_c('p',[_vm._v(\" $$h(\\\\theta) = \\\\frac{1}{1 + e^{-\\\\theta^Tx}}$$ \")]),_c('p',[_vm._v(\" \\\\(h(\\\\theta)\\\\) is a good predictor if \\\\(\\\\theta^Tx\\\\) is really big when y (the true class label) is equal to 1, and \\\\(\\\\theta^Tx\\\\) is really small when y is equal to 0. \")]),_c('p',[_vm._v(\" The loss incurred given input example \\\\(x^i\\\\) and a set of parameters \\\\(\\\\theta\\\\) is: \")]),_c('p',[_vm._v(\" $$J(\\\\theta) = -\\\\Big(y^i \\\\cdot log(h_{\\\\theta}(x^i)) + (1 - y^i) \\\\cdot log(1 - h_{\\\\theta}(x^i))\\\\Big)$$ \")]),_c('p',[_vm._v(\" For example, if \\\\(\\\\theta^Tx\\\\) is really big, \\\\(h(\\\\theta)\\\\) ends up predicting that y = 1. \\\\(log(1) = 0\\\\), so the total loss given these particular parameters for this training example \\\\(i\\\\) is 0. \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"200px\"},attrs:{\"src\":require(\"./logistic-regression.png\")}}),_c('p',{staticStyle:{\"width\":\"300px\",\"color\":\"#888\"}},[_vm._v(\"Loss function for logistic regression in the case where y = 1.\")])]),_c('p',[_vm._v(\" Even in the case where \\\\(h(\\\\theta)\\\\) correctly predicts y = 1, training data will continue to affect the output of the loss function. More interestingly, data points that lie further from the decision boundary have a much larger affect on the loss function. \")]),_c('h2',[_vm._v(\"Large margin classification\")]),_c('p',[_vm._v(\" Often, hinge loss is used in place of logistic loss to change the cost distribution. As long as \\\\(\\\\theta^Tx\\\\) is \\\"good enough\\\", it will no longer change the loss. Likewise, the cost grows only linearly as \\\\(\\\\theta^Tx\\\\) becomes further from the correct prediction. \")]),_c('p',[_vm._v(\" Another property of the hinge loss is its computational efficiency. \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"200px\"},attrs:{\"src\":require(\"./large-margin.png\")}}),_c('p',{staticStyle:{\"width\":\"300px\",\"color\":\"#888\"}},[_vm._v(\"The green, \\\\(cost_1\\\\) line is our new cost function.\")])]),_c('p',[_vm._v(\" This brings us to our loss function for large margin classifiers: \")]),_c('p',[_vm._v(\" $$J(\\\\theta) = C \\\\sum_{i=1}^{m}\\\\Big(y^i \\\\cdot cost_1(\\\\theta^Tx^i) + (1 - y^i)cost_0(\\\\theta^Tx^i)\\\\Big) + \\\\frac{1}{2}\\\\sum_{j=1}^{n}\\\\theta^2_j$$ \")]),_c('div',{staticStyle:{\"display\":\"flex\",\"justify-content\":\"center\",\"margin-bottom\":\"20px\"}},[_c('ul',[_c('li',[_vm._v(\"When \\\\(y = 1\\\\), we want \\\\(\\\\theta^Tx\\\\) to be >= 1\")]),_c('li',[_vm._v(\"When \\\\(y = 0\\\\), we want \\\\(\\\\theta^Tx\\\\) to be <= -1\")])])]),_c('p',[_vm._v(\" C helps us to control how much each training example effects the total loss: By choosing a larger C our there is more motivation to overfit our model to the training data. \")]),_c('p',[_vm._v(\" Also note the use of L2 regularization: \\\\(\\\\frac{1}{2}\\\\sum_{j=1}^{n}\\\\theta^2_j\\\\). In the next section, we'll see why this, paired with our constraints on \\\\(\\\\theta^Tx\\\\), leads to a more intuitive decision boundary. \")]),_c('h2',[_vm._v(\"Large margin intuition\")]),_c('p',[_vm._v(\" We'll use the following known properties in this section: \")]),_c('ol',[_c('li',[_vm._v(\" \\\\(u^Tv = p \\\\cdot \\\\Vert u \\\\Vert\\\\), where \\\\(p\\\\) is the length of the projection of \\\\(v\\\\) onto \\\\(u\\\\) \")]),_c('li',[_vm._v(\" \\\\(\\\\Vert u \\\\Vert = \\\\sqrt{u_1^2 + u_2^2}\\\\) \")])]),_c('p',[_vm._v(\" Consider the fact that we used L2 regularization in our objective function. By applying the second of these properties: \")]),_c('p',[_vm._v(\" $$\\\\frac{1}{2}\\\\sum_{j=1}^{n}\\\\theta^2_j = \\\\frac{1}{2} \\\\Big(\\\\sqrt{\\\\theta_0^2 + \\\\theta_1^2}\\\\Big)^2 = \\\\frac{1}{2} \\\\Vert \\\\theta \\\\Vert^2$$ \")]),_c('p',[_vm._v(\" Remember that during optimization, we want \\\\(\\\\theta^Tx\\\\) to be >= 1. If our regularization term is weighted sufficiently (with a small enough C), we will be focused more on minimizing \\\\(\\\\theta\\\\) and less on fitting the decision boundary to each training instance. Therefore, during optimization we will be maximizing the projection of x onto \\\\(\\\\theta\\\\) because for \\\\(p \\\\cdot \\\\Vert \\\\theta \\\\Vert >= 1\\\\) to hold true, \\\\(p\\\\) will have to be very large. \")]),_c('p',[_vm._v(\" It's pretty hard to draw this relationship in a way that ends up being convincing, but I'll do my best. The first illustration shows a decision boundary that a large margin classifier would probably find: \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"200px\"},attrs:{\"src\":require(\"./good.png\")}}),_c('p',{staticStyle:{\"width\":\"300px\",\"color\":\"#888\"}},[_vm._v(\"The solid green line represents the vector \\\\(\\\\theta\\\\). The dotted green line represents the resulting decision boundary. The magenta line shows that the training example with the shortest projection onto \\\\(\\\\theta\\\\) has been maximized.\")])]),_c('p',[_vm._v(\" Each line is described in the caption, but the main point here is that for each of the training examples, the classifer is attempting to maximize its projection onto \\\\(\\\\theta\\\\). As a result, we get this imaginary decision boundary that is far from each of the training examples. \")]),_c('p',[_vm._v(\" To see an example of a bad classification, see the image below: \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"200px\"},attrs:{\"src\":require(\"./bad.png\")}}),_c('p',{staticStyle:{\"width\":\"300px\",\"color\":\"#888\"}},[_vm._v(\"The magenta line here is the nearest projection given the same training examples, but a different \\\\(\\\\theta\\\\) vector. The dashed green line makes it clear which example is projecting.\")])]),_c('p',[_vm._v(\" This example, while still perfectly separating all of the training examples, is still not very good one. That's because the projections of many of the training examples onto \\\\(\\\\theta\\\\) are much smaller than they were in the illustration above. \")])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h2>Motivation for large margin classifiers</h2>\n    <p>\n      Remember that with logistic regression we assume that training data is generally linearly separable and our objective is to find a decision boundary between them. Of course, if there are outliers logicstic regression may still work well enough, and there are interesting hacks that can be used to apply even logistic regression to more than two classes.\n    </p>\n    <p>\n      As a result, it turns out that logistic regression could probably be used in many of the cases that one may choose a large margin classifier, but the latter is computationally less expensive and may provide a cleaner and more accurate decision boundary.\n    </p>\n    <p>\n      Large margin classifiers are probably always chosen over logistic regression in practice, but it is helpful to describe them by formulating logistic regression first.\n    </p>\n    <p>\n      (Note that large margin classifiers are commonly called Support Vector Machines, but for consistency I continue to refer to them as large margin classifiers throught the rest of this page.)\n    </p>\n\n    <h2>Logistic regression</h2>\n    <p>\n      Logistic regression gives us a function by which we can learn to predict whether or not a data point belongs to class 0 or 1:\n    </p>\n    <p>\n      $$h(\\theta) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n    </p>\n    <p>\n      \\(h(\\theta)\\) is a good predictor if \\(\\theta^Tx\\) is really big when y (the true class label) is equal to 1, and \\(\\theta^Tx\\) is really small when y is equal to 0.\n    </p>\n    <p>\n      The loss incurred given input example \\(x^i\\) and a set of parameters \\(\\theta\\) is:\n    </p>\n    <p>\n      $$J(\\theta) = -\\Big(y^i \\cdot log(h_{\\theta}(x^i)) + (1 - y^i) \\cdot log(1 - h_{\\theta}(x^i))\\Big)$$\n    </p>\n    <p>\n      For example, if \\(\\theta^Tx\\) is really big, \\(h(\\theta)\\) ends up predicting that y = 1. \\(log(1) = 0\\), so the total loss given these particular parameters for this training example \\(i\\) is 0.\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./logistic-regression.png' style='height:200px'/>\n      <p style='width: 300px;color:#888'>Loss function for logistic regression in the case where y = 1.</p>\n    </div>\n    <p>\n      Even in the case where \\(h(\\theta)\\) correctly predicts y = 1, training data will continue to affect the output of the loss function. More interestingly, data points that lie further from the decision boundary have a much larger affect on the loss function.\n    </p>\n\n    <h2>Large margin classification</h2>\n    <p>\n      Often, hinge loss is used in place of logistic loss to change the cost distribution. As long as \\(\\theta^Tx\\) is \"good enough\", it will no longer change the loss. Likewise, the cost grows only linearly as \\(\\theta^Tx\\) becomes further from the correct prediction.\n    </p>\n    <p>\n      Another property of the hinge loss is its computational efficiency.\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./large-margin.png' style='height:200px'/>\n      <p style='width: 300px;color:#888'>The green, \\(cost_1\\) line is our new cost function.</p>\n    </div>\n    <p>\n      This brings us to our loss function for large margin classifiers:\n    </p>\n    <p>\n      $$J(\\theta) = C \\sum_{i=1}^{m}\\Big(y^i \\cdot cost_1(\\theta^Tx^i) + (1 - y^i)cost_0(\\theta^Tx^i)\\Big) + \\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j$$\n    </p>\n    <div style='display:flex;justify-content:center;margin-bottom:20px'>\n      <ul>\n        <li>When \\(y = 1\\), we want \\(\\theta^Tx\\) to be >= 1</li>\n        <li>When \\(y = 0\\), we want \\(\\theta^Tx\\) to be &lt;= -1</li>\n      </ul>\n    </div>\n    <p>\n      C helps us to control how much each training example effects the total loss: By choosing a larger C our there is more motivation to overfit our model to the training data.\n    </p>\n    <p>\n      Also note the use of L2 regularization: \\(\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j\\). In the next section, we'll see why this, paired with our constraints on \\(\\theta^Tx\\), leads to a more intuitive decision boundary.\n    </p>\n\n    <h2>Large margin intuition</h2>\n\n    <p>\n      We'll use the following known properties in this section:\n    </p>\n    <ol>\n      <li>\n        \\(u^Tv = p \\cdot \\Vert u \\Vert\\), where \\(p\\) is the length of the projection of \\(v\\) onto \\(u\\)\n      </li>\n      <li>\n        \\(\\Vert u \\Vert = \\sqrt{u_1^2 + u_2^2}\\)\n      </li>\n    </ol>\n    <p>\n      Consider the fact that we used L2 regularization in our objective function. By applying the second of these properties:\n    </p>\n    <p>\n      $$\\frac{1}{2}\\sum_{j=1}^{n}\\theta^2_j = \\frac{1}{2} \\Big(\\sqrt{\\theta_0^2 + \\theta_1^2}\\Big)^2 = \\frac{1}{2} \\Vert \\theta \\Vert^2$$\n    </p>\n    <p>\n      Remember that during optimization, we want \\(\\theta^Tx\\) to be >= 1. If our regularization term is weighted sufficiently (with a small enough C), we will be focused more on minimizing \\(\\theta\\) and less on fitting the decision boundary to each training instance. Therefore, during optimization we will be maximizing the projection of x onto \\(\\theta\\) because for \\(p \\cdot \\Vert \\theta \\Vert >= 1\\) to hold true, \\(p\\) will have to be very large.\n    </p>\n    <p>\n      It's pretty hard to draw this relationship in a way that ends up being convincing, but I'll do my best. The first illustration shows a decision boundary that a large margin classifier would probably find:\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./good.png' style='height:200px'/>\n      <p style='width: 300px;color:#888'>The solid green line represents the vector \\(\\theta\\). The dotted green line represents the resulting decision boundary. The magenta line shows that the training example with the shortest projection onto \\(\\theta\\) has been maximized.</p>\n    </div>\n    <p>\n      Each line is described in the caption, but the main point here is that for each of the training examples, the classifer is attempting to maximize its projection onto \\(\\theta\\). As a result, we get this imaginary decision boundary that is far from each of the training examples.\n    </p>\n    <p>\n      To see an example of a bad classification, see the image below:\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./bad.png' style='height:200px'/>\n      <p style='width: 300px;color:#888'>The magenta line here is the nearest projection given the same training examples, but a different \\(\\theta\\) vector. The dashed green line makes it clear which example is projecting.</p>\n    </div>\n    <p>\n      This example, while still perfectly separating all of the training examples, is still not very good one. That's because the projections of many of the training examples onto \\(\\theta\\) are much smaller than they were in the illustration above.\n    </p>\n  </div>\n</template>\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  .image-container {\n    padding: 18px;\n    display: flex;\n    justify-content: center;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=79d6fb9a&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=79d6fb9a&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"79d6fb9a\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h1',[_vm._v(\"Singular Value Decomposition: Two Perspectives\")]),_c('p',[_vm._v(\" The equality \\\\(A = U \\\\Sigma V^{T}\\\\) is one of most important in linear algebra. In this essay, I want to make SVD a little more intuitive. I'll try to do so with examples, visualizations, and the corresponding math for completeness. \")]),_c('p',[_vm._v(\" I'll look at SVD in the following contexts: \")]),_c('ol',[_c('li',[_vm._v(\"Approximating a rank r matrix with a rank k matrix.\")]),_c('li',[_vm._v(\"Understanding the relationships between the four fundamental subspaces.\")])]),_c('h2',[_vm._v(\"1. Approximating a rank r matrix with a rank k matrix\")]),_c('p',[_vm._v(\" In the first perspective a matrix of rank r is broken down into a set of rank 1 matrices. \")]),_c('p',[_vm._v(\" To solve \\\\(A = U \\\\Sigma V^{T}\\\\) we split the problem into two parts, solving for \\\\(U\\\\) in the first and \\\\(V\\\\) in the second, by diagonalizing \\\\(AA^{T}\\\\) and \\\\(A^{T}A\\\\). In both cases, we're diagonalizing a positive semidefinite matrix, and we end up with two equivalent orthonormal matrices of eigenvectors and one eigenvalue matrix. For example, finding the columns of \\\\(V\\\\) would involve the following: \")]),_c('p',[_vm._v(\" $$A^{T}A = V \\\\Sigma^{T} U^{T} U \\\\Sigma V^{T} = V \\\\Sigma^2 V^{T}$$ \")]),_c('p',[_vm._v(\" What does this really mean? Because \\\\(V \\\\Sigma^2 V^{T}\\\\) diagonalizes \\\\(A^{T}A\\\\), each element \\\\(v_i\\\\) is an eigenvector. Similarly, each \\\\(u_i\\\\) is an eigenvector of \\\\(AA^{T}\\\\). Suppose \\\\(A\\\\) is rank r. Then the following are true: \")]),_c('ol',[_c('li',[_vm._v(\"\\\\(v_1 \\\\cdots v_r\\\\) form an orthonormal basis.\")]),_c('li',[_vm._v(\"\\\\(u_1 \\\\cdots u_r\\\\) form an orthonormal basis.\")]),_c('li',[_vm._v(\"\\\\(r\\\\) is equal to the number of non-zero diagonal values of \\\\(\\\\Sigma\\\\).\")])]),_c('p',[_vm._v(\" All vectors of \\\\(V\\\\) and \\\\(U\\\\) beyond the first r comprise the nullspace, and their corresponding diagonal entries in \\\\(\\\\Sigma\\\\) are zero. \")]),_c('p',[_vm._v(\" We can express A as a sum of r rank 1 matrices, each the outer product of a left singular vector, it's singular value, and the corresponding right singular vector: \")]),_c('p',[_vm._v(\"$$A = \\\\sum_{i=1}^{r} \\\\sigma_i u_i v_i^T$$\")]),_c('p',[_vm._v(\"Suppose we start with the following image matrix A:\")]),_c('div',{staticClass:\"image-container\"},[_c('img',{attrs:{\"src\":require(\"./panama.jpg\")}})]),_c('p',[_vm._v(\" If I run SVD on A, and set all of the items on the diagonal of \\\\(\\\\Sigma\\\\) to 0 except for the most significant \\\\(\\\\sigma_1\\\\), and then render the resulting recombination, I end up with the following: \")]),_c('div',{staticClass:\"image-container\"},[_c('img',{attrs:{\"src\":require(\"./panama-1.png\")}})]),_c('p',[_vm._v(\" In this first matrix, each row is really just the combination of the first left singular vector, the first singular value, and the first right singular vector: \\\\(A' = \\\\sigma_1 u_1 v_1^T\\\\). \")]),_c('p',[_vm._v(\" Taking a look at the first 10 rank 1 matricies gives us: \")]),_c('div',{staticClass:\"image-container\"},[_c('img',{attrs:{\"src\":require(\"./panama-10.png\")}})]),_c('p',[_vm._v(\" We can now start to see many of the features of the original image! Each singular vector of \\\\(U\\\\) and \\\\(V\\\\) encodes information from the original matrix \\\\(A\\\\). \")]),_c('h2',[_vm._v(\"2. Understanding the relationships between the four fundamental subspaces\")]),_c('p',[_vm._v(\" Let's instead look at \\\\(A\\\\) as a function from \\\\(R^n \\\\rightarrow R^m\\\\). Because \\\\(U\\\\) is orthogonal with sides of length m and \\\\(V\\\\) is orthogonal with sides of length n, we can represent any \\\\(x\\\\) as \\\\(Vc\\\\), where \\\\(c\\\\) describes how to make \\\\(x\\\\) out of basis \\\\(V\\\\). \")]),_c('p',[_vm._v(\" Looking at \\\\(A\\\\) in this way, we can write \\\\(Ax = U \\\\Sigma V^T Vc = U \\\\Sigma c\\\\): \")]),_c('div',{staticClass:\"image-container\"},[_c('img',{attrs:{\"src\":require(\"./functionA.png\")}})]),_c('p',[_vm._v(\" So \\\\(x = Vc\\\\) and \\\\(Ax = U \\\\Sigma c\\\\). The input, a vector in \\\\(R^n\\\\), is transformed to \\\\(R^m\\\\) via the function that is \\\\(A\\\\). \")]),_c('p',[_vm._v(\" The number of non-zero values in \\\\(c\\\\) is equal to the rank of both \\\\(A\\\\) and \\\\(A^T\\\\). When calculating the output \\\\(Ax\\\\), the remaining \\\\(m-r\\\\) zero entries in \\\\(c\\\\) will cancel out the last \\\\(m-r\\\\) columns of \\\\(U\\\\). \")]),_c('p',[_vm._v(\" Most importantly, this brings to light the four fundamental subspaces that are players in the \\\\(Ax\\\\) operation: \")]),_c('ol',[_c('li',[_vm._v(\"The \"),_c('strong',[_vm._v(\"nullspace of \\\\(A^T\\\\)\")]),_vm._v(\" is a subspace of \\\\(R^m\\\\) that is \\\"unreachable\\\" during the operation \\\\(Ax\\\\). It's the last \\\\(m-r\\\\) columns of \\\\(U\\\\) in our SVD and can never be reached because \\\\(\\\\Sigma c \\\\in R^m\\\\), and the \\\\(r-m\\\\) final vector positions are zeros.\")]),_c('li',[_vm._v(\"The \"),_c('strong',[_vm._v(\"nullspace of \\\\(A\\\\)\")]),_vm._v(\" is a subspace of \\\\(R^n\\\\). Similarly here, changes in this subspace don't matter because they'll ultimately be killed by the zero entries of \\\\(\\\\Sigma\\\\). We'll call this the \\\"doesn't matter\\\" space.\")]),_c('li',[_vm._v(\"The \"),_c('strong',[_vm._v(\"columnspace of A\")]),_vm._v(\" is a subspace of \\\\(R^m\\\\). In the SVD view, the column space is the part of \\\\(U\\\\) that can actually be manipulated, i.e., the \\\"reachable space\\\".\")]),_c('li',[_vm._v(\"The \"),_c('strong',[_vm._v(\"rowspace of A\")]),_vm._v(\" is a subspace of \\\\(R^n\\\\), and is the part of \\\\(x\\\\) that will affect the output \\\\(Ax\\\\). It's the \\\"matters\\\" space.\")])]),_c('p',[_vm._v(\"To sum up this perspective, SVD can be used as a tool to illustrate the fundamental subspaces of \\\\(A\\\\). When \\\\(A\\\\) is viewed as a function, it can help us to understand what changes in the input will make an inpact on the output \\\\(Ax\\\\).\")]),_c('small',[_vm._v(\"Robert Kotcher, 2019\")])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h1>Singular Value Decomposition: Two Perspectives</h1>\n    <p>\n      The equality \\(A = U \\Sigma V^{T}\\) is one of most important in linear algebra. In this essay, I want to make SVD a little more intuitive. I'll try to do so with examples, visualizations, and the corresponding math for completeness.\n    </p>\n    <p>\n      I'll look at SVD in the following contexts:\n    </p>\n    <ol>\n      <li>Approximating a rank r matrix with a rank k matrix.</li>\n      <li>Understanding the relationships between the four fundamental subspaces.</li>\n    </ol>\n    <h2>1. Approximating a rank r matrix with a rank k matrix</h2>\n    <p>\n      In the first perspective a matrix of rank r is broken down into a set of rank 1 matrices.\n    </p>\n    <p>\n      To solve \\(A = U \\Sigma V^{T}\\) we split the problem into two parts, solving for \\(U\\) in the first and \\(V\\) in the second, by diagonalizing \\(AA^{T}\\) and \\(A^{T}A\\). In both cases, we're diagonalizing a positive semidefinite matrix, and we end up with two equivalent orthonormal matrices of eigenvectors and one eigenvalue matrix. For example, finding the columns of \\(V\\) would involve the following:\n    </p>\n    <p>\n      $$A^{T}A = V \\Sigma^{T} U^{T} U \\Sigma V^{T} = V \\Sigma^2 V^{T}$$\n    </p>\n    <p>\n      What does this really mean? Because \\(V \\Sigma^2 V^{T}\\) diagonalizes \\(A^{T}A\\), each element \\(v_i\\) is an eigenvector. Similarly, each \\(u_i\\) is an eigenvector of \\(AA^{T}\\). Suppose \\(A\\) is rank r. Then the following are true:\n    </p>\n    <ol>\n      <li>\\(v_1 \\cdots v_r\\) form an orthonormal basis.</li>\n      <li>\\(u_1 \\cdots u_r\\) form an orthonormal basis.</li>\n      <li>\\(r\\) is equal to the number of non-zero diagonal values of \\(\\Sigma\\).</li>\n    </ol>\n    <p>\n      All vectors of \\(V\\) and \\(U\\) beyond the first r comprise the nullspace, and their corresponding diagonal entries in \\(\\Sigma\\) are zero.\n    </p>\n    <p>\n      We can express A as a sum of r rank 1 matrices, each the outer product of a left singular vector, it's singular value, and the corresponding right singular vector:\n    </p>\n    <p>$$A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$</p>\n    <p>Suppose we start with the following image matrix A:</p>\n    <div class='image-container'>\n      <img src='./panama.jpg'/>\n    </div>\n    <p>\n      If I run SVD on A, and set all of the items on the diagonal of \\(\\Sigma\\) to 0 except for the most significant \\(\\sigma_1\\), and then render the resulting recombination, I end up with the following:\n    </p>\n    <div class='image-container'>\n      <img src='./panama-1.png'/>\n    </div>\n    <p>\n      In this first matrix, each row is really just the combination of the first left singular vector, the first singular value, and the first right singular vector: \\(A' = \\sigma_1 u_1 v_1^T\\).\n    </p>\n    <p>\n      Taking a look at the first 10 rank 1 matricies gives us:\n    </p>\n    <div class='image-container'>\n      <img src='./panama-10.png'/>\n    </div>\n    <p>\n      We can now start to see many of the features of the original image! Each singular vector of \\(U\\) and \\(V\\) encodes information from the original matrix \\(A\\).\n    </p>\n\n    <h2>2. Understanding the relationships between the four fundamental subspaces</h2>\n\n    <p>\n      Let's instead look at \\(A\\) as a function from \\(R^n \\rightarrow R^m\\). Because \\(U\\) is orthogonal with sides of length m and \\(V\\) is orthogonal with sides of length n, we can represent any \\(x\\) as \\(Vc\\), where \\(c\\) describes how to make \\(x\\) out of basis \\(V\\).\n    </p>\n    <p>\n      Looking at \\(A\\) in this way, we can write \\(Ax = U \\Sigma V^T Vc = U \\Sigma c\\):\n    </p>\n    <div class='image-container'>\n      <img src='./functionA.png'/>\n    </div>\n    <p>\n      So \\(x = Vc\\) and \\(Ax = U \\Sigma c\\). The input, a vector in \\(R^n\\), is transformed to \\(R^m\\) via the function that is \\(A\\).\n    </p>\n    <p>\n      The number of non-zero values in \\(c\\) is equal to the rank of both \\(A\\) and \\(A^T\\). When calculating the output \\(Ax\\), the remaining \\(m-r\\) zero entries in \\(c\\) will cancel out the last \\(m-r\\) columns of \\(U\\).\n    </p>\n    <p>\n      Most importantly, this brings to light the four fundamental subspaces that are players in the \\(Ax\\) operation:\n    </p>\n    <ol>\n      <li>The <strong>nullspace of \\(A^T\\)</strong> is a subspace of \\(R^m\\) that is \"unreachable\" during the operation \\(Ax\\). It's the last \\(m-r\\) columns of \\(U\\) in our SVD and can never be reached because \\(\\Sigma c \\in R^m\\), and the \\(r-m\\) final vector positions are zeros.</li>\n      <li>The <strong>nullspace of \\(A\\)</strong> is a subspace of \\(R^n\\). Similarly here, changes in this subspace don't matter because they'll ultimately be killed by the zero entries of \\(\\Sigma\\). We'll call this the \"doesn't matter\" space.</li>\n      <li>The <strong>columnspace of A</strong> is a subspace of \\(R^m\\). In the SVD view, the column space is the part of \\(U\\) that can actually be manipulated, i.e., the \"reachable space\".</li>\n      <li>The <strong>rowspace of A</strong> is a subspace of \\(R^n\\), and is the part of \\(x\\) that will affect the output \\(Ax\\). It's the \"matters\" space.</li>\n    </ol>\n\n    <p>To sum up this perspective, SVD can be used as a tool to illustrate the fundamental subspaces of \\(A\\). When \\(A\\) is viewed as a function, it can help us to understand what changes in the input will make an inpact on the output \\(Ax\\).</p>\n\n    <small>Robert Kotcher, 2019</small>\n  </div>\n</template>\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  h2 {\n    margin: 18px 0;\n    line-height: 1.2;\n  }\n\n  .image-container {\n    padding: 18px;\n    display: flex;\n    justify-content: center;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=a8ae26b0&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=a8ae26b0&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"a8ae26b0\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h1',[_vm._v(\"I made a Vanilla Neural Network from Scratch\")]),_c('h2',[_vm._v(\"Motivation\")]),_c('p',[_vm._v(\" As I continue to grow my understanding of deep learning, it seems an inevitable rite of passage to build a functional neural network without the help of libraries. I have tinkered with the likes of Keras and Tensorflow, always amazed by the results and still yet somehow unfulfilled knowing that the brilliance behind what was actually happening was inside the black box. \")]),_c('p',[_vm._v(\" This project started with the intention to calculate a few derivatives and translate those over to python code. In the end, I learned so much that it seemed worthwhile to write down every last detail to ensure that my future self would be able to revisit what I did without any confusion. \")]),_c('h2',[_vm._v(\"Neural networks are functions\")]),_c('p',[_vm._v(\" I'll use the words \\\"neural network\\\" and \\\"function\\\" interchangeably in this writeup. The operations that together make up a neural network form a computational graph when expressed visually. We'll be walking through the example graph that is depicted below. It's actually pretty hard to illustrate every last computational detail visually, and I try to elaborate in words where the imagery is lacking. \")]),_c('p',[_vm._v(\" In general, inputs to a neural network consist of some data that you are trying to understand, the function reshapes that data such that the output can be provide insight into the input. For just a few examples, the output can classify the input (\\\"this is a picture of a cat\\\") or it can predict the next value in a sequence (\\\"My favorite movie is The Jedi Strikes ___\\\"). \")]),_c('p',[_vm._v(\" For now, it actually doesn't matter a whole lot what we expect our example network to do. We can assume that the data we use is arbitrary. This example is used more to show the mechanics neural networks, how to measure the success of their output, and how to optimize (or \\\"train\\\") their parameters in order to improve output accuracy. \")]),_c('p',[_vm._v(\" I mentioned that a neural network is just a function. Let's first look at what exactly this function looks like, and then we'll talk about how to \\\"teach\\\" this function to learn our task at hand. \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"350px\"},attrs:{\"src\":require(\"./basic-net.png\")}}),_c('p',{staticStyle:{\"width\":\"500px\",\"color\":\"#888\"}},[_vm._v(\"A neural network with input \\\\(X\\\\), two bias parameters \\\\(B_1\\\\) and \\\\(B_2\\\\), a hidden layer with three neurons, and output \\\\(O\\\\). Edges connecting \\\\(X\\\\) to \\\\(H\\\\), and \\\\(H\\\\) to \\\\(O\\\\) each contain a weight. The image above only shows \\\\(w_1\\\\).\")])]),_c('p',[_vm._v(\" The image above is a representation of a pretty straightforward function. It takes two input scalars, passes them to a \\\"hidden layer\\\", which in turn spits out two output scalars. This same thing can be concisely represented with the function: \")]),_c('p',[_vm._v(\"$$f(\\\\theta, X) = \\\\phi_2(\\\\phi_1(XW_1 + B_1)W_2 + B_2)$$\")]),_c('p',[_vm._v(\" The \\\\(\\\\theta\\\\) stands for \\\"all of the parameters involved in calculating the output of this function\\\", and will be omitted from the function signature for brevity. \")]),_c('h5',[_vm._v(\"Expressing the transformation from \\\\(X\\\\) to \\\\(H\\\\) in matrix notation\")]),_c('p',[_vm._v(\" Let's move across the network from left to right and see how this relates to the underlying math. \")]),_c('p',[_vm._v(\"The \\\\(X\\\\) matrix contains the inputs to our function. In this example it has one row and two columns. \")]),_c('p',[_vm._v(\" $$X = \\\\begin{bmatrix}x_1 & x_2\\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" You can see that \\\\(X\\\\) is multiplied by \\\\(W_1\\\\). I allude to \\\\(W_1\\\\) in the image above with \\\\(w_1\\\\), but \\\\(W_1\\\\) (capital \\\"W\\\") is a matrix containing the \\\\(w_1\\\\) edge weight together with 5 others. \")]),_c('p',[_vm._v(\" $$W_1 = \\\\begin{bmatrix}w_1 & w_3 & w_5\\\\\\\\w_2 & w_4 & w_6\\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\"You'll often see a \\\"bias\\\" value, in this case \\\\(B_1\\\\), which is used to translate the hidden state away from the origin. In other words, bias values are added to the matrix that results from multiplying input values by the weight parameters. \")]),_c('p',[_vm._v(\" $$B_1 = \\\\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" Let's put together the first half of this network, which takes us from the input \\\\(X\\\\) to the hidden layer: \")]),_c('p',[_vm._v(\" $$XW_1 + B_1 = \\\\begin{bmatrix}x_1 & x_2\\\\end{bmatrix} \\\\begin{bmatrix}w_1 & w_3 & w_5\\\\\\\\w_2 & w_4 & w_6\\\\end{bmatrix} + \\\\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\\\end{bmatrix} = \\\\begin{bmatrix}h_1 & h_2 & h_3\\\\end{bmatrix} = H$$ \")]),_c('h5',[_vm._v(\"Roles of each player\")]),_c('p',[_vm._v(\" Each of the players mentioned so far have different roles, and I want to make sure I'm particularly clear on what those are before moving forward. \")]),_c('p',[_vm._v(\" \\\\(X\\\\), \\\\(O\\\\), and \\\\(H\\\\) are, for lack of a better word, \\\"placeholders\\\" in the computational graph. For each execution of the function they will be filled in based on the current input. \")]),_c('p',[_vm._v(\" The weight matrices \\\\(W_1\\\\) and \\\\(W_2\\\\), as well as biases \\\\(B_1\\\\) and \\\\(B_2\\\\) are referred to as \\\"variables\\\", or maybe more commonly \\\"parameters\\\" or \\\"weights\\\". These players stay the same between executions. It's our goal to figure out the best parameter values during the training process. Typically these are \\\"initialized\\\" with random numbers. \")]),_c('h5',[_vm._v(\"Activation functions\")]),_c('p',[_vm._v(\" Returning to our function, we now arrive at the \\\"hidden layer\\\": \\\\(H = \\\\begin{bmatrix}h_1 & h_2 & h_3\\\\end{bmatrix}\\\\). There's one really important thing to note here: Each neuron \\\\(h_i\\\\) in the hidden layer is actually comprised of two numbers. The first is the immediate result of the matrix expression just mentioned. We'll refer to that as the \\\"net\\\" output. The second is the \\\"activated\\\" output. We'll call that \\\"out\\\" for brevity. I won't use the names \\\\(H\\\\) or \\\\(h_i\\\\) anymore because we actually need to be a little more specific. This section will explain how we'll refer to the hidden layer for the remainder of the writeup. \")]),_c('p',[_vm._v(\" You might have noticed that our equation above contained \\\\(\\\\phi_1\\\\) and \\\\(\\\\phi_2\\\\). This is where neural networks really get their power from. Both of these \\\\(\\\\phi\\\\)s are functions too, but they are a little different from what we've seen so far because they are non-linear. We call them \\\"activation\\\" functions because they activate the output of the linear operations inside of them. (An important thing to note about activation functions is that they must be differentiable.) \")]),_c('p',[_vm._v(\" There are lots of types of activation functions. For this example, we'll use the logistic activation function \\\\(\\\\phi(x) = \\\\frac{1}{1 + e^{-x}}\\\\). \")]),_c('p',[_vm._v(\" We said that \\\\(H = XW_1 + B_1\\\\), which is true. But let's be more specific and say that \\\\(XW_1 + B_1 = H_{net} = \\\\begin{bmatrix}h_{net,1} & h_{net,2} & h_{net,3}\\\\end{bmatrix}\\\\) of net values. To this we apply the activation function to each item individually for the final out matrix of the hidden layer \\\\(H_{out} = \\\\begin{bmatrix}h_{out,1} & h_{out,2} & h_{out,3}\\\\end{bmatrix} = \\\\begin{bmatrix}\\\\phi(h_{net,1}) & \\\\phi(h_{net,2}) & \\\\phi(h_{net,3})\\\\end{bmatrix}\\\\). \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"220px\"},attrs:{\"src\":require(\"./zoom.png\")}}),_c('p',{staticStyle:{\"width\":\"500px\",\"color\":\"#888\"}},[_vm._v(\"\\\\(h_{out,1}\\\\), or the \\\"activated\\\" first element \\\\(h_1\\\\) in the hidden layer is calculated by applying the logistic activation function to \\\\(h_{net,1}\\\\).\")])]),_c('p',[_vm._v(\" The operation described here is to be done three times when going from the input layer \\\\(X\\\\) to the hidden layer \\\\(H\\\\), once for each \\\\(h_i\\\\). Referring back to the entire function \\\\(O = \\\\phi_2(\\\\phi_1(XW_1 + B_1)W_2 + B_2)\\\\), \\\\(h_{out,1}\\\\) ends up being the first element of \\\\(\\\\phi_1(XW_1 + B_1)\\\\). Expanding the matrix operations we get \\\\(h_{out,1} = \\\\phi((x_1w_1 + x_2w_2) + b_{1,1})\\\\). \")]),_c('p',[_vm._v(\" Likewise, \\\\(h_{out,2} = \\\\phi((x_1w_3 + x_2w_4) + b_{1,2})\\\\) and \\\\(h_{out,3} = \\\\phi((x_1w_5 + x_2w_6) + b_{1,3})\\\\). \")]),_c('h5',[_vm._v(\"Hidden layer to output layer\")]),_c('p',[_vm._v(\" We want to turn the \\\\(H_{out}\\\\) that we just calculated into the output layer \\\\(O\\\\). The output layer can also be though of in two parts, \\\\(O_{net}\\\\) and \\\\(O_{out}\\\\). \")]),_c('p',[_vm._v(\" To get to \\\\(O_{net}\\\\), we similarly multiply by a weight matrix \\\\(W_2\\\\), which has 3 rows and 2 columns of weight parameters, and add a bias \\\\(B_2\\\\) to the result. Finally we apply a logistic activation function element-wise to get to our final network output, \\\\(O_{out}\\\\). \")]),_c('p',[_vm._v(\" \\\\(O_{out}\\\\) contains the \\\"final output\\\" of our function. \")]),_c('h2',[_vm._v(\"Calculating loss\")]),_c('p',[_vm._v(\" Because the values of our parameters \\\\(W_1\\\\), \\\\(W_2\\\\), \\\\(B_1\\\\), and \\\\(B_2\\\\) were initialized randomly, the output of our function will, at first, be random as well. This is where the concept of \\\"loss\\\" comes in. Loss is a measure of how well a function is able to match the provided training label \\\\(T\\\\) for a particular data point. A label is really just another matrix, generally of the same shape as \\\\(O_{out}\\\\). By running both the label and output for a given training example through a loss function \\\\(E\\\\), we can get a numeric value that tells us how well our function performed on that data point. \")]),_c('p',[_vm._v(\" Let's use the squared error for this example. For an arbitrary training example with label \\\\(T = \\\\begin{bmatrix}t_{1} & t_{2}\\\\end{bmatrix}\\\\) we can calculate the squared loss as: \")]),_c('p',[_vm._v(\" $$E_{total}(\\\\theta) = \\\\sum_{i=1}^2\\\\frac{1}{2}(t_i - o_{out,i})^2$$ \")]),_c('p',[_vm._v(\" Notice that the error relies on \\\\(\\\\theta\\\\), which is really just a placeholder for all of the parameters in the model. \\\\(\\\\theta\\\\) again will be omitted in the future. Our goal is to figure out how to change all of the individual matrix values to minimize \\\\(E_{total}\\\\). \")]),_c('h2',[_vm._v(\"Minimizing error using gradient descent\")]),_c('p',[_vm._v(\" Up until this point we've seen that a neural network is just a function whose output accuracy is measured by a loss function. Next, we want to train the model parameters to minimize what the loss function returns. \")]),_c('p',[_vm._v(\" Gradient descent is a technique that is used to update paramater weights. By taking the derivative of the total error with respect to each parameter individually, we can determine how that parameter affects the total error. Consequently, this gives us insight on how to change the weight to decrease the total error. \")]),_c('p',[_vm._v(\" For example, to update the parameter \\\\(w_7\\\\), which is the edge connecting \\\\(h_1\\\\) and \\\\(o_1\\\\) above, you would perform the following update: \")]),_c('p',[_vm._v(\" $$w_7 := w_7 - \\\\alpha\\\\Big(\\\\frac{\\\\delta E_{total}}{\\\\delta w_7}\\\\Big)$$ \")]),_c('p',[_vm._v(\" \\\\(\\\\alpha\\\\) is called the \\\"learning rate\\\". It's just a parameter that helps determine how quickly the parameters are updated. A value too large may explode, and a value too small might take forever to converge. \")]),_c('p',[_vm._v(\" In the upcoming subsections, we'll work through the math required to perform updates on \\\\(w_7\\\\) and \\\\(w_1\\\\), although the same principle should be used to update all of the function's parameters. \")]),_c('h5',[_vm._v(\"Updating weight \\\\(w_7\\\\)\")]),_c('p',[_vm._v(\" Let's first locate where weight \\\\(w_7\\\\) even is in our computational graph. In the very first image up at the top I actually don't label it, but it's the edge connecting \\\\(h_1\\\\) with \\\\(o_1\\\\). To perform the update as described, we need to determine \\\\(\\\\frac{\\\\delta E_{total}}{\\\\delta w_7}\\\\). \")]),_c('p',[_vm._v(\" Breaking this down using the chain rule gives us: \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta E_{total}}{\\\\delta w_7} = \\\\frac{\\\\delta E_{total}}{\\\\delta o_{out,1}} \\\\frac{\\\\delta o_{out,1}}{\\\\delta o_{net,1}} \\\\frac{\\\\delta o_{net,1}}{\\\\delta w_7} \\\\) \")]),_c('p',[_vm._v(\" You can see that each of these derivatives are stepping backwards from the error to the weight being updated, which is why this step is called backpropagation. One of the nice features of backpropagation is that it gives us modular blocks that we can recycle when we're finding gradients higher up in the graph. We'll see this when we update \\\\(w_1\\\\) next. \")]),_c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"220px\"},attrs:{\"src\":require(\"./w7.png\")}}),_c('p',{staticStyle:{\"width\":\"500px\",\"color\":\"#888\"}},[_vm._v(\"The error that our network currently outputs for \\\\(o_1\\\\) is \\\\(\\\\frac{1}{2}(t_1 - o_{out,1})^2\\\\)\")])]),_c('p',[_vm._v(\" In order for us to find \\\\(\\\\frac{\\\\delta E_{total}}{\\\\delta w_7}\\\\) let's first individually solve for each item in the chain rule decomposition. \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta E_{total}}{\\\\delta o_{out,1}}\\\\): \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(E_{total} = \\\\frac{1}{2}(t_1 - o_{out,1})^2 + \\\\frac{1}{2}(t_2 - o_{out,2})^2\\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta E_{total}}{\\\\delta o_{out,1}} = -(t_1 - o_{out,1})\\\\). \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta o_{out,1}}{\\\\delta o_{net,1}}\\\\): \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(o_{out,1} = \\\\frac{1}{1 + e^{-o_{net,1}}}\\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta o_{out,1}}{\\\\delta o_{net,1}} = o_{out,1}(1 - o_{out,1})\\\\) \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta o_{net,1}}{\\\\delta w_7}\\\\): \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\( o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1} \\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta o_{net,1}}{\\\\delta w_7} = h_{out,1}\\\\) \")]),_c('p',[_vm._v(\" And the complete update to \\\\(w_7\\\\) would be: \")]),_c('p',[_vm._v(\" $$w_7 := w_7 - \\\\alpha\\\\Big( -(o_{out,1})(h_{out,1})(t_1 - o_{out,1})(1 - o_{out,1}) \\\\Big)$$ \")]),_c('p',[_vm._v(\" How is this actually used in practice? After a given training example is passed through the function, the resulting values for each of these variables are then used to update \\\\(w_7\\\\). In an actual coded implementation, for example, the values ending up in each of these variables would be cached during forward propagation so that they could be used to calculate the gradient during backpropagation. \")]),_c('h5',[_vm._v(\"Updating weight \\\\(w_1\\\\)\")]),_c('p',[_vm._v(\" Updating \\\\(w_1\\\\) is really similar to updating \\\\(w_7\\\\), the main difference being that \\\\(w_1\\\\) affects two components of \\\\(E_{total}\\\\): \\\\(E_1\\\\) and \\\\(E_2\\\\). \")]),_c('p',[_vm._v(\" $$ \\\\frac{\\\\delta E_{total}}{\\\\delta w_1} = \\\\frac{\\\\delta E_{total}}{\\\\delta h_{out,1}} \\\\frac{\\\\delta h_{out,1}}{\\\\delta h_{net,1}} \\\\frac{\\\\delta h_{net,1}}{\\\\delta w_1} $$ \")]),_c('p',[_vm._v(\" $$ \\\\frac{\\\\delta E_{total}}{\\\\delta w_1} = \\\\Big( \\\\frac{\\\\delta E_1}{\\\\delta h_{out,1}} + \\\\frac{\\\\delta E_2}{\\\\delta h_{out,1}} \\\\Big) \\\\frac{\\\\delta h_{out,1}}{\\\\delta h_{net,1}} \\\\frac{\\\\delta h_{net,1}}{\\\\delta w_1} $$ \")]),_c('p',[_vm._v(\" We'll find each of these partial derivatives, but note that we've already done these before for the most part when we did our updates for the weights in the hidden to output layer. \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta E_1}{\\\\delta h_{out,1}}\\\\): \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta E_1}{\\\\delta h_{out,1}} = \\\\frac{\\\\delta E_1}{\\\\delta o_{out,1}} \\\\frac{\\\\delta o_{out,1}}{\\\\delta o_{net,1}} \\\\frac{\\\\delta o_{net,1}}{\\\\delta h_{out,1}}\\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta E_1}{\\\\delta o_{out,1}} \\\\frac{\\\\delta o_{out,1}}{\\\\delta o_{net,1}} = -o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\\\). \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1}\\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta o_{net,1}}{\\\\delta h_{out,1}} = w_7\\\\) \")]),_c('p',{staticStyle:{\"margin-left\":\"30px\"}},[_vm._v(\" \\\\(\\\\frac{\\\\delta E_1}{\\\\delta h_{out,1}} = -w_7o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\\\) \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta E_2}{\\\\delta h_{out,1}} = -w_{10}o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\\\) \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta h_{out,1}}{\\\\delta h_{net,1}} = h_{out,1}(1 - h_{out,1})\\\\) \")]),_c('p',[_vm._v(\" \\\\(\\\\frac{\\\\delta h_{net,1}}{\\\\delta w_1} = x_1\\\\) \")]),_c('p',[_vm._v(\" Inserting all of these partial derivatives into the larger expression above will give us how we should update \\\\(w_1\\\\) for this training example. \")]),_c('p',[_vm._v(\" And there we have it! To see this neural network in action, you can check out the \"),_c('a',{attrs:{\"href\":\"./simple_nn_from_scratch.py\",\"download\":\"\"}},[_vm._v(\"corresponding code\")]),_vm._v(\". \")]),_c('small',[_vm._v(\"Robert Kotcher, 2019\")])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h1>I made a Vanilla Neural Network from Scratch</h1>\n    <h2>Motivation</h2>\n    <p>\n      As I continue to grow my understanding of deep learning, it seems an inevitable rite of passage to build a functional neural network without the help of libraries. I have tinkered with the likes of Keras and Tensorflow, always amazed by the results and still yet somehow unfulfilled knowing that the brilliance behind what was actually happening was inside the black box.\n    </p>\n    <p>\n      This project started with the intention to calculate a few derivatives and translate those over to python code. In the end, I learned so much that it seemed worthwhile to write down every last detail to ensure that my future self would be able to revisit what I did without any confusion.\n    </p>\n\n    <h2>Neural networks are functions</h2>\n    <p>\n      I'll use the words \"neural network\" and \"function\" interchangeably in this writeup. The operations that together make up a neural network form a computational graph when expressed visually. We'll be walking through the example graph that is depicted below. It's actually pretty hard to illustrate every last computational detail visually, and I try to elaborate in words where the imagery is lacking.\n    <p>\n      In general, inputs to a neural network consist of some data that you are trying to understand, the function reshapes that data such that the output can be provide insight into the input. For just a few examples, the output can classify the input (\"this is a picture of a cat\") or it can predict the next value in a sequence (\"My favorite movie is The Jedi Strikes ___\").\n    </p>\n    <p>\n      For now, it actually doesn't matter a whole lot what we expect our example network to do. We can assume that the data we use is arbitrary. This example is used more to show the mechanics neural networks, how to measure the success of their output, and how to optimize (or \"train\") their parameters in order to improve output accuracy.\n    </p>\n    <p>\n      I mentioned that a neural network is just a function. Let's first look at what exactly this function looks like, and then we'll talk about how to \"teach\" this function to learn our task at hand.\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./basic-net.png' style='height:350px'/>\n      <p style='width: 500px;color:#888'>A neural network with input \\(X\\), two bias parameters \\(B_1\\) and \\(B_2\\), a hidden layer with three neurons, and output \\(O\\). Edges connecting \\(X\\) to \\(H\\), and \\(H\\) to \\(O\\) each contain a weight. The image above only shows \\(w_1\\).</p>\n    </div>\n    <p>\n      The image above is a representation of a pretty straightforward function. It takes two input scalars, passes them to a \"hidden layer\", which in turn spits out two output scalars. This same thing can be concisely represented with the function:\n    </p>\n    <p>$$f(\\theta, X) = \\phi_2(\\phi_1(XW_1 + B_1)W_2 + B_2)$$</p>\n    <p>\n      The \\(\\theta\\) stands for \"all of the parameters involved in calculating the output of this function\", and will be omitted from the function signature for brevity.\n    </p>\n    <h5>Expressing the transformation from \\(X\\) to \\(H\\) in matrix notation</h5>\n    <p>\n      Let's move across the network from left to right and see how this relates to the underlying math.\n    </p>\n    <p>The \\(X\\) matrix contains the inputs to our function. In this example it has one row and two columns.\n    </p>\n    <p>\n      $$X = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}$$\n    </p>\n    <p>\n    You can see that \\(X\\) is multiplied by \\(W_1\\). I allude to \\(W_1\\) in the image above with \\(w_1\\), but \\(W_1\\) (capital \"W\") is a matrix containing the \\(w_1\\) edge weight together with 5 others.\n    </p>\n    <p>\n      $$W_1 = \\begin{bmatrix}w_1 & w_3 & w_5\\\\w_2 & w_4 & w_6\\end{bmatrix}$$\n    </p>\n    <p>You'll often see a \"bias\" value, in this case \\(B_1\\), which is used to translate the hidden state away from the origin. In other words, bias values are added to the matrix that results from multiplying input values by the weight parameters.\n    </p>\n    <p>\n      $$B_1 = \\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\end{bmatrix}$$\n    </p>\n    <p>\n    Let's put together the first half of this network, which takes us from the input \\(X\\) to the hidden layer:\n    </p>\n    <p>\n      $$XW_1 + B_1 = \\begin{bmatrix}x_1 & x_2\\end{bmatrix} \\begin{bmatrix}w_1 & w_3 & w_5\\\\w_2 & w_4 & w_6\\end{bmatrix} + \\begin{bmatrix}b_{1,1} & b_{1,2} & b_{1,3}\\end{bmatrix} = \\begin{bmatrix}h_1 & h_2 & h_3\\end{bmatrix} = H$$\n    </p>\n    <h5>Roles of each player</h5>\n    <p>\n      Each of the players mentioned so far have different roles, and I want to make sure I'm particularly clear on what those are before moving forward.\n    </p>\n    <p>\n      \\(X\\), \\(O\\), and \\(H\\) are, for lack of a better word, \"placeholders\" in the computational graph. For each execution of the function they will be filled in based on the current input.\n    </p>\n    <p>\n      The weight matrices \\(W_1\\) and \\(W_2\\), as well as biases \\(B_1\\) and \\(B_2\\) are referred to as \"variables\", or maybe more commonly \"parameters\" or \"weights\". These players stay the same between executions. It's our goal to figure out the best parameter values during the training process. Typically these are \"initialized\" with random numbers.\n    </p>\n    <h5>Activation functions</h5>\n    <p>\n    Returning to our function, we now arrive at the \"hidden layer\": \\(H = \\begin{bmatrix}h_1 & h_2 & h_3\\end{bmatrix}\\). There's one really important thing to note here: Each neuron \\(h_i\\) in the hidden layer is actually comprised of two numbers. The first is the immediate result of the matrix expression just mentioned. We'll refer to that as the \"net\" output. The second is the \"activated\" output. We'll call that \"out\" for brevity. I won't use the names \\(H\\) or \\(h_i\\) anymore because we actually need to be a little more specific. This section will explain how we'll refer to the hidden layer for the remainder of the writeup.\n    </p>\n    <p>\n      You might have noticed that our equation above contained \\(\\phi_1\\) and \\(\\phi_2\\). This is where neural networks really get their power from. Both of these \\(\\phi\\)s are functions too, but they are a little different from what we've seen so far because they are non-linear. We call them \"activation\" functions because they activate the output of the linear operations inside of them. (An important thing to note about activation functions is that they must be differentiable.)\n    </p>\n    <p>\n      There are lots of types of activation functions. For this example, we'll use the logistic activation function \\(\\phi(x) = \\frac{1}{1 + e^{-x}}\\).\n    </p>\n    <p>\n    We said that \\(H = XW_1 + B_1\\), which is true. But let's be more specific and say that \\(XW_1 + B_1 = H_{net} = \\begin{bmatrix}h_{net,1} & h_{net,2} & h_{net,3}\\end{bmatrix}\\) of net values. To this we apply the activation function to each item individually for the final out matrix of the hidden layer \\(H_{out} = \\begin{bmatrix}h_{out,1} & h_{out,2} & h_{out,3}\\end{bmatrix} = \\begin{bmatrix}\\phi(h_{net,1}) & \\phi(h_{net,2}) & \\phi(h_{net,3})\\end{bmatrix}\\).\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./zoom.png' style='height:220px'/>\n      <p style='width: 500px;color:#888'>\\(h_{out,1}\\), or the \"activated\" first element \\(h_1\\) in the hidden layer is calculated by applying the logistic activation function to \\(h_{net,1}\\).</p>\n    </div>\n    <p>\n    The operation described here is to be done three times when going from the input layer \\(X\\) to the hidden layer \\(H\\), once for each \\(h_i\\). Referring back to the entire function \\(O = \\phi_2(\\phi_1(XW_1 + B_1)W_2 + B_2)\\), \\(h_{out,1}\\) ends up being the first element of \\(\\phi_1(XW_1 + B_1)\\). Expanding the matrix operations we get \\(h_{out,1} = \\phi((x_1w_1 + x_2w_2) + b_{1,1})\\).\n    </p>\n    <p>\n      Likewise, \\(h_{out,2} = \\phi((x_1w_3 + x_2w_4) + b_{1,2})\\) and \\(h_{out,3} = \\phi((x_1w_5 + x_2w_6) + b_{1,3})\\).\n    </p>\n    <h5>Hidden layer to output layer</h5>\n    <p>\n      We want to turn the \\(H_{out}\\) that we just calculated into the output layer \\(O\\). The output layer can also be though of in two parts, \\(O_{net}\\) and \\(O_{out}\\).\n    </p>\n    <p>\n      To get to \\(O_{net}\\), we similarly multiply by a weight matrix \\(W_2\\), which has 3 rows and 2 columns of weight parameters, and add a bias \\(B_2\\) to the result. Finally we apply a logistic activation function element-wise to get to our final network output, \\(O_{out}\\).\n    </p>\n    <p>\n      \\(O_{out}\\) contains the \"final output\" of our function.\n    </p>\n\n    <h2>Calculating loss</h2>\n    <p>\n      Because the values of our parameters \\(W_1\\), \\(W_2\\), \\(B_1\\), and \\(B_2\\) were initialized randomly, the output of our function will, at first, be random as well. This is where the concept of \"loss\" comes in. Loss is a measure of how well a function is able to match the provided training label \\(T\\) for a particular data point. A label is really just another matrix, generally of the same shape as \\(O_{out}\\). By running both the label and output for a given training example through a loss function \\(E\\), we can get a numeric value that tells us how well our function performed on that data point.\n    </p>\n    <p>\n      Let's use the squared error for this example. For an arbitrary training example with label \\(T = \\begin{bmatrix}t_{1} & t_{2}\\end{bmatrix}\\) we can calculate the squared loss as:\n    </p>\n    <p>\n      $$E_{total}(\\theta) = \\sum_{i=1}^2\\frac{1}{2}(t_i - o_{out,i})^2$$\n    </p>\n    <p>\n      Notice that the error relies on \\(\\theta\\), which is really just a placeholder for all of the parameters in the model. \\(\\theta\\) again will be omitted in the future. Our goal is to figure out how to change all of the individual matrix values to minimize \\(E_{total}\\).\n    </p>\n\n    <h2>Minimizing error using gradient descent</h2>\n    <p>\n      Up until this point we've seen that a neural network is just a function whose output accuracy is measured by a loss function. Next, we want to train the model parameters to minimize what the loss function returns.\n    </p>\n    <p>\n      Gradient descent is a technique that is used to update paramater weights. By taking the derivative of the total error with respect to each parameter individually, we can determine how that parameter affects the total error. Consequently, this gives us insight on how to change the weight to decrease the total error.\n    <p>\n      For example, to update the parameter \\(w_7\\), which is the edge connecting \\(h_1\\) and \\(o_1\\) above, you would perform the following update:\n    </p>\n    <p>\n      $$w_7 := w_7 - \\alpha\\Big(\\frac{\\delta E_{total}}{\\delta w_7}\\Big)$$\n    </p>\n    <p>\n      \\(\\alpha\\) is called the \"learning rate\". It's just a parameter that helps determine how quickly the parameters are updated. A value too large may explode, and a value too small might take forever to converge.\n    </p>\n    <p>\n      In the upcoming subsections, we'll work through the math required to perform updates on \\(w_7\\) and \\(w_1\\), although the same principle should be used to update all of the function's parameters.\n    </p>\n    <h5>Updating weight \\(w_7\\)</h5>\n    <p>\n      Let's first locate where weight \\(w_7\\) even is in our computational graph. In the very first image up at the top I actually don't label it, but it's the edge connecting \\(h_1\\) with \\(o_1\\). To perform the update as described, we need to determine \\(\\frac{\\delta E_{total}}{\\delta w_7}\\).\n    </p>\n    <p>\n      Breaking this down using the chain rule gives us:\n    </p>\n    <p>\n      \\(\\frac{\\delta E_{total}}{\\delta w_7} = \\frac{\\delta E_{total}}{\\delta o_{out,1}} \\frac{\\delta o_{out,1}}{\\delta o_{net,1}} \\frac{\\delta o_{net,1}}{\\delta w_7} \\)\n    </p>\n    <p>\n      You can see that each of these derivatives are stepping backwards from the error to the weight being updated, which is why this step is called backpropagation. One of the nice features of backpropagation is that it gives us modular blocks that we can recycle when we're finding gradients higher up in the graph. We'll see this when we update \\(w_1\\) next.\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./w7.png' style='height:220px'/>\n      <p style='width: 500px;color:#888'>The error that our network currently outputs for \\(o_1\\) is \\(\\frac{1}{2}(t_1 - o_{out,1})^2\\)</p>\n    </div>\n    <p>\n      In order for us to find \\(\\frac{\\delta E_{total}}{\\delta w_7}\\) let's first individually solve for each item in the chain rule decomposition.\n    </p>\n    <p>\n      \\(\\frac{\\delta E_{total}}{\\delta o_{out,1}}\\):\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(E_{total} = \\frac{1}{2}(t_1 - o_{out,1})^2 + \\frac{1}{2}(t_2 - o_{out,2})^2\\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta E_{total}}{\\delta o_{out,1}} = -(t_1 - o_{out,1})\\).\n    </p>\n    <p>\n      \\(\\frac{\\delta o_{out,1}}{\\delta o_{net,1}}\\):\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(o_{out,1} = \\frac{1}{1 + e^{-o_{net,1}}}\\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta o_{out,1}}{\\delta o_{net,1}} = o_{out,1}(1 - o_{out,1})\\)\n    </p>\n    <p>\n      \\(\\frac{\\delta o_{net,1}}{\\delta w_7}\\):\n    </p>\n    <p style=\"margin-left:30px\">\n      \\( o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1} \\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta o_{net,1}}{\\delta w_7} = h_{out,1}\\)\n    </p>\n    <p>\n      And the complete update to \\(w_7\\) would be:\n    </p>\n    <p>\n      $$w_7 := w_7 - \\alpha\\Big( -(o_{out,1})(h_{out,1})(t_1 - o_{out,1})(1 - o_{out,1}) \\Big)$$\n    </p>\n    <p>\n      How is this actually used in practice? After a given training example is passed through the function, the resulting values for each of these variables are then used to update \\(w_7\\). In an actual coded implementation, for example, the values ending up in each of these variables would be cached during forward propagation so that they could be used to calculate the gradient during backpropagation.\n    </p>\n    <h5>Updating weight \\(w_1\\)</h5>\n    <p>\n      Updating \\(w_1\\) is really similar to updating \\(w_7\\), the main difference being that \\(w_1\\) affects two components of \\(E_{total}\\): \\(E_1\\) and \\(E_2\\).\n    </p>\n    <p>\n      $$ \\frac{\\delta E_{total}}{\\delta w_1} = \\frac{\\delta E_{total}}{\\delta h_{out,1}} \\frac{\\delta h_{out,1}}{\\delta h_{net,1}} \\frac{\\delta h_{net,1}}{\\delta w_1} $$\n    </p>\n    <p>\n      $$ \\frac{\\delta E_{total}}{\\delta w_1} = \\Big( \\frac{\\delta E_1}{\\delta h_{out,1}} + \\frac{\\delta E_2}{\\delta h_{out,1}} \\Big) \\frac{\\delta h_{out,1}}{\\delta h_{net,1}} \\frac{\\delta h_{net,1}}{\\delta w_1} $$\n    </p>\n    <p>\n      We'll find each of these partial derivatives, but note that we've already done these before for the most part when we did our updates for the weights in the hidden to output layer.\n    </p>\n    <p>\n      \\(\\frac{\\delta E_1}{\\delta h_{out,1}}\\):\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta E_1}{\\delta h_{out,1}} =\n        \\frac{\\delta E_1}{\\delta o_{out,1}}\n        \\frac{\\delta o_{out,1}}{\\delta o_{net,1}}\n        \\frac{\\delta o_{net,1}}{\\delta h_{out,1}}\\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta E_1}{\\delta o_{out,1}} \\frac{\\delta o_{out,1}}{\\delta o_{net,1}} = -o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\).\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(o_{net,1} = w_7h_{out,1} + w_8h_{out,2} + w_9h_{out,3} + b_{2,1}\\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta o_{net,1}}{\\delta h_{out,1}} = w_7\\)\n    </p>\n    <p style=\"margin-left:30px\">\n      \\(\\frac{\\delta E_1}{\\delta h_{out,1}} = -w_7o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\)\n    </p>\n    <p>\n      \\(\\frac{\\delta E_2}{\\delta h_{out,1}} = -w_{10}o_{out,1}(t_1 - o_{out,1})(1 - o_{out,1})\\)\n    </p>\n    <p>\n      \\(\\frac{\\delta h_{out,1}}{\\delta h_{net,1}} = h_{out,1}(1 - h_{out,1})\\)\n    </p>\n    <p>\n      \\(\\frac{\\delta h_{net,1}}{\\delta w_1} = x_1\\)\n    </p>\n    <p>\n      Inserting all of these partial derivatives into the larger expression above will give us how we should update \\(w_1\\) for this training example.\n    </p>\n    <p>\n      And there we have it! To see this neural network in action, you can check out the <a href=\"./simple_nn_from_scratch.py\" download>corresponding code</a>.\n    </p>\n    <small>Robert Kotcher, 2019</small>\n  </div>\n</template>\n\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  h2 {\n    margin: 18px 0;\n    line-height: 1.2;\n  }\n\n  .image-container {\n    padding: 18px;\n    display: flex;\n    justify-content: center;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=0d57277a&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=0d57277a&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"0d57277a\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h1',[_vm._v(\"Deep CryptoKitties\")]),_c('p',[_vm._v(\" Back when Ethereum was popular I wanted to own a \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://www.cryptokitties.co/about\"}},[_vm._v(\"CryptoKitty\")]),_vm._v(\" like all of the cool kids did. I remember spending about $30 on a completely unique, second generation CryptoKitty, and enjoying that for about 5 minutes before completely forgetting about it. \")]),_c('div',{staticClass:\"image-container\"},[_c('img',{staticClass:\"cryptokitty\",attrs:{\"src\":require(\"./cryptokitty.png\")}}),_c('small',[_vm._v(\"My 2nd generation CryptoKitty\")])]),_c('p',[_vm._v(\" For those who are unfamiliar with the project, a CryptoKitty is essentially a blockchain-based game where users can breed CryptoKitties that they own to unlock new traits. It comes with the whole proof-of-ownership guarantee that blockchains provide, but that's a whole different discussion. \")]),_c('p',[_vm._v(\" In this post, I resurrect my interest in CryptoKitties. The objective here is to build a general adversarial network (GAN) to learn what a CryptoKitty is and generate new and unique creatures by studying existing ones. \")]),_c('h2',[_vm._v(\" Data creation \")]),_c('p',[_vm._v(\" As this was the first time I had ever built a GAN, I decided to simplify the input data to a level I thought would be reasonably challenging without risking not having enough memory/compute power/knowledge to get results worth posting about. \")]),_c('p',[_vm._v(\" A quick check of the \"),_c('a',{attrs:{\"href\":\"https://cryptokitties.co\"}},[_vm._v(\"cryptokitties.co\")]),_vm._v(\"source code revealed that I'd be able to fetch images by querying \"),_c('a',{attrs:{\"href\":\"https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg\"}},[_vm._v(\"https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg\")]),_vm._v(\", and replacing the image name (1831008.svg) with randomly-generated numbers. \")]),_c('p',[_vm._v(\" Because the images were SVG, I had to use a command-line tool to both convert to PNG and select a fixed size. Not knowing the scale by which adding a few more pixels would increase memory/compute requirements, I decided to be conservative and set each image at 200x200. \")]),_c('p',[_vm._v(\" Finally I reduced the number of channels from 3 to 1. \")]),_c('h2',[_vm._v(\" GAN overview \")]),_c('p',[_vm._v(\" At a high level, a GAN consists of two deep neural networks that are being optimized at the same time: \")]),_c('ul',[_c('li',[_vm._v(\" The \"),_c('strong',[_vm._v(\"generator\")]),_vm._v(\" tries to create output that matches the distribution of the training data, given some arbitrary input. In this example, out input data is random and normally distributed. \")]),_c('li',[_vm._v(\" The \"),_c('strong',[_vm._v(\"discriminator\")]),_vm._v(\" tries to determine whether or not an input image is real or fake. \")])]),_c('br'),_c('p',[_vm._v(\" The following code snippet is the final version of my \"),_c('strong',[_vm._v(\"generator\")]),_vm._v(\": \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"def make_generator_model():\\n    model = tf.keras.Sequential()\\n    model.add(layers.Dense(25*25*400, use_bias=False, input_shape=(1500,)))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.LeakyReLU())\\n\\n    model.add(layers.Reshape((25, 25, 400)))\\n    assert model.output_shape == (None, 25, 25, 400) # Note: None is the batch size\\n\\n    model.add(layers.Conv2DTranspose(100, (5, 5), strides=(2, 2), padding='same', use_bias=True))\\n    assert model.output_shape == (None, 50, 50, 100)\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.LeakyReLU())\\n\\n    model.add(layers.Conv2DTranspose(50, (5, 5), strides=(2, 2), padding='same', use_bias=True))\\n    assert model.output_shape == (None, 100, 100, 50)\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.LeakyReLU())\\n\\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=True))\\n    assert model.output_shape == (None, 200, 200, 1)\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.LeakyReLU())\\n\\n    return model\\n\")]),_c('p',[_vm._v(\" What's happening here is that this network takes as input a batch of training data that has shape [None, 1500]. Remember that this data is normally distributed, and the generator's goal is to learn how to redistribute the data so that it matches the distribution of the training data. \")]),_c('p',[_vm._v(\" Something that I wanted to point out is that the Conv2DTranspose \\\"spreads\\\" the data from a lower to a higher dimensionality, which is how we're able to go from a shape of (25, 25, 400) to a shape of (50, 50, 100). This operation is sometimes misleadingly referred to as a \\\"deconvolution\\\". Notice how the strides actually multiply the length and width of each layer, rather than dividing. \")]),_c('p',[_vm._v(\" The following code snippet is the final version of my \"),_c('strong',[_vm._v(\"discriminator\")]),_vm._v(\": \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"def make_discriminator_model():\\n    model = tf.keras.Sequential()\\n\\n    # 64 because 64 filters were used, 100 bc stride was 2\\n    # From docs: if Conv2D is the first layer of the network, the sample axis\\n    # should not be included in \\\"input_shape\\\"\\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\\n                                     input_shape=[200, 200, 1]))\\n    assert model.output_shape == (None, 100, 100, 64)\\n    model.add(layers.LeakyReLU())\\n    model.add(layers.Dropout(0.1))\\n\\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\\n    assert model.output_shape == (None, 50, 50, 128)\\n    model.add(layers.LeakyReLU())\\n    model.add(layers.Dropout(0.1))\\n\\n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\\n    assert model.output_shape == (None, 25, 25, 256)\\n    model.add(layers.LeakyReLU())\\n    model.add(layers.Dropout(0.1))\\n\\n    model.add(layers.Flatten())\\n    model.add(layers.Dense(1))\\n\\n    return model\\n\")]),_c('p',[_vm._v(\" The discriminator is composed of more standard convolution, activation, and normalization layers. The objective on the discriminator is, given an image, to determine whether it is real or fake. \")]),_c('p',[_vm._v(\" As I monitored the loss of this network during training I found that it was almost always monotonically decreasing, therefore I found myself spending much less time tweaking the hyperparameters of this network. \")]),_c('p',[_vm._v(\" Speaking of loss functions - here are the loss functions: \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"def discriminator_loss(real_output, fake_output):\\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\\n    total_loss = real_loss + fake_loss\\n    return total_loss\\n\\ndef generator_loss(fake_output):\\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\\n\")]),_c('p',[_vm._v(\" We know the discriminator did well if \"),_c('strong',[_vm._v(\"real_output\")]),_vm._v(\" is all ones and \"),_c('strong',[_vm._v(\"fake_output\")]),_vm._v(\" is all zeros, i.e., it is able to correctly determine which images are real and which are fake. \")]),_c('p',[_vm._v(\" We know the generator did well if it was able to trick the discriminator, which equates to the discriminator assigning a higher value for a fake input. \")]),_c('h2',[_vm._v(\"Results\")]),_c('p',[_vm._v(\" At first, both networks perform randomly. For example, here is the generator's output from a number of different runs, prior to the first epoch of training. \")]),_c('div',{staticClass:\"image-container\"},[_c('div',{staticClass:\"image-container-row\"},[_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./random-1.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./random-2.png\")}})]),_c('small',[_vm._v(\"Normally distributed random data across two runs\")])]),_c('p',[_vm._v(\" I was trying to think of the most insightful way to group and describe images that were generated during various training attempts. There are so many variants of each network (depth, dimensionality, activation, bias, dropout, etc) that I ran that it quickly became an overwhelming task to organize them in an informative and fun way - so I now give to you some generator outputs in no particular order: \")]),_c('div',{staticClass:\"image-container\"},[_c('div',{staticClass:\"image-container-row\"},[_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./gen-1.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./gen-2.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./gen-3.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./gen-4.png\")}})]),_c('small',[_vm._v(\"Examples of generator output during training\")])]),_c('p',[_vm._v(\" A key breakthrough that I made was that, although increasing the depth and dimensionality of the generator network itself did change the its accuracy, the single most impactful change I made was to increase the dimensionality of the input data. In my final and most successful run, where each training sample had 1500 normally-distributed numbers, I was able to achieve the following results: \")]),_c('div',{staticClass:\"image-container\"},[_c('div',{staticClass:\"image-container-row\"},[_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./final-1.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./final-2.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./final-3.png\")}}),_c('img',{staticClass:\"example-image\",attrs:{\"src\":require(\"./final-4.png\")}})]),_c('small',[_vm._v(\" Although the input data is random, I was able to increase output quality significantly just by increasing its dimensionality. \")])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h1>Deep CryptoKitties</h1>\n    <p>\n      Back when Ethereum was popular I wanted to own a <a target='_blank' href=\"https://www.cryptokitties.co/about\">CryptoKitty</a> like all of the cool kids did. I remember spending about $30 on a completely unique, second generation CryptoKitty, and enjoying that for about 5 minutes before completely forgetting about it.\n    </p>\n    <div class=\"image-container\">\n      <img class=\"cryptokitty\" src='./cryptokitty.png'/>\n      <small>My 2nd generation CryptoKitty</small>\n    </div>\n    <p>\n      For those who are unfamiliar with the project, a CryptoKitty is essentially a blockchain-based game where users can breed CryptoKitties that they own to unlock new traits. It comes with the whole proof-of-ownership guarantee that blockchains provide, but that's a whole different discussion.\n    </p>\n    <p>\n      In this post, I resurrect my interest in CryptoKitties. The objective here is to build a general adversarial network (GAN) to learn what a CryptoKitty is and generate new and unique creatures by studying existing ones.\n    </p>\n\n    <h2>\n      Data creation\n    </h2>\n    <p>\n      As this was the first time I had ever built a GAN, I decided to simplify the input data to a level I thought would be reasonably challenging without risking not having enough memory/compute power/knowledge to get results worth posting about.\n    </p>\n    <p>\n      A quick check of the <a href='https://cryptokitties.co'>cryptokitties.co</a>source code revealed that I'd be able to fetch images by querying <a href='https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg'>https://img.cryptokitties.co/0x06012c8cf97bead5deae237070f9587f8e7a266d/1831008.svg</a>, and replacing the image name (1831008.svg) with randomly-generated numbers.\n    </p>\n    <p>\n      Because the images were SVG, I had to use a command-line tool to both convert to PNG and select a fixed size. Not knowing the scale by which adding a few more pixels would increase memory/compute requirements, I decided to be conservative and set each image at 200x200.\n    </p>\n    <p>\n      Finally I reduced the number of channels from 3 to 1.\n    </p>\n\n    <h2>\n      GAN overview\n    </h2>\n    <p>\n      At a high level, a GAN consists of two deep neural networks that are being optimized at the same time:\n    </p>\n    <ul>\n      <li>\n        The <strong>generator</strong> tries to create output that matches the distribution of the training data, given some arbitrary input. In this example, out input data is random and normally distributed.\n      </li>\n      <li>\n        The <strong>discriminator</strong> tries to determine whether or not an input image is real or fake.\n      </li>\n    </ul>\n    <br/>\n    <p>\n      The following code snippet is the final version of my <strong>generator</strong>:\n    </p>\n<pre data-lang='python' class='prettyprint'>\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(25*25*400, use_bias=False, input_shape=(1500,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((25, 25, 400)))\n    assert model.output_shape == (None, 25, 25, 400) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(100, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 50, 50, 100)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(50, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 100, 100, 50)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n    assert model.output_shape == (None, 200, 200, 1)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    return model\n</pre>\n    <p>\n      What's happening here is that this network takes as input a batch of training data that has shape [None, 1500]. Remember that this data is normally distributed, and the generator's goal is to learn how to redistribute the data so that it matches the distribution of the training data.\n    </p>\n    <p>\n      Something that I wanted to point out is that the Conv2DTranspose \"spreads\" the data from a lower to a higher dimensionality, which is how we're able to go from a shape of (25, 25, 400) to a shape of (50, 50, 100). This operation is sometimes misleadingly referred to as a \"deconvolution\". Notice how the strides actually multiply the length and width of each layer, rather than dividing.\n    </p>\n\n    <p>\n      The following code snippet is the final version of my <strong>discriminator</strong>:\n    </p>\n<pre data-lang='python' class='prettyprint'>\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n\n    # 64 because 64 filters were used, 100 bc stride was 2\n    # From docs: if Conv2D is the first layer of the network, the sample axis\n    # should not be included in \"input_shape\"\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[200, 200, 1]))\n    assert model.output_shape == (None, 100, 100, 64)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    assert model.output_shape == (None, 50, 50, 128)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    assert model.output_shape == (None, 25, 25, 256)\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.1))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n</pre>\n\n    <p>\n      The discriminator is composed of more standard convolution, activation, and normalization layers. The objective on the discriminator is, given an image, to determine whether it is real or fake.\n    </p>\n    <p>\n      As I monitored the loss of this network during training I found that it was almost always monotonically decreasing, therefore I found myself spending much less time tweaking the hyperparameters of this network.\n    </p>\n    <p>\n      Speaking of loss functions - here are the loss functions:\n    </p>\n\n<pre data-lang='python' class='prettyprint'>\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n</pre>\n    <p>\n      We know the discriminator did well if <strong>real_output</strong> is all ones and <strong>fake_output</strong> is all zeros, i.e., it is able to correctly determine which images are real and which are fake.\n    </p>\n    <p>\n      We know the generator did well if it was able to trick the discriminator, which equates to the discriminator assigning a higher value for a fake input.\n    </p>\n\n    <h2>Results</h2>\n    <p>\n      At first, both networks perform randomly. For example, here is the generator's output from a number of different runs, prior to the first epoch of training.\n    </p>\n    <div class=\"image-container\">\n      <div class=\"image-container-row\">\n        <img class=\"example-image\" src=\"./random-1.png\"/>\n        <img class=\"example-image\" src=\"./random-2.png\"/>\n      </div>\n      <small>Normally distributed random data across two runs</small>\n    </div>\n    <p>\n      I was trying to think of the most insightful way to group and describe images that were generated during various training attempts. There are so many variants of each network (depth, dimensionality, activation, bias, dropout, etc) that I ran that it quickly became an overwhelming task to organize them in an informative and fun way - so I now give to you some generator outputs in no particular order:\n    </p>\n    <div class=\"image-container\">\n      <div class=\"image-container-row\">\n        <img class=\"example-image\" src=\"./gen-1.png\"/>\n        <img class=\"example-image\" src=\"./gen-2.png\"/>\n        <img class=\"example-image\" src=\"./gen-3.png\"/>\n        <img class=\"example-image\" src=\"./gen-4.png\"/>\n      </div>\n      <small>Examples of generator output during training</small>\n    </div>\n    <p>\n      A key breakthrough that I made was that, although increasing the depth and dimensionality of the generator network itself did change the its accuracy, the single most impactful change I made was to increase the dimensionality of the input data. In my final and most successful run, where each training sample had 1500 normally-distributed numbers, I was able to achieve the following results:\n    </p>\n    <div class=\"image-container\">\n      <div class=\"image-container-row\">\n        <img class=\"example-image\" src=\"./final-1.png\"/>\n        <img class=\"example-image\" src=\"./final-2.png\"/>\n        <img class=\"example-image\" src=\"./final-3.png\"/>\n        <img class=\"example-image\" src=\"./final-4.png\"/>\n      </div>\n      <small>\n        Although the input data is random, I was able to increase output quality significantly just by increasing its dimensionality.\n      </small>\n    </div>\n  </div>\n</template>\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  pre {\n    padding: 0;\n    background-color: #333;\n    font-size: 10px;\n  }\n\n  .image-container {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    margin: 18px 0;\n  }\n\n  .cryptokitty {\n    width: 220px;\n    object-fit: contain;\n  }\n\n  .example-image {\n    width: 130px;\n    height: 130px;\n    margin: 12px;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=1996c03d&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=1996c03d&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"1996c03d\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h1',[_vm._v(\"Fun with Recurrent Neural Networks\")]),_c('p',[_vm._v(\" Recurrent Neural Networks (RNNs) are useful for making sense of sequential data in a way that would be more difficult for a standard feed-forward neural network. \")]),_c('p',[_vm._v(\" The name RNN is a broad term used to describe many network implementations that exhibit temporal awareness. Some RNNs can be unrolled to look like a feedforward network due to the fact that the length of the input sequence being evaluated is finite, however the main difference here is that, in an RNN, a new piece of input data is introduced at each layer and combined with an internal state that has been passed along from previous states. \")]),_c('p',[_vm._v(\" Vanilla RNNs (Elman network, Jordan network, for example) have drawbacks that make them not very useful in practice, however I am going to step through two variants and use cases of the Elman network to build a foundation upon which more modern architectures can be built. \")]),_c('h3',[_vm._v(\"Simple network overview\")]),_c('p',[_vm._v(\" It is assumed that you have some ordered sequence of data \\\\(X\\\\) that you want to train your network with. Examples of data types that work nicely with RNNs include, but are not limited to characters, numbers, and words in an ordered sequence. We will see both types throughout this writeup. \")]),_vm._m(0),_c('p',[_vm._v(\" We'll be iterating through the sequence, one item at a time. \")]),_c('p',[_vm._v(\" At step \\\\(t\\\\) of sequence \\\\(X\\\\), we want to \\\"introduce\\\" new information to the network that will persist through the remainder of the timesteps. We introduce the information by combining the current information (represented by \\\\(h_t\\\\), or the \\\"state\\\" at step \\\\(t\\\\)) with the new input \\\\(x_t\\\\). \")]),_c('p',[_vm._v(\" At time step 0 the state is usually initialized with zeros. \")]),_c('p',[_vm._v(\" Before choosing a particular implementation of our simple RNN, there are a few basic questions that need to be considered: What is the desired output? Are we using the network for generation or for classification? What is the dimensionality of the data we're working with? \")]),_c('p',[_vm._v(\" In this writeup, I'm going to put together two simple networks with the above topology, one for text generation and one for text classification, and describe how the implementation details change between the two. \")]),_c('h2',[_vm._v(\"Network 1: Echo RNN\")]),_c('p',[_vm._v(\" In the first network we consider an input pattern constructed from zeros and ones. When passed through the feed-forward pass of the network we wish to add a delay (or \\\"echo\\\") to the pattern. In effect, the network will learn to shift the input pattern by some number of timesteps. You could generate dummy data in the following way: \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"def generateData():\\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\\n    y = np.roll(x, DELAY_STEPS)\\n    y[0:DELAY_STEPS] = 0\\n\\n    x = x.reshape((batch_size, -1))\\n    y = y.reshape((batch_size, -1))\\n\\n    return (x, y)\\n\")]),_c('p',[_vm._v(\" Now that we have some training data, we'll want to group it into minibatches in order to perform \\\"minibatch gradient descent\\\". This is worthy of another entire writeup, but putting training samples into small batches is a common training practice that, by assuming that the batch is distributed in the same way that your entire pool of training data is distributed, can speed up learning by performing the backward pass more frequently, all while decreasing the resources required. \")]),_c('h3',[_vm._v(\"Input data and placeholders\")]),_c('p',[_vm._v(\" The graph is first initialized with three placeholders, as shown in the code sample below. batchX_placeholder accepts a tensor containing a batch of the original (no echo) sequences, batchY_placeholder accepts a tensor containing a batch of the expected output (input delayed by DELAY_STEPS timesteps), and init_state, which is the state that is passed between each timestep in the sequence. \")]),_c('p',[_vm._v(\" Note that \\\"truncated_backprop_length\\\" refers to the number of steps in the sequence that we're training the network on. \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size])\\n\")]),_c('p',[_vm._v(\" Another way to understand these placeholders is to consider the shape and nature of the training examples that will be fed to them as they enter the network: \")]),_c('p',[_vm._v(\" Before building the matrices, let's create a dummy sequence of zeros and ones that we'll use to train our network: \")]),_c('p',[_vm._v(\" $$1 0 0 0 0 ... 0 1 0 0 0 ...$$ \")]),_c('p',[_vm._v(\" What we'll do, although in the end the exact details of this aren't super important as they're specific to this particular case study, is break up our sequence into batches. For our example, let's create two batches of 5 consecutive items, as shown above. \")]),_c('p',[_vm._v(\" We could create the following batched input \\\\(X\\\\) to the network: \")]),_c('div',[_vm._v(\" $$X = \\\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\\\\\ 0 & 1 & 0 & 0 & 0 \\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" \\\\(X\\\\) contains a batch of two training examples (batch size = 2), with truncated_backprop_length of 5. \")]),_c('p',[_vm._v(\" The corresponding \\\"echoed\\\" matrix \\\\(Y\\\\) would then look like this if, for example, DELAY_STEPS was set to 3: \")]),_c('div',[_vm._v(\" $$Y = \\\\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\\\\\\\ 0 & 0 & 0 & 0 & 1 \\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" Notice that we've just taken the items from \\\\(X\\\\) and shifted them 3 positions to the right. \")]),_c('p',[_vm._v(\" We finally pass in a state matrix that is initialized to all zeros, whose size is [batch_size state_size]: \")]),_c('div',[_vm._v(\" $$h_0 = \\\\begin{bmatrix} 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0\\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" The size of the column axis of \\\\(h_0\\\\) (axis = 1) might seem arbitrary, and in some ways it is. In practice, if the state is too small the network might not have the capacity to learn the nature of the training data, i.e. as the complexities of the data being learned grows, so must the available state in order to learn it. On the other hand, too much state leads to inefficiencies with storage and computation. \")]),_c('h3',[_vm._v(\"Variables\")]),_c('p',[_vm._v(\" The variables introduced in this section will come into play throughout the remainder of this section of the writeup - I'm just presenting them now for reference. The values that they receive upon initialization is less important because these are the values that will be learned during training. I'll talk more about their dimensionality as we go. \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\\nb = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\\n\\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\\n\")]),_c('h3',[_vm._v(\"Network architecture\")]),_c('p',[_vm._v(\" This is a nice transition into the remainder of the computational graph, which combines these three placeholders to compute both the predicted output as well as loss. As before, let's start with the relevant code. \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"inputs_series = tf.unstack(batchX_placeholder, axis=1)\\nlabels_series = tf.unstack(batchY_placeholder, axis=1)\\n\\ncurrent_state = init_state\\nstates_series = []\\nfor current_input in inputs_series:\\n    current_input = tf.reshape(current_input, [batch_size, 1])\\n    input_and_state_concatenated = tf.concat([current_input, current_state], 1)\\n\\n    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\\n    states_series.append(next_state)\\n    current_state = next_state\\n\")]),_c('p',[_vm._v(\" For both the input (batchX_placeholder) and corresponding labels (batchY_placeholder), we split on the column and end with a list of tensors, each of size [batch_size 1]. In our dummy example, inputs_series would look like: \")]),_c('p'),_c('div',[_vm._v(\" $$[\\\\begin{bmatrix} 1 \\\\\\\\ 0 \\\\end{bmatrix}, \\\\begin{bmatrix} 0 \\\\\\\\ 1 \\\\end{bmatrix}, ...]$$ \")]),_c('p',[_vm._v(\" At each iteration of the loop, we combine the input with the most up-to-date state value, of size [batch_size, state_size]. For example, on the first iteration, we end up with: \")]),_c('div',[_vm._v(\" $$\\\\mathrm{input\\\\_and\\\\_state\\\\_concatenated = concat([x\\\\_0, h\\\\_0], axis=1)} = \\\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\\\\\ 0 & 0 & 0 & 0\\\\end{bmatrix}$$ \")]),_c('p',[_vm._v(\" We then transition to the next state by matrix multiplying with \\\\(W\\\\). We must choose \\\\(W\\\\) in such a way that we again end up with a state matrix of shape [batch_size, state_size]. In other words, we want to find a shape [A, B] for \\\\(W\\\\) such that: \")]),_c('div',[_vm._v(\" $$[batch\\\\_size, state\\\\_size + 1] @ [A, B] = [batch\\\\_size, state\\\\_size]$$ \")]),_c('v-alert',{attrs:{\"icon\":\"mdi-help\",\"prominent\":\"\",\"text\":\"\",\"type\":\"info\"}},[_c('p',[_vm._v(\" Lately I've been using \\\"@\\\" to mean \\\"matrix multiply\\\" because \"),_c('strong',[_vm._v(\"Since python >= 3.5 the @ operator is supported (see PEP 465). In TensorFlow, it simply calls the tf.matmul() function...\")])])]),_c('p',[_vm._v(\" It's clear that \\\\(W\\\\) should be of shape [state_size + 1, state_size], and that exactly what is done (see \\\"Variables\\\" above). \")]),_c('h3',[_vm._v(\"Output layer\")]),_c('p',[_vm._v(\" After iterating through all of the input values in input_series, we end up with truncated_backprop_length state matrices. Each of these matrices is then multiplied by \\\\(W_2\\\\). The purpose of \\\\(W_2\\\\) is to take the shape down to [batch_size, 2]. The same \\\\(W_2\\\\) is used for each multiplication within a batch. \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"logits_series = [tf.matmul(state, W2) + b2 for state in states_series]\\n\\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\\n\\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\\n\\ntotal_loss = tf.reduce_mean(losses)\\n\")]),_c('p',[_vm._v(\" We end up with a truncated_backprop_length logits (weights that are intended to be converted to probabilities) of the following size: \")]),_c('div',[_vm._v(\" $$[ [batch\\\\_size, num\\\\_classes], ...]$$ \")]),_c('p',[_vm._v(\" After running each of the logits through a softmax, we end up with a series of probabilities, where the matrix at \\\\(i\\\\) gives the probability of each output per class, per batch index. So, using the \\\\(X\\\\) we defined with dummy data above, we might end up with the following predication_series: \")]),_c('div',[_vm._v(\" $$ [ \\\\left[ \\\\begin{array}{cc} 0.5 & 0.5 \\\\\\\\ 0.5 & 0.5 \\\\end{array} \\\\right] , \\\\left[ \\\\begin{array}{cc} 0.5 & 0.5 \\\\\\\\ 0.5 & 0.5 \\\\end{array} \\\\right] , \\\\left[ \\\\begin{array}{cc} 0.5 & 0.5 \\\\\\\\ 0.5 & 0.5 \\\\end{array} \\\\right] , \\\\left[ \\\\begin{array}{cc} 0.1 & 0.9 \\\\\\\\ 0.9 & 0.1 \\\\end{array} \\\\right] , \\\\left[ \\\\begin{array}{cc} 0.9 & 0.1 \\\\\\\\ 0.1 & 0.9 \\\\end{array} \\\\right] ] $$ \")]),_c('p',[_vm._v(\" To interpret this dummy output, the network expects that for the first batch the output at the 4th index is most likely to be a 1, followed by a 0. Conversely, for the second item in the batch, the 4th and 5th item in the sequence is expected to be a 0 followed by a 1. \")]),_c('p',[_vm._v(\" We could use these predictions to compute the loss ourselves, but since this is such a common series of operations, Tensorflow provides a function sparse_softmax_cross_entropy_with_logits, which basically takes the logits, computes the softmax over it, and then finds the cross-entropy loss. See the note on cross-entropy below. \")]),_c('v-alert',{attrs:{\"icon\":\"mdi-help\",\"prominent\":\"\",\"text\":\"\",\"type\":\"info\"}},[_c('p',[_c('strong',[_vm._v(\"Cross-Entropy\")]),_vm._v(\" measures the performance of a classification model whose output is a probability value between 0 and 1. Loss increase the further away the predicted label is from the true label. \")]),_c('p',[_vm._v(\" $$H(p,q) = -\\\\sum_{i \\\\in C} p(i) log q(i)$$ \")]),_c('p',[_vm._v(\" where \\\\(p(i)\\\\) is the true class label for the \\\\(i^{th}\\\\) training example, \\\\(q(i)\\\\) is the predicted probability, and C is the number of classes. \")]),_c('p',[_vm._v(\" What this means is, for each training example, we only take into consideratihow close the probability of the true class is to 1. That means we can simplify to \\\\(H(p,q) = -log(q(i))\\\\). \")])]),_c('p',[_vm._v(\" The built-in function sparse_softmax_cross_entropy_with_logits will give us a per-batch index, per-sequence step loss. For example, \")]),_c('div',[_vm._v(\" $$[\\\\begin{bmatrix} 0.84 \\\\\\\\ 0.75 \\\\end{bmatrix}, \\\\begin{bmatrix} 0.75 \\\\\\\\ 0.8 \\\\end{bmatrix}, ...]$$ \")]),_c('p',[_vm._v(\" To find a single loss value, we find the mean across all axis, but may not be the only way to do so. \")]),_c('h2',[_vm._v(\"Network 2: Classification network\")]),_c('p',[_vm._v(\" We have just seen a network that produces N outputs for N inputs. This network is great when translating one sequence to another, but dosen't really make sense for the use case where a sequence is being classified because the output is non-sequential. \")]),_c('p',[_vm._v(\" I decided to take the Echo RNN and modify it to accept sequential data and output a one-hot vector containing a 1 in the position representing the class label. The particular use case that I had in mind was a language detector, however in the end I could have equivalently been measuring anything - it really depends on how you decide to label your training data. \")]),_c('p',[_vm._v(\" The architectural differences between this network and the Echo RNN actually ended up being subtle, but I did run into some unrelated challenges when it came to dealing with batch data and 1-hot encoded data, which I touch on briefly below. \")]),_c('h3',[_vm._v(\"Classification network architecture\")]),_c('p',[_vm._v(\" Recall how we maintain a states_series variable while looping over input_series in the Echo RNN from section 1. In the classification network, this is no longer necessary because our output is non-sequential (i.e. has a different shape) and information is passed across all timesteps. We can just pass the final state \\\\(h\\\\) to a \\\"dense layer\\\" to reduce its dimensionality. \")]),_c('p',[_vm._v(\" It's probably easiest to show the code for this in its entirety, and then after discuss the points of interest. \")]),_c('pre',{staticClass:\"prettyprint\",attrs:{\"data-lang\":\"python\"}},[_vm._v(\"batchX_placeholder = tf.placeholder(tf.float32,\\n    [batch_size, truncated_backprop_length, encoding_size])\\nbatchY_placeholder = tf.placeholder(tf.int32,\\n    [batch_size, num_classes])\\n\\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size, 1])\\n\\nWxh = tf.Variable(np.random.rand(batch_size, state_size, encoding_size), dtype=tf.float32)\\nbh = tf.Variable(np.zeros((batch_size, state_size, 1)), dtype=tf.float32)\\n\\nWhh = tf.Variable(np.random.rand(batch_size, state_size, state_size), dtype=tf.float32)\\n\\nWhy = tf.Variable(np.random.rand(batch_size, num_classes, state_size), dtype=tf.float32)\\nby = tf.Variable(np.zeros((batch_size, num_classes, 1)), dtype=tf.float32)\\n\\ninputs_series = tf.unstack(batchX_placeholder, axis=1)\\n\\nh = init_state\\nfor x in inputs_series:\\n    x = tf.reshape(x, [batch_size, encoding_size, 1])\\n    h = tf.tanh(tf.matmul(Wxh, x) + tf.matmul(Whh, h) + bh)\\n\\ny_logits = tf.tanh(tf.matmul(Why, h) + by)\\ny_logits = tf.reshape(y_logits, [batch_size, num_classes])\\n\")]),_c('p',[_vm._v(\" One challenge I ran into when dealing with text data was dealing with the extra dimension of encoding. In our Echo RNN, we were fortunate that our input data consisted of just 0s and 1s. \")]),_vm._m(1),_c('p',[_vm._v(\" Dealing with batches and one-hot data at the same time ended up being more straightforward than I thought it would be. It turns out that, although you can't matrix multiply a tensor with anything other than 2 dimensions, matrix multiplication in both Tensorflow and numpy only require that the inner two dimensions are compatible. \")]),_c('p',[_vm._v(\" For example, multiplying matrices with shapes [3, 4, 5] and [3, 5, 4] will result in a matrix with a shape of [3, 4, 4]. Matrix multiplying in this case would result in 3 matrix multiplies, one for each of the 3 inputs in the batch. \")]),_c('h2',[_vm._v(\"The vanishing gradient\")]),_c('p',[_vm._v(\" In the end, vanilla RNNs aren't really used in practice. The reason is that backpropagation across so many layers makes it difficult for the network to learn because the gradients become \\\"vanishingly\\\" small. \")]),_c('p',[_vm._v(\" Common solutions to the vanishing gradient problem include using LSTM or GRU networks, and I'll have to wait for another day to do a writeup on these ones. \")])],1)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"image-container\",staticStyle:{\"display\":\"flex\",\"flex-direction\":\"column\",\"align-items\":\"center\"}},[_c('img',{staticStyle:{\"height\":\"150px\"},attrs:{\"src\":require(\"./vanilla.png\")}}),_c('p',{staticStyle:{\"width\":\"500px\",\"color\":\"#888\"}},[_vm._v(\" Figure 1: A high-level, details-omitted view of a simple RNN. For each item in sequence \\\\(X\\\\), \\\\(x_t\\\\), there is a corresponding state \\\\(h_t\\\\). Together, \\\\(x_t\\\\) and \\\\(h_t\\\\) can be used to determine the next state \\\\(h_{t+1}\\\\) and the output for that layer, \\\\(y_t\\\\). \")])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('p',[_vm._v(\" When dealing with text data however, you end up needing to encode each input with 1-hot vectors (or \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://en.wikipedia.org/wiki/Word_embedding\"}},[_vm._v(\"word embeddings\")]),_vm._v(\"). \")])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h1>Fun with Recurrent Neural Networks</h1>\n    <p>\n      Recurrent Neural Networks (RNNs) are useful for making sense of sequential data in a way that would be more difficult for a standard feed-forward neural network.\n    </p>\n    <p>\n      The name RNN is a broad term used to describe many network implementations that exhibit temporal awareness. Some RNNs can be unrolled to look like a feedforward network due to the fact that the length of the input sequence being evaluated is finite, however the main difference here is that, in an RNN, a new piece of input data is introduced at each layer and combined with an internal state that has been passed along from previous states.\n    </p>\n    <p>\n      Vanilla RNNs (Elman network, Jordan network, for example) have drawbacks that make them not very useful in practice, however I am going to step through two variants and use cases of the Elman network to build a foundation upon which more modern architectures can be built.\n    </p>\n    <h3>Simple network overview</h3>\n    <p>\n      It is assumed that you have some ordered sequence of data \\(X\\) that you want to train your network with. Examples of data types that work nicely with RNNs include, but are not limited to characters, numbers, and words in an ordered sequence. We will see both types throughout this writeup.\n    </p>\n    <div class='image-container' style='display:flex;flex-direction:column;align-items:center;'>\n      <img src='./vanilla.png' style='height:150px'/>\n      <p style='width: 500px;color:#888'>\n        Figure 1: A high-level, details-omitted view of a simple RNN. For each item in sequence \\(X\\), \\(x_t\\), there is a corresponding state \\(h_t\\). Together, \\(x_t\\) and \\(h_t\\) can be used to determine the next state \\(h_{t+1}\\) and the output for that layer, \\(y_t\\).\n      </p>\n    </div>\n    <p>\n      We'll be iterating through the sequence, one item at a time.\n    </p>\n    <p>\n      At step \\(t\\) of sequence \\(X\\), we want to \"introduce\" new information to the network that will persist through the remainder of the timesteps. We introduce the information by combining the current information (represented by \\(h_t\\), or the \"state\" at step \\(t\\)) with the new input \\(x_t\\).\n    </p>\n    <p>\n      At time step 0 the state is usually initialized with zeros.\n    </p>\n    <p>\n      Before choosing a particular implementation of our simple RNN, there are a few basic questions that need to be considered: What is the desired output? Are we using the network for generation or for classification? What is the dimensionality of the data we're working with?\n    </p>\n    <p>\n      In this writeup, I'm going to put together two simple networks with the above topology, one for text generation and one for text classification, and describe how the implementation details change between the two.\n    </p>\n    <h2>Network 1: Echo RNN</h2>\n    <p>\n      In the first network we consider an  input pattern constructed from zeros and ones. When passed through the feed-forward pass of the network we wish to add a delay (or \"echo\") to the pattern. In effect, the network will learn to shift the input pattern by some number of timesteps. You could generate dummy data in the following way:\n    </p>\n<pre data-lang='python' class='prettyprint'>\ndef generateData():\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n    y = np.roll(x, DELAY_STEPS)\n    y[0:DELAY_STEPS] = 0\n\n    x = x.reshape((batch_size, -1))\n    y = y.reshape((batch_size, -1))\n\n    return (x, y)\n</pre>\n    <p>\n      Now that we have some training data, we'll want to group it into minibatches in order to perform \"minibatch gradient descent\". This is worthy of another entire writeup, but putting training samples into small batches is a common training practice that, by assuming that the batch is distributed in the same way that your entire pool of training data is distributed, can speed up learning by performing the backward pass more frequently, all while decreasing the resources required.\n    </p>\n    <h3>Input data and placeholders</h3>\n    <p>\n      The graph is first initialized with three placeholders, as shown in the code sample below. batchX_placeholder accepts a tensor containing a batch of the original (no echo) sequences, batchY_placeholder accepts a tensor containing a batch of the expected output (input delayed by DELAY_STEPS timesteps), and init_state, which is the state that is passed between each timestep in the sequence.\n    </p>\n    <p>\n      Note that \"truncated_backprop_length\" refers to the number of steps in the sequence that we're training the network on.\n    </p>\n<pre data-lang='python' class='prettyprint'>\nbatchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size])\n</pre>\n    <p>\n      Another way to understand these placeholders is to consider the shape and nature of the training examples that will be fed to them as they enter the network:\n    </p>\n    <p>\n      Before building the matrices, let's create a dummy sequence of zeros and ones that we'll use to train our network:\n    </p>\n    <p>\n      $$1 0 0 0 0 ... 0 1 0 0 0 ...$$\n    </p>\n    <p>\n      What we'll do, although in the end the exact details of this aren't super important as they're specific to this particular case study, is break up our sequence into batches. For our example, let's create two batches of 5 consecutive items, as shown above.\n    </p>\n    <p>\n      We could create the following batched input \\(X\\) to the network:\n    </p>\n    <div>\n      $$X = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\end{bmatrix}$$\n    </div>\n    <p>\n      \\(X\\) contains a batch of two training examples (batch size = 2), with truncated_backprop_length of 5.\n    </p>\n    <p>\n      The corresponding \"echoed\" matrix \\(Y\\) would then look like this if, for example, DELAY_STEPS was set to 3:\n    </p>\n    <div>\n      $$Y = \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{bmatrix}$$\n    </div>\n    <p>\n      Notice that we've just taken the items from \\(X\\) and shifted them 3 positions to the right.\n    </p>\n    <p>\n      We finally pass in a state matrix that is initialized to all zeros, whose size is [batch_size state_size]:\n    </p>\n    <div>\n      $$h_0 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$$\n    </div>\n    <p>\n      The size of the column axis of \\(h_0\\) (axis = 1) might seem arbitrary, and in some ways it is. In practice, if the state is too small the network might not have the capacity to learn the nature of the training data, i.e. as the complexities of the data being learned grows, so must the available state in order to learn it. On the other hand, too much state leads to inefficiencies with storage and computation.\n    </p>\n    <h3>Variables</h3>\n    <p>\n      The variables introduced in this section will come into play throughout the remainder of this section of the writeup - I'm just presenting them now for reference. The values that they receive upon initialization is less important because these are the values that will be learned during training. I'll talk more about their dimensionality as we go.\n    </p>\n<pre data-lang='python' class='prettyprint'>\nW = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\nb = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n</pre>\n    <h3>Network architecture</h3>\n    <p>\n      This is a nice transition into the remainder of the computational graph, which combines these three placeholders to compute both the predicted output as well as loss. As before, let's start with the relevant code.\n    </p>\n<pre data-lang='python' class='prettyprint'>\ninputs_series = tf.unstack(batchX_placeholder, axis=1)\nlabels_series = tf.unstack(batchY_placeholder, axis=1)\n\ncurrent_state = init_state\nstates_series = []\nfor current_input in inputs_series:\n    current_input = tf.reshape(current_input, [batch_size, 1])\n    input_and_state_concatenated = tf.concat([current_input, current_state], 1)\n\n    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)\n    states_series.append(next_state)\n    current_state = next_state\n</pre>\n    <p>\n      For both the input (batchX_placeholder) and corresponding labels (batchY_placeholder), we split on the column and end with a list of tensors, each of size [batch_size 1]. In our dummy example, inputs_series would look like:\n    <p>\n    <div>\n      $$[\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, ...]$$\n    </div>\n    <p>\n      At each iteration of the loop, we combine the input with the most up-to-date state value, of size [batch_size, state_size]. For example, on the first iteration, we end up with:\n    </p>\n    <div>\n      $$\\mathrm{input\\_and\\_state\\_concatenated = concat([x\\_0, h\\_0], axis=1)} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0\\end{bmatrix}$$\n    </div>\n    <p>\n      We then transition to the next state by matrix multiplying with \\(W\\). We must choose \\(W\\) in such a way that we again end up with a state matrix of shape [batch_size, state_size]. In other words, we want to find a shape [A, B] for \\(W\\) such that:\n    </p>\n    <div>\n      $$[batch\\_size, state\\_size + 1] @ [A, B] = [batch\\_size, state\\_size]$$\n    </div>\n    <v-alert\n      icon=\"mdi-help\"\n      prominent\n      text\n      type=\"info\"\n    >\n      <p>\n        Lately I've been using \"@\" to mean \"matrix multiply\" because <strong>Since python >= 3.5 the @ operator is supported (see PEP 465). In TensorFlow, it simply calls the tf.matmul() function...</strong>\n      </p>\n    </v-alert>\n\n    <p>\n      It's clear that \\(W\\) should be of shape [state_size + 1, state_size], and that exactly what is done (see \"Variables\" above).\n    </p>\n    <h3>Output layer</h3>\n    <p>\n      After iterating through all of the input values in input_series, we end up with truncated_backprop_length state matrices. Each of these matrices is then multiplied by \\(W_2\\). The purpose of \\(W_2\\) is to take the shape down to [batch_size, 2]. The same \\(W_2\\) is used for each multiplication within a batch.\n    </p>\n\n<pre data-lang='python' class='prettyprint'>\nlogits_series = [tf.matmul(state, W2) + b2 for state in states_series]\n\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n\ntotal_loss = tf.reduce_mean(losses)\n</pre>\n\n    <p>\n      We end up with a truncated_backprop_length logits (weights that are intended to be converted to probabilities) of the following size:\n    </p>\n    <div>\n      $$[ [batch\\_size, num\\_classes], ...]$$\n    </div>\n    <p>\n      After running each of the logits through a softmax, we end up with a series of probabilities, where the matrix at \\(i\\) gives the probability of each output per class, per batch index. So, using the \\(X\\) we defined with dummy data above, we might end up with the following predication_series:\n    </p>\n    <div>\n$$\n[ \\left[ \\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.5 & 0.5\n\\end{array} \\right]\n,\n\\left[ \\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.5 & 0.5\n\\end{array} \\right]\n,\n\\left[ \\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.5 & 0.5\n\\end{array} \\right]\n,\n\\left[ \\begin{array}{cc}\n0.1 & 0.9 \\\\\n0.9 & 0.1\n\\end{array} \\right]\n,\n\\left[ \\begin{array}{cc}\n0.9 & 0.1 \\\\\n0.1 & 0.9\n\\end{array} \\right]\n]\n$$\n    </div>\n    <p>\n      To interpret this dummy output, the network expects that for the first batch the output at the 4th index is most likely to be a 1, followed by a 0. Conversely, for the second item in the batch, the 4th and 5th item in the sequence is expected to be a 0 followed by a 1.\n    </p>\n    <p>\n      We could use these predictions to compute the loss ourselves, but since this is such a common series of operations, Tensorflow provides a function sparse_softmax_cross_entropy_with_logits, which basically takes the logits, computes the softmax over it, and then finds the cross-entropy loss. See the note on cross-entropy below.\n    </p>\n\n    <v-alert\n      icon=\"mdi-help\"\n      prominent\n      text\n      type=\"info\"\n    >\n      <p>\n        <strong>Cross-Entropy</strong> measures the performance of a classification model whose output is a probability value between 0 and 1. Loss increase the further away the predicted label is from the true label.\n      </p>\n      <p>\n        $$H(p,q) = -\\sum_{i \\in C} p(i) log q(i)$$\n      </p>\n      <p>\n        where \\(p(i)\\) is the true class label for the \\(i^{th}\\) training example, \\(q(i)\\) is the predicted probability, and C is the number of classes.\n      </p>\n      <p>\n        What this means is, for each training example, we only take into consideratihow close the probability of the true class is to 1. That means we can simplify to \\(H(p,q) = -log(q(i))\\).\n      </p>\n    </v-alert>\n\n    <p>\n      The built-in function sparse_softmax_cross_entropy_with_logits will give us a per-batch index, per-sequence step loss. For example,\n    </p>\n    <div>\n      $$[\\begin{bmatrix} 0.84 \\\\ 0.75 \\end{bmatrix}, \\begin{bmatrix} 0.75 \\\\ 0.8 \\end{bmatrix}, ...]$$\n    </div>\n    <p>\n      To find a single loss value, we find the mean across all axis, but may not be the only way to do so.\n    </p>\n    <h2>Network 2: Classification network</h2>\n    <p>\n      We have just seen a network that produces N outputs for N inputs. This network is great when translating one sequence to another, but dosen't really make sense for the use case where a sequence is being classified because the output is non-sequential.\n    </p>\n    <p>\n      I decided to take the Echo RNN and modify it to accept sequential data and output a one-hot vector containing a 1 in the position representing the class label. The particular use case that I had in mind was a language detector, however in the end I could have equivalently been measuring anything - it really depends on how you decide to label your training data.\n    </p>\n    <p>\n      The architectural differences between this network and the Echo RNN actually ended up being subtle, but I did run into some unrelated challenges when it came to dealing with batch data and 1-hot encoded data, which I touch on briefly below.\n    </p>\n    <h3>Classification network architecture</h3>\n    <p>\n      Recall how we maintain a states_series variable while looping over input_series in the Echo RNN from section 1. In the classification network, this is no longer necessary because our output is non-sequential (i.e. has a different shape) and information is passed across all timesteps. We can just pass the final state \\(h\\) to a \"dense layer\" to reduce its dimensionality.\n    </p>\n    <p>\n      It's probably easiest to show the code for this in its entirety, and then after discuss the points of interest.\n    </p>\n<pre data-lang='python' class='prettyprint'>\nbatchX_placeholder = tf.placeholder(tf.float32,\n    [batch_size, truncated_backprop_length, encoding_size])\nbatchY_placeholder = tf.placeholder(tf.int32,\n    [batch_size, num_classes])\n\ninit_state = tf.placeholder(tf.float32, [batch_size, state_size, 1])\n\nWxh = tf.Variable(np.random.rand(batch_size, state_size, encoding_size), dtype=tf.float32)\nbh = tf.Variable(np.zeros((batch_size, state_size, 1)), dtype=tf.float32)\n\nWhh = tf.Variable(np.random.rand(batch_size, state_size, state_size), dtype=tf.float32)\n\nWhy = tf.Variable(np.random.rand(batch_size, num_classes, state_size), dtype=tf.float32)\nby = tf.Variable(np.zeros((batch_size, num_classes, 1)), dtype=tf.float32)\n\ninputs_series = tf.unstack(batchX_placeholder, axis=1)\n\nh = init_state\nfor x in inputs_series:\n    x = tf.reshape(x, [batch_size, encoding_size, 1])\n    h = tf.tanh(tf.matmul(Wxh, x) + tf.matmul(Whh, h) + bh)\n\ny_logits = tf.tanh(tf.matmul(Why, h) + by)\ny_logits = tf.reshape(y_logits, [batch_size, num_classes])\n</pre>\n    <p>\n      One challenge I ran into when dealing with text data was dealing with the extra dimension of encoding. In our Echo RNN, we were fortunate that our input data consisted of just 0s and 1s.\n    </p>\n    <p>\n      When dealing with text data however, you end up needing to encode each input with 1-hot vectors (or <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Word_embedding\">word embeddings</a>).\n    </p>\n    <p>\n      Dealing with batches and one-hot data at the same time ended up being more straightforward than I thought it would be. It turns out that, although you can't matrix multiply a tensor with anything other than 2 dimensions, matrix multiplication in both Tensorflow and numpy only require that the inner two dimensions are compatible.\n    </p>\n    <p>\n      For example, multiplying matrices with shapes [3, 4, 5] and [3, 5, 4] will result in a matrix with a shape of [3, 4, 4]. Matrix multiplying in this case would result in 3 matrix multiplies, one for each of the 3 inputs in the batch.\n    </p>\n    <h2>The vanishing gradient</h2>\n    <p>\n      In the end, vanilla RNNs aren't really used in practice. The reason is that backpropagation across so many layers makes it difficult for the network to learn because the gradients become \"vanishingly\" small.\n    </p>\n    <p>\n      Common solutions to the vanishing gradient problem include using LSTM or GRU networks, and I'll have to wait for another day to do a writeup on these ones.\n    </p>\n  </div>\n</template>\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  h2 {\n    margin: 18px 0;\n    line-height: 1.2;\n  }\n\n  .image-container {\n    padding: 18px;\n    display: flex;\n    justify-content: center;\n  }\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=25959adf&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=25959adf&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"25959adf\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',[_c('h1',[_vm._v(\"Chinese Proverbs with Airflow\")]),_c('p',[_vm._v(\" My current job involves generating AI data at scale in a pipeline. As we develop the system, the Apache Airflow project has come up in conversation quite a bit. Since I’d never used it before, I thought I’d give it a try. \")]),_c('p',[_vm._v(\" This project is a Chinese Proverb generator. Every day, it scans my Chinese blog to see what words I’m using and their frequencies. Then it reads a dataset of Chinese proverbs from \"),_c('a',{attrs:{\"href\":\"https://www.kaggle.com/datasets/bryanb/phrases-and-sayings\",\"target\":\"_blank\"}},[_vm._v(\"Kaggle\")]),_vm._v(\" and suggests one that introduces new words (but not too many new words). \")]),_c('p',[_vm._v(\" The objective was to understand what is possible with Airflow and where it falls short. Any efforts that did not meet that description were done quickly and quite possibly suboptimally. \")]),_c('h2',[_vm._v(\"Airflow\")]),_c('p',[_vm._v(\" Airflow is a platform for defining, running, and monitoring workflows. The workflows are defined as a \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\"}},[_vm._v(\"DAG\")]),_vm._v(\" containing many nodes - in airflow these are called tasks. A scheduler is responsible for understanding the relationships between tasks in a DAG, and triggering tasks when their parents have completed. A scheduler is also responsible for creating new task instances, i.e., for “kicking off” the DAG and orchestrating the instantiation of tasks. There are lots of ways to do this, but in this project I kick off the workflow once per day. In other words, my scheduler suggests one new Chinese proverb per day. \")]),_c('p',[_vm._v(\" Another important concept in Airflow is an operator, which is essentially a class that, when instantiated, becomes a task. Operators define different ways that data can be operated on, for example through a script, pod execution, or network call. In this project, I generally found that just running all tasks as a Python script was pretty easy, but it’s good to know that the flexibility of having other operator types exists. For example, there is an HttpOperator but I found that just using Python’s requests module was more intuitive for me. Whatever. \")]),_c('h2',[_vm._v(\"Chinese Proverb Pipeline\")]),_c('p',[_vm._v(\" The Chinese Proverb Pipeline is fairly straightforward, as depicted in the diagram below. \"),_c('strong',[_vm._v(\"fetch_known_words\")]),_vm._v(\" scans my \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://robertkotcher.github.io/blog\"}},[_vm._v(\"language blog\")]),_vm._v(\" for Chinese words that I've used in my writeups. \"),_c('strong',[_vm._v(\"fetch_proverbs\")]),_vm._v(\" fetches a CSV containing about 150 Chinese proverbs. \"),_c('strong',[_vm._v(\"choose_proverb\")]),_vm._v(\" combines the results of both of these operations and chooses a proverb that introduces new words, but not too many new words. Finally, \"),_c('strong',[_vm._v(\"save_proverb\")]),_vm._v(\" stores the proverb in an in-cluster data store. \")]),_c('div',{staticClass:\"image-container\"},[_c('img',{staticClass:\"pipeline-diagram\",attrs:{\"src\":require(\"./chinese_proverb.png\")}})]),_c('p',[_vm._v(\" I used the \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html\"}},[_vm._v(\"TaskFlow API\")]),_vm._v(\", which is part of Airflow 2. TaskFlow allows the developer to define tasks as Python functions with decorators. In the case that the developer wants to write a task that isn’t in the form of a Python function (which operator is used?), they can fall back to instantiating that Operator directly. It does other things like abstract out the inner-workings of \"),_c('a',{attrs:{\"target\":\"_blank\",\"href\":\"https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html\"}},[_vm._v(\"Xcoms\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" I found TaskFlow to be much easier and cleaner to work with. \")]),_c('h3',[_vm._v(\"Design Choices\")]),_c('ul',[_c('li',[_vm._v(\" I wanted to be sure that, as the number of known Chinese words grew, that the mechanism that passes data between tasks would be able to handle the growing size of the frequency map. Airflow allows task instances to communicate between one another with xcom. xcom uses the “metadata DB” to store return values from one task instance, and reveal those return values to children. I set up my metadata DB using Postgres (required when running tasks on kubernetes), and found that there weren’t any size constraints on the DB column that would be storing the word table. \")]),_c('li',[_vm._v(\" I debated different ways that the daily chosen Proverb would be communicated to me. As this project was focused on learning Airtable, I thought that configuring the final task to send me an email would be a good exercise. This can be done by overriding the pod template for the final task with credentials (set in a secret). Since email providers are paid, and I didn't feel like entering credit card information anywhere, I built a simple key/value store, spun it up in my cluster, and used that to store the most recently chosen proverb. \")])]),_c('h3',[_vm._v(\"Things I really like about working with Airflow\")]),_c('ul',[_c('li',[_vm._v(\" Being able to run task instances locally, either via a full workflow instance, or as a single task instance. \")]),_c('li',[_vm._v(\" The ease of setting up a workflow, especially with TaskFlow API. \")]),_c('li',[_vm._v(\" Configurability (the flexibility of being able to run the same workflow on different environments, with different underlying transport mechanisms) \")]),_c('li',[_vm._v(\" A really intuitive GUI for monitoring, that just works quite well out-of-the-box. \")])]),_c('h3',[_vm._v(\"Frustrations\")]),_c('p',[_vm._v(\" While I actually had no frustrations with Airflow (at least with this simple project), I found that DigitalOcean and Airtable don't always play together nicely. This isn’t really a limitation of Airflow. For example, volumes can only run in ReadWriteOnce accessMode. This has implications for larger clusters. For example, if I have many nodes, I have to be sure that pods belonging to the same airflow task are always schedule on the same node. \")]),_c('h2',[_vm._v(\"The final product\")]),_c('p',[_vm._v(\"Finally, here’s the pipeline in action. The following proverb was generated at 8 am UTC, based on the words I used in my blog:\")]),_c('div',{staticClass:\"proverb-container\"},[_c('span',{staticClass:\"proverb\"},[_vm._v(\" 今天的中国谚语是: 不作不死。 \")])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div>\n    <h1>Chinese Proverbs with Airflow</h1>\n    <p>\n        My current job involves generating AI data at scale in a pipeline. As we develop the system, the Apache Airflow project has come up in conversation quite a bit. Since I’d never used it before, I thought I’d give it a try.\n    </p>\n    <p>\n        This project is a Chinese Proverb generator. Every day, it scans my Chinese blog to see what words I’m using and their frequencies. Then it reads a dataset of Chinese proverbs from <a href='https://www.kaggle.com/datasets/bryanb/phrases-and-sayings' target='_blank'>Kaggle</a> and suggests one that introduces new words (but not too many new words).\n    </p>\n    <p>\n        The objective was to understand what is possible with Airflow and where it falls short. Any efforts that did not meet that description were done quickly and quite possibly suboptimally.\n    </p>\n    <h2>Airflow</h2>\n    <p>\n        Airflow is a platform for defining, running, and monitoring workflows. The workflows are defined as a <a target='_blank' href='https://en.wikipedia.org/wiki/Directed_acyclic_graph'>DAG</a> containing many nodes - in airflow these are called tasks. A scheduler is responsible for understanding the relationships between tasks in a DAG, and triggering tasks when their parents have completed. A scheduler is also responsible for creating new task instances, i.e., for “kicking off” the DAG and orchestrating the instantiation of tasks. There are lots of ways to do this, but in this project I kick off the workflow once per day. In other words, my scheduler suggests one new Chinese proverb per day.\n    </p>\n    <p>\n        Another important concept in Airflow is an operator, which is essentially a class that, when instantiated, becomes a task. Operators define different ways that data can be operated on, for example through a script, pod execution, or network call. In this project, I generally found that just running all tasks as a Python script was pretty easy, but it’s good to know that the flexibility of having other operator types exists. For example, there is an HttpOperator but I found that just using Python’s requests module was more intuitive for me. Whatever.\n    </p>\n    <h2>Chinese Proverb Pipeline</h2>\n    <p>\n        The Chinese Proverb Pipeline is fairly straightforward, as depicted in the diagram below. <strong>fetch_known_words</strong> scans my <a target='_blank' href='https://robertkotcher.github.io/blog'>language blog</a> for Chinese words that I've used in my writeups. <strong>fetch_proverbs</strong> fetches a CSV containing about 150 Chinese proverbs. <strong>choose_proverb</strong> combines the results of both of these operations and chooses a proverb that introduces new words, but not too many new words. Finally, <strong>save_proverb</strong> stores the proverb in an in-cluster data store.\n    </p>\n    <div class=\"image-container\">\n        <img class=\"pipeline-diagram\" src=\"./chinese_proverb.png\"/>\n    </div>\n    <p>\n        I used the <a target='_blank' href='https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html'>TaskFlow API</a>, which is part of Airflow 2. TaskFlow allows the developer to define tasks as Python functions with decorators. In the case that the developer wants to write a task that isn’t in the form of a Python function (which operator is used?), they can fall back to instantiating that Operator directly. It does other things like abstract out the inner-workings of <a target='_blank' href='https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html'>Xcoms</a>.\n    </p>\n    <p>\n        I found TaskFlow to be much easier and cleaner to work with.\n    </p>\n    <h3>Design Choices</h3>\n    <ul>\n        <li>\n            I wanted to be sure that, as the number of known Chinese words grew, that the mechanism that passes data between tasks would be able to handle the growing size of the frequency map. Airflow allows task instances to communicate between one another with xcom. xcom uses the “metadata DB” to store return values from one task instance, and reveal those return values to children. I set up my metadata DB using Postgres (required when running tasks on kubernetes), and found that there weren’t any size constraints on the DB column that would be storing the word table.\n        </li>\n        <li>\n            I debated different ways that the daily chosen Proverb would be communicated to me. As this project was focused on learning Airtable, I thought that configuring the final task to send me an email would be a good exercise. This can be done by overriding the pod template for the final task with credentials (set in a secret). Since email providers are paid, and I didn't feel like entering credit card information anywhere, I built a simple key/value store, spun it up in my cluster, and used that to store the most recently chosen proverb.\n        </li>\n    </ul>\n    <h3>Things I really like about working with Airflow</h3>\n    <ul>\n        <li>\n            Being able to run task instances locally, either via a full workflow instance, or as a single task instance.\n        </li>\n        <li>\n            The ease of setting up a workflow, especially with TaskFlow API.\n        </li>\n        <li>\n            Configurability (the flexibility of being able to run the same workflow on different environments, with different underlying transport mechanisms)\n        </li>\n        <li>\n            A really intuitive GUI for monitoring, that just works quite well out-of-the-box.\n        </li>\n    </ul>\n    <h3>Frustrations</h3>\n    <p>\n        While I actually had no frustrations with Airflow (at least with this simple project), I found that DigitalOcean and Airtable don't always play together nicely. This isn’t really a limitation of Airflow. For example, volumes can only run in ReadWriteOnce accessMode. This has implications for larger clusters. For example, if I have many nodes, I have to be sure that pods belonging to the same airflow task are always schedule on the same node.\n    </p>\n    <h2>The final product</h2>\n    <p>Finally, here’s the pipeline in action. The following proverb was generated at 8 am UTC, based on the words I used in my blog:</p>\n    <div class='proverb-container'>\n        <span class='proverb'>\n            今天的中国谚语是: 不作不死。\n        </span>\n    </div>\n  </div>\n</template>\n\n<script>\n  export default {\n    name: 'Main'\n  };\n</script>\n\n<style scoped>\n  .image-container {\n      margin: 30px 0;\n      width: 100%;\n      display: flex;\n      justify-content: center;\n  }\n\n  .pipeline-diagram {\n    width: 320px;\n    height: 130px;\n  }\n\n  ul {\n      margin: 0 32px;\n  }\n\n  li {\n      margin-top: 12px;\n      margin-bottom: 12px;\n  }\n\n  .proverb-container {\n      display: flex;\n      justify-content: center;\n  }\n\n  .proverb {\n      padding: 14px 28px;\n      background-color: #fff665;\n      margin-bottom: 42px;\n  }\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--13-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=script&lang=js\"","import { render, staticRenderFns } from \"./Main.vue?vue&type=template&id=d3540cfa&scoped=true\"\nimport script from \"./Main.vue?vue&type=script&lang=js\"\nexport * from \"./Main.vue?vue&type=script&lang=js\"\nimport style0 from \"./Main.vue?vue&type=style&index=0&id=d3540cfa&prod&scoped=true&lang=css\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"d3540cfa\",\n  null\n  \n)\n\nexport default component.exports","import LargeMarginClassifiers from './large_margin_classifiers/Main';\nimport SingularValueDecomposition from './singular_value_decomposition/Main';\nimport VanillaNeuralNetwork from './vanilla_neural_network/Main';\nimport DeepCryptokitties from './deep_cryptokitties/Main';\nimport Rnns from './rnns/Main';\nimport ChineseProverbsWithAirflow from './chinese_proverbs_with_airflow/Main';\n\n// Although it might not be the very best solution, I am going to just\n// register each article on the router for now. Anything that doesn't match\n// will pass right along to the 404 handler.\nexport default [\n  {\n    path: 'large-margin-classifiers',\n    component: LargeMarginClassifiers,\n    meta: { title: 'Large margin classifiers' }\n  },\n  {\n    path: 'singular-value-decomposition',\n    component: SingularValueDecomposition,\n    meta: { title: 'Singular Value Decomposition' }\n  },\n  {\n    path: 'vanilla-neural-network',\n    component: VanillaNeuralNetwork,\n    meta: { title: 'I made a vanilla neural network from scratch' }\n  },\n  {\n    path: 'deep-cryptokitties',\n    component: DeepCryptokitties,\n    meta: { title: 'Deep Cryptokitties' }\n  },\n  {\n    path: 'recurrent-neural-networks',\n    component: Rnns,\n    meta: { title: 'Some Recurrent Neural Networks, just for fun' }\n  },\n  {\n    path: 'chinese-proverbs-with-airflow',\n    component: ChineseProverbsWithAirflow,\n    meta: { title: 'Chinese Proverb generator with Airflow' }\n  }\n];\n","import routes from '../../posts';\n\nexport default routes;\n","import Vue from 'vue';\nimport VueRouter from 'vue-router';\nimport vuetify from './plugins/vuetify';\n\nimport App from './App';\n\nimport Home from './components/Home/Home';\nimport PostWrapper from './components/Posts/PostWrapper';\nimport PostSubrouter from './components/Posts/PostSubrouter';\n\nVue.use(VueRouter);\nVue.config.productionTip = false;\n\nconst router = new VueRouter({\n  mode: 'history',\n  routes: [\n    {\n      path: '/',\n      component: Home,\n      meta: { title: 'Robert Kotcher' }\n    },\n    {\n      path: '/posts',\n      component: PostWrapper,\n      children: PostSubrouter\n    }\n  ]\n});\n\nrouter.beforeEach((to, from, next) => {\n  document.title = to.meta.title;\n  next();\n});\n\nnew Vue({\n  router,\n  vuetify,\n  render: h => h(App),\n}).$mount('#root');\n","module.exports = __webpack_public_path__ + \"img/final-4.4fe516f3.png\";","module.exports = __webpack_public_path__ + \"img/svd.f3200088.png\";","module.exports = __webpack_public_path__ + \"img/li.ba045568.png\";","module.exports = __webpack_public_path__ + \"img/hf.a1f7d9ec.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=style&index=0&id=666ed3f6&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/quadratic.9d2016c3.png\";","module.exports = __webpack_public_path__ + \"img/adrich.da0bf58e.jpg\";","module.exports = __webpack_public_path__ + \"img/codecov.b6dfe4ee.jpg\";","module.exports = __webpack_public_path__ + \"img/large-margin.b196f5e9.png\";","module.exports = __webpack_public_path__ + \"img/random-1.a7a4888d.png\";","module.exports = __webpack_public_path__ + \"img/gen-2.2ce520b8.png\";","module.exports = __webpack_public_path__ + \"img/gen-1.3f00c017.png\";","module.exports = __webpack_public_path__ + \"img/random-2.c4646f3f.png\";","module.exports = __webpack_public_path__ + \"img/npm.e7ec3ec0.png\";","module.exports = __webpack_public_path__ + \"img/w7.58f44988.png\";","module.exports = __webpack_public_path__ + \"img/moment.616a10a3.png\";","module.exports = __webpack_public_path__ + \"img/functionA.25b4df33.png\";","module.exports = __webpack_public_path__ + \"img/logo.63a7d78d.svg\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Body.vue?vue&type=style&index=0&id=f9c8eb22&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/gen-4.5f3e67e7.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Item.vue?vue&type=style&index=0&id=f3f48df2&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/kitchen_zen.6b036aba.png\";","module.exports = __webpack_public_path__ + \"img/chinese_proverb.6be6cff3.png\";","module.exports = __webpack_public_path__ + \"img/logistic-regression.00af903d.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./PostWrapper.vue?vue&type=style&index=0&id=11b6bad8&prod&lang=css\"","module.exports = __webpack_public_path__ + \"img/profile.918c4550.png\";","module.exports = __webpack_public_path__ + \"img/svm.4acc9e8f.png\";","module.exports = __webpack_public_path__ + \"img/tunessence.dc5c1060.png\";","module.exports = __webpack_public_path__ + \"img/gh.84c8be2f.png\";","module.exports = __webpack_public_path__ + \"img/travel-map.1564c9c8.png\";","module.exports = __webpack_public_path__ + \"img/logo.82b9c7a5.png\";","module.exports = __webpack_public_path__ + \"img/final-3.2b9af48f.png\";","module.exports = __webpack_public_path__ + \"img/nn.daa9842c.png\";","module.exports = __webpack_public_path__ + \"img/zoom.74e22543.png\";","module.exports = __webpack_public_path__ + \"img/panama-1.82e5725f.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--7-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--7-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--7-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--1-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Main.vue?vue&type=style&index=0&id=d3540cfa&prod&scoped=true&lang=css\"","module.exports = __webpack_public_path__ + \"img/cryptokitty.0cbe12d5.png\";","module.exports = __webpack_public_path__ + \"img/timing_attack.4ab4d27c.png\";","module.exports = __webpack_public_path__ + \"img/carnegie_mellon.4e4972d8.png\";","module.exports = __webpack_public_path__ + \"img/basic-net.e9111ddb.png\";","module.exports = __webpack_public_path__ + \"img/panama.73980cde.jpg\";"],"sourceRoot":""}