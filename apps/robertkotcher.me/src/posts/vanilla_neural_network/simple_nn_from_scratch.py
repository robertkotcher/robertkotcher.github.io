# -*- coding: utf-8 -*-
"""Simple NN from scratch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18l2m4NJEkwsxW8IO1la274ZfWHg18WGz
"""

import math
import random

class NeuralNetwork:
    def __init__(self, n_input, n_hidden, n_output):
        self.hidden_layer = NeuronLayer(n_input, n_hidden)
        self.output_layer = NeuronLayer(n_hidden, n_output)
        self.LEARNING_RATE = 0.5

    def feed_forward(self, input):
        out = self.hidden_layer.feed_forward(input)
        return self.output_layer.feed_forward(out)

    def train(self, train_input, train_target):
        # Pass one dataset through the network. This will cache the output
        # at each of the neurons to be used in backprop
        self.feed_forward(train_input)

        # (1) dE_total/dout_01 * dout_01/dnet_01
        change_in_error_wrt_output_layer = [0] * len(self.output_layer.neurons)
        for i in range(len(self.output_layer.neurons)):
            change_in_error_wrt_output_layer[i] = self.output_layer.neurons[i].calculate_change_in_error_wrt_net(train_target[i])

        # (2) dEtotal/dout_01 * dout_01/dnet_01
        change_in_error_wrt_hidden_layer = [0] * len(self.hidden_layer.neurons) 
        for i in range(len(self.hidden_layer.neurons)):
            sum = 0
            for j in range(len(self.output_layer.neurons)):
                w = self.output_layer.neurons[j].weights[i]
                grad = self.output_layer.neurons[j].calculate_change_in_error_wrt_net(train_target[j])
                sum += (grad * w)

            sum *= self.hidden_layer.neurons[i].derivative_of_activation()

            change_in_error_wrt_hidden_layer[i] = sum

        # (3)
        for i in range(len(self.output_layer.neurons)):
            for j in range(len(self.hidden_layer.neurons)):

                # Find the full delta error with respect to each of the weights
                pd = change_in_error_wrt_output_layer[i] * self.hidden_layer.neurons[j].output

                # Change the existing weights by some weighting of the partial derivative
                self.output_layer.neurons[i].weights[j] -= self.LEARNING_RATE * pd

        
        for i in range(len(self.hidden_layer.neurons)):
            for j in range(len(train_input)):

                # Multiply partial derivative by partial derivative net with respect to the
                # weight, i.e. the input that connects to that edge                
                pd = change_in_error_wrt_hidden_layer[i] * self.hidden_layer.neurons[i].derivative_of_net_wrt_weight(j)

                # Update each of the four inputs into the hidden layer
                self.hidden_layer.neurons[i].weights[j] -= self.LEARNING_RATE * pd

    def calculate_total_error(self, training_sets):
        res = 0
        for i in range(len(training_sets)):
            self.feed_forward(training_sets[i][0])
            for j in range(len(self.output_layer.neurons)):
                res += self.output_layer.neurons[j].calculate_loss(training_sets[i][1][j])
        return res / len(training_sets)

class NeuronLayer:
    def __init__(self, n_inputs, n_neurons):
        self.bias = 0
        self.neurons = [Neuron(n_inputs, self.bias) for i in range(n_neurons)]

    def feed_forward(self, input):
        output = []
        for neuron in self.neurons:
            output.append(neuron.calculate_output(input))
        return output

# The following neuron uses the standard logistic
# activation function and squared loss:
#
# φ = 1 / (1 + math.exp(-1 * total))
#
# L = 0.5 * (target - output)^2
#
# Note that the word "net" is used to denote the
# matrix multiplication before φ is applied.
#
class Neuron:
    def __init__(self, n_inputs, bias):
        self.bias = bias
        self.weights = [random.random() for i in range(n_inputs)]

    def calculate_output(self, inputs):
        self.inputs = inputs

        total = 0
        for i in range(len(inputs)):
            total += (inputs[i] * self.weights[i])
        total += self.bias
        
        self.output = 1 / (1 + math.exp(-total))
        return self.output

    # ∂E/∂net
    # This part will be used internally to calculate the
    # updates to the weight matrix, as well as externally to
    # update the bias in the previous layer as well as
    # potentially weights of previous neuron layers.
    def calculate_change_in_error_wrt_net(self, target):
        return self.derivative_of_loss(target) * self.derivative_of_activation()

    def derivative_of_activation(self):
        return self.output * (1 - self.output)

    def derivative_of_loss(self, target):
        return self.output - target

    def derivative_of_net_wrt_weight(self, index):
        return self.inputs[index]

    def calculate_loss(self, target):
        return 0.5 * (target - self.output) ** 2

# Simple example that maps this specific input pair to this specific output pair
nn = NeuralNetwork(2, 10, 2)
for i in range(10000):
    nn.train([0.05, 0.1], [0.01, 0.99])
    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))